<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 4: Images as Data - Open-Source AI for Research</title>
    
    <!-- Google Fonts Import -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600;700&display=swap" rel="stylesheet">

    <!-- All CSS is in this <style> tag. Includes original styles + new subpage styles -->
    <style>
        :root {
            --color-primary-dark: #1a1a2e;
            --color-primary-light: #16213e;
            --color-secondary: #0f3460;
            --color-accent: #53d3b0;
            --color-accent-dark: #46b394;
            --color-text-light: #e0e0e0;
            --color-text-dark: #000000;
            --color-card-bg: #ffffff;
            --font-family: 'Poppins', sans-serif;
        }

        /* --- Global Reset --- */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-family);
            background-color: #f4f7fc;
            color: var(--color-text-dark);
            line-height: 1.6;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        .container {
            width: 90%;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px 0;
        }

        h1, h2, h3, h4 {
            font-weight: 600;
        }

        h1 {
            font-size: 2.8rem;
            margin-bottom: 0.5rem;
        }

        h2 {
            font-size: 2.2rem;
            margin-bottom: 2rem;
            text-align: center;
            color: var(--color-primary-dark);
        }
        
        h3 {
            font-size: 1.25rem;
            color: var(--color-primary-light);
            margin-bottom: 0.75rem;
        }
        
        h4 {
            font-size: 1.1rem;
            color: var(--color-primary-dark);
            margin-bottom: 1rem;
            margin-top: 1.5rem;
        }

        p {
            font-size: 1rem;
            font-weight: 300;
            margin-bottom: 1rem;
        }
        
        li {
            font-size: 1rem;
            font-weight: 300;
            margin-bottom: 0.5rem;
            margin-left: 20px;
        }

        /* --- NEW: Subpage Header --- */
        .subpage-header {
            background: var(--color-card-bg);
            border-bottom: 1px solid #e0e0e0;
            padding: 20px 0;
        }
        
        .subpage-header .container {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .subpage-header h1 {
            font-size: 1.8rem;
            color: var(--color-primary-dark);
            margin-bottom: 0;
        }
        
        .back-link {
            display: inline-block;
            text-decoration: none;
            color: var(--color-primary-light);
            font-weight: 600;
            font-size: 0.9rem;
            transition: color 0.3s ease;
        }
        
        .back-link:hover {
            color: var(--color-accent);
        }

        /* --- Hero Section (Original) --- */
        .hero {
            background: linear-gradient(135deg, var(--color-primary-dark) 0%, var(--color-secondary) 100%);
            color: #ffffff;
            padding: 100px 20px 80px 20px;
            text-align: center;
            border-bottom-left-radius: 40px;
            border-bottom-right-radius: 40px;
        }

        .hero p {
            font-size: 1.25rem;
            font-weight: 300;
            max-width: 700px;
            margin: 0 auto 1.5rem auto;
            color: var(--color-text-light);
        }

        .hero-badge {
            display: inline-block;
            background-color: rgba(255, 255, 255, 0.1);
            color: var(--color-accent);
            padding: 8px 16px;
            border-radius: 50px;
            font-weight: 600;
            font-size: 0.9rem;
            margin-bottom: 1rem;
            letter-spacing: 0.5px;
        }

        /* --- NEW: Module Detail Section --- */
        .module-detail-section {
            padding: 40px 0;
        }
        
        .module-intro {
            background: var(--color-card-bg);
            padding: 30px;
            border-radius: 20px;
            margin-bottom: 40px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.07);
        }
        
        .module-intro h2 {
            text-align: left;
            margin-bottom: 1rem;
        }
        
        .module-intro p {
            font-size: 1.1rem;
            color: #555;
        }

        /* --- Modules Section --- */
        .modules-section {
            padding: 60px 0;
        }

        .modules-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 30px;
        }
        
        /* --- NEW: Grid for this page --- */
        .module-detail-grid {
            display: grid;
            grid-template-columns: 1fr;
            gap: 30px;
        }

        /* --- Module Card --- */
        .module-card {
            background: var(--color-card-bg);
            border-radius: 20px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.07);
            padding: 30px;
            display: flex;
            flex-direction: column;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .module-card:hover {
            transform: translateY(-5px); /* Reduced hover effect for subpage */
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.08);
        }

        .module-number {
            font-size: 1.1rem;
            font-weight: 700;
            color: var(--color-accent);
            margin-bottom: 0.5rem;
        }

        .module-card p {
            color: #555;
            font-size: 0.95rem;
            flex-grow: 1;
        }
        
        .module-card h3 {
            min-height: 2.5em; /* Ensures titles align even if one wraps */
        }
        
        /* --- NEW: Style for model name --- */
        .model-name {
            font-weight: 700;
            color: var(--color-primary-light);
            background-color: #f0f4f8;
            padding: 2px 8px;
            border-radius: 6px;
            font-size: 0.9rem;
        }

        /* --- Call to Action (CTA) Section --- */
        .cta-section {
            background: linear-gradient(135deg, var(--color-secondary) 0%, var(--color-primary-dark) 100%);
            padding: 60px 20px;
            text-align: center;
            border-radius: 20px;
            margin: 40px auto;
            max-width: 1200px;
            width: 90%;
        }

        .cta-section h2 {
            color: #ffffff;
            margin-bottom: 1.5rem;
        }

        .cta-button {
            display: inline-block;
            background-color: var(--color-accent);
            color: var(--color-primary-dark);
            font-size: 1.1rem;
            font-weight: 700;
            padding: 15px 35px;
            border-radius: 50px;
            text-decoration: none;
            transition: background-color 0.3s ease, transform 0.3s ease;
        }

        .cta-button:hover {
            background-color: var(--color-accent-dark);
            transform: scale(1.05);
        }
        
        /* --- NEW: Task Section (modified CTA) --- */
        .task-section {
            background: var(--color-card-bg);
            border: 2px solid var(--color-accent);
            padding: 40px;
            text-align: left;
            border-radius: 20px;
            margin: 40px auto;
            max-width: 1200px;
            width: 100%;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.05);
        }
        
        .task-section h2 {
            color: var(--color-primary-dark);
            text-align: left;
            margin-bottom: 1rem;
        }
        
        .task-section p {
            color: #333;
            font-size: 1.05rem;
        }

        /* --- Footer --- */
        footer {
            text-align: center;
            padding: 30px 0;
            color: #777;
            font-size: 0.9rem;
        }

        /* --- Responsive Design --- */
        @media (max-width: 768px) {
            h1 {
                font-size: 2.2rem;
            }
            .hero {
                padding: 60px 20px 40px 20px;
                border-bottom-left-radius: 30px;
                border-bottom-right-radius: 30px;
            }
            .modules-grid {
                grid-template-columns: 1fr;
            }
            .subpage-header .container {
                flex-direction: column;
                align-items: flex-start;
                gap: 10px;
            }
            .subpage-header h1 {
                font-size: 1.5rem;
            }
        }
    </style>
</head>
<body>

    <!-- Subpage Header -->
    <header class="subpage-header">
        <div class="container">
            <h1>Module 4: Images as Data</h1>
            <!-- Assume the main page is index.html. Change '#' as needed. -->
            <a href="index.html" class="back-link">&larr; Back to All Modules</a>
        </div>
    </header>

    <main>
        <!-- Module Detail Section -->
        <section class="module-detail-section">
            <div class="container">

                <!-- Module Introduction -->
                <div class="module-intro">
                    <h2>Computer Vision (Natural Sciences & Digital Humanities)</h2>
                    <p>
                        This module focuses on Computer Vision (CV). You will learn how to
                        treat images as a rich source of research data, moving beyond
                        manual observation to automated, large-scale analysis. We will
                        cover an end-to-end workflow: from automatically labeling a dataset
                        to training a custom object detection model and using it for segmentation.
                    </p>
                </div>
                
                <h2>End-to-End Computer Vision Workflow</h2>
                
                <!-- Grid for Module Patterns -->
                <div class="module-detail-grid">

                    <!-- Part 1: Getting Started with Pre-trained Models -->
                    <div class="module-card">
                        <span class="module-number">Part 1</span>
                        <h3>Getting Started: Object Detection with Pre-trained Models</h3>
                        <p>
                            This part introduces the core task of object detection. We demonstrate how to use powerful, pre-trained object detection models to immediately find objects in images. This is the fastest way to begin a computer vision project. We explore not just how to use these models, but also how they work architecturally. Crucially, we investigate their performance on a specialized research dataset (e.g. camera trap images of animals) to understand the limitations of general-purpose models, such as the 'class mismatch' problem, where a model trained on common objects (like COCO) is applied to a new domain.
                        </p>
                        <h4>Key Research Applications:</h4>
                        <ul>
                            <li><strong>Rapid Data Exploration:</strong> Quickly scan a new image dataset to see what a general-purpose model can identify out-of-the-box.</li>
                            <li><strong>Understanding Model Architectures:</strong> Compare and contrast different model philosophies, such as the single-stage efficiency of certain models versus transformer-based approaches.</li>
                            <li><strong>Evaluating Out-of-Domain Performance:</strong> Quantify the "class mismatch" problem by analyzing how a model trained on one dataset (e.g., COCO) performs on a different, specialized dataset (e.g., camera trap images), which is a critical first step before deciding to fine-tune.</li>
                        </ul>
                        <h4>Concrete Steps from the Notebook:</h4>
                        <ul>
                            <li>Load pre-trained <span class="model-name">YOLOv8</span> and <span class="model-name">DETR</span> models.</li>
                            <li>Run inference on sample images to get bounding boxes and class labels.</li>
                            <li>Visualize internal model workings, such as feature maps (for single-stage detectors) and attention maps (for transformer-based models), to gain intuition.</li>
                            <li>Systematically evaluate model performance on a new dataset by comparing ground truth labels to model predictions.</li>
                            <li>Analyze the "class mismatch" problem using tools like cross-tabulation (confusion matrix) and ROC curves.</li>
                        </ul>
                        <div style="display: flex; flex-wrap: wrap; gap: 10px; margin-top: 15px;">
                            <a href="https://github.com/jpata/ai-course/blob/main/notebooks/module4_pt1_object_detection.ipynb" class="cta-button" style="padding: 10px 20px; font-size: 0.9rem; flex: 1; text-align: center; min-width: 150px;">View on GitHub</a>
                            <a href="https://colab.research.google.com/github/jpata/ai-course/blob/main/notebooks/module4_pt1_object_detection.ipynb" target="_blank" class="cta-button" style="padding: 10px 20px; font-size: 0.9rem; flex: 1; text-align: center; min-width: 150px; background-color: #4285F4;">Open in Colab</a>
                        </div>
                    </div>

                    <!-- Part 2: Zero-Shot Detection & Auto-Labeling -->
                    <div class="module-card">
                        <span class="module-number">Part 2</span>
                        <h3>Zero-Shot Detection &amp; Auto-Labeling with OWL2</h3>
                        <p>
                            We start with a powerful, general-purpose object detection model called <span class="model-name">OWL2</span>.
                            Its key feature is "zero-shot" detection: you can provide it with text descriptions of objects to find (e.g., "a photo of a leopard") without having to train it on pre-labeled examples.
                        </p>
                        <h4>Key Research Applications:</h4>
                        <ul>
                            <li><strong>Rapid Prototyping:</strong> Quickly test if objects of interest can be found in a new dataset.</li>
                            <li><strong>Automatic Labeling:</strong> Use OWL2's predictions to automatically generate bounding boxes for a large number of images, creating a training dataset for a more specialized model.</li>
                        </ul>
                        <h4>Concrete Steps from the Notebook:</h4>
                        <ul>
                            <li>Load the ENA24 dataset of camera trap images.</li>
                            <li>Use <span class="model-name">OWL2</span> with a general prompt like "a photo of an animal" to detect animals in the images.</li>
                            <li>Convert the detected bounding boxes into the YOLO label format.</li>
                        </ul>
                        <div style="display: flex; flex-wrap: wrap; gap: 10px; margin-top: 15px;">
                            <a href="https://github.com/jpata/ai-course/blob/main/notebooks/module4_pt2_open_set_detection.ipynb" class="cta-button" style="padding: 10px 20px; font-size: 0.9rem; flex: 1; text-align: center; min-width: 150px;">View on GitHub</a>
                            <a href="https://colab.research.google.com/github/jpata/ai-course/blob/main/notebooks/module4_pt2_open_set_detection.ipynb" target="_blank" class="cta-button" style="padding: 10px 20px; font-size: 0.9rem; flex: 1; text-align: center; min-width: 150px; background-color: #4285F4;">Open in Colab</a>
                        </div>
                    </div>

                    <!-- Part 2: Fine-Tuning a Specialized Detector -->
                    <div class="module-card">
                        <span class="module-number">Part 3</span>
                        <h3>Fine-Tuning a Specialized Object Detector</h3>
                        <p>
                            While powerful, OWL2 can be slow. For large-scale analysis, we need a faster model. We use the dataset we automatically labeled in Part 2 to fine-tune a specialized object detection model. Fine-tuning adapts a pre-trained model to a new, specific task.
                        </p>
                        <h4>Key Research Applications:</h4>
                        <ul>
                            <li><strong>Domain Specialization:</strong> Create a model that is highly accurate for a specific set of objects (e.g., certain animal species, historical artifacts).</li>
                            <li><strong>Efficient Processing:</strong> Build a model that is fast enough to process thousands or millions of images on standard hardware.</li>
                        </ul>
                        <h4>Concrete Steps from the Notebook:</h4>
                        <ul>
                            <li>Create a `dataset.yaml` file to configure the training process for the chosen object detector.</li>
                            <li>Load a pre-trained base model (e.g., <span class="model-name">YOLOv8n</span>).</li>
                            <li>Train the model on the auto-generated labels from OWL2, creating a new model specialized for the ENA24 dataset.</li>
                            <li>Evaluate the model's performance using a confusion matrix and loss curves.</li>
                        </ul>
                        <div style="display: flex; flex-wrap: wrap; gap: 10px; margin-top: 15px;">
                            <a href="https://github.com/jpata/ai-course/blob/main/notebooks/module4_pt3_finetuning.ipynb" class="cta-button" style="padding: 10px 20px; font-size: 0.9rem; flex: 1; text-align: center; min-width: 150px;">View on GitHub</a>
                            <a href="https://colab.research.google.com/github/jpata/ai-course/blob/main/notebooks/module4_pt3_finetuning.ipynb" target="_blank" class="cta-button" style="padding: 10px 20px; font-size: 0.9rem; flex: 1; text-align: center; min-width: 150px; background-color: #4285F4;">Open in Colab</a>
                        </div>
                    </div>

                    <!-- Part 3: Using the Fine-Tuned Model -->
                    <div class="module-card">
                        <span class="module-number">Part 4</span>
                        <h3>Object Detection with the Fine-Tuned Model</h3>
                        <p>
                            With our fine-tuned model, we now have an efficient and specialized tool for our research task. This model can rapidly detect the specific classes it was trained on (in this case, the various animals from the ENA24 dataset) across the entire dataset.
                        </p>
                        <h4>Key Research Applications:</h4>
                        <ul>
                            <li><strong>Large-Scale Surveys:</strong> Automatically count and locate objects of interest across massive image collections.</li>
                            <li><strong>Real-Time Analysis:</strong> Use the efficient model for applications that require immediate results, like video analysis.</li>
                        </ul>
                        <h4>Concrete Steps from the Notebook:</h4>
                        <ul>
                            <li>Load the `best.pt` weights from the YOLOv8 training process.</li>
                            <li>Run inference on new images to get fast and accurate bounding boxes for the target classes.</li>
                            <li>Visualize the results to confirm the model's effectiveness.</li>
                        </ul>
                        <div style="display: flex; flex-wrap: wrap; gap: 10px; margin-top: 15px;">
                            <a href="https://github.com/jpata/ai-course/blob/main/notebooks/module4_pt4_evaluation.ipynb" class="cta-button" style="padding: 10px 20px; font-size: 0.9rem; flex: 1; text-align: center; min-width: 150px;">View on GitHub</a>
                            <a href="https://colab.research.google.com/github/jpata/ai-course/blob/main/notebooks/module4_pt4_evaluation.ipynb" target="_blank" class="cta-button" style="padding: 10px 20px; font-size: 0.9rem; flex: 1; text-align: center; min-width: 150px; background-color: #4285F4;">Open in Colab</a>
                        </div>
                    </div>
                    
                    <!-- Part 4: Advanced Segmentation with SAM + YOLO -->
                    <div class="module-card">
                        <span class="module-number">Part 5</span>
                        <h3>Promptable Segmentation with SAM and Object Detectors</h3>
                        <p>
                            Bounding boxes are good, but what if you need the exact outline of an object? We can combine our fine-tuned object detection model with the <span class="model-name">Segment Anything Model (SAM)</span>. SAM can take prompts, like bounding boxes, and produce highly detailed segmentation masks.
                        </p>
                        <h4>Key Research Applications:</h4>
                        <ul>
                            <li><strong>Precise Measurement:</strong> Calculate the exact size, area, or shape of objects (e.g., tumor size in medical scans, leaf area in botanical studies).</li>
                            <li><strong>Object Extraction:</strong> Isolate and extract objects from their background for further analysis or visualization.</li>
                        </ul>
                        <h4>Workflow:</h4>
                        <ol>
                            <li>Run the fine-tuned object detection model (e.g., <span class="model-name">YOLOv8</span>) on an image to get bounding boxes.</li>
                            <li>Pass the image and the bounding boxes as prompts to <span class="model-name">SAM</span>.</li>
                            <li>SAM returns a precise pixel-level mask for each object within the boxes.</li>
                        </ol>
                        <div style="display: flex; flex-wrap: wrap; gap: 10px; margin-top: 15px;">
                            <a href="https://github.com/jpata/ai-course/blob/main/notebooks/module4_pt5_sam.ipynb" class="cta-button" style="padding: 10px 20px; font-size: 0.9rem; flex: 1; text-align: center; min-width: 150px;">View on GitHub</a>
                            <a href="https://colab.research.google.com/github/jpata/ai-course/blob/main/notebooks/module4_pt5_sam.ipynb" target="_blank" class="cta-button" style="padding: 10px 20px; font-size: 0.9rem; flex: 1; text-align: center; min-width: 150px; background-color: #4285F4;">Open in Colab</a>
                        </div>
                    </div>

                </div>
                
                <!-- Practical Task Section -->
                <div class="task-section">
                    <h2>Module 4: Practical Task</h2>
                    <p>
                        This module's practical task is to follow the complete workflow demonstrated in the notebooks. You will start by using <span class="model-name">OWL2</span> to automatically label a sample of the ENA24 dataset. Then, you will use those labels to fine-tune a specialized object detection model (e.g., <span class="model-name">YOLOv8</span>). Finally, you will use your new, specialized model to perform object detection on the remaining images.
                    </p>
                </div>

            </div>
        </section>
    </main>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>&copy; 2025 Open-Source AI for Research. All rights reserved.</p>
        </div>
    </footer>

</body>
</html>