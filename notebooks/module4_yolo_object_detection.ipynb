{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29f70c6",
   "metadata": {},
   "source": [
    "# Object Detection with YOLO\n",
    "\n",
    "This notebook demonstrates how to use the YOLO model for object detection on the IDLE-OO-Camera-Traps dataset.\n",
    "\n",
    "First, let's install the necessary libraries.\n",
    "\n",
    "```python\n",
    "!pip install -q transformers datasets torch torchvision Pillow ultralytics scikit-learn seaborn\n",
    "```\n",
    "\n",
    "Now, let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from IPython.display import display,HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f1121f",
   "metadata": {},
   "source": [
    "## YOLO Object Detection\n",
    "\n",
    "First, we'll use a pretrained YOLOv8 model to perform object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8117575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLO model\n",
    "model_yolo = YOLO('../yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80444d",
   "metadata": {},
   "source": [
    "Next, we load the `imageomics/IDLE-OO-Camera-Traps` dataset. We'll just take one example from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ba4e0",
   "metadata": {
    "label": "load-image-cell"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=\"../data/IDLE-OO-Camera-Traps\", split=\"test\")\n",
    "iterator = iter(dataset)\n",
    "sample = next(iterator)\n",
    "image = sample[\"image\"]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049c4fb",
   "metadata": {},
   "source": [
    "Now we can run the YOLO model on the image. We'll test three different confidence thresholds: 0.5, 0.1, and 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec86f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "confidence_thresholds = [0.5, 0.1, 0.001]\n",
    "\n",
    "for conf in confidence_thresholds:\n",
    "    print(f\"Running YOLO detection with confidence threshold: {conf}\")\n",
    "    # Run inference on a copy of the image\n",
    "    results_yolo = model_yolo(image.copy(), conf=conf)\n",
    "\n",
    "    # Plot results\n",
    "    im_array = results_yolo[0].plot()\n",
    "    im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576eb19",
   "metadata": {},
   "source": [
    "### YOLO Network Architecture Analysis\n",
    "\n",
    "To understand how the YOLO model turns image features into predictions, we can inspect its architecture. The model is generally composed of three main parts: the **Backbone**, the **Neck**, and the **Head**.\n",
    "\n",
    "*   **Backbone**: This is a deep convolutional neural network that extracts features from the input image at various scales. YOLOv8 uses a modified CSPDarknet53 architecture.\n",
    "*   **Neck**: This part connects the backbone to the head. It takes feature maps from different stages of the backbone and combines them to create richer feature pyramids. This allows the model to detect objects of different sizes more effectively. YOLOv8 uses a Path Aggregation Network (PANet) for this.\n",
    "*   **Head (Detection)**: This is the final part of the network that generates the output predictions. It takes the feature maps from the neck and produces bounding boxes, class probabilities, and objectness scores.\n",
    "\n",
    "Let's print the model structure to see the layers. We will use the pretrained `yolov8n.pt` model for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21889b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLO model to inspect its architecture\n",
    "model_to_inspect = YOLO('../yolov8n.pt')\n",
    "print(model_to_inspect.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c5388",
   "metadata": {},
   "source": [
    "### From Features to Predictions\n",
    "\n",
    "The key to understanding the prediction process lies in the `Detect` module at the end of the network structure (the last layer in the printed output).\n",
    "\n",
    "1.  **Input Feature Maps**: The `Detect` head receives feature maps from the neck at three different scales (e.g., 80x80, 40x40, 20x20 for a 640x640 input). Each scale is responsible for detecting objects of a corresponding size (small, medium, large).\n",
    "\n",
    "2.  **Convolutional Prediction**: For each feature map, the `Detect` head applies a set of 1x1 convolutional layers. These convolutions transform the feature map's channels into a format that represents the predictions. For each location (or \"patch\") in the feature map, the model predicts:\n",
    "    *   **Bounding Box Coordinates (4 values)**: These are typically `(x_center, y_center, width, height)`, which are regressed relative to the grid cell's location.\n",
    "    *   **Class Probabilities (C values)**: A probability for each of the `C` classes the model was trained on.\n",
    "    *   **Objectness Score (1 value)**: This is often implicitly part of the class prediction or a separate score indicating the confidence that an object is present in the bounding box. In YOLOv8, the box and class predictions are decoupled.\n",
    "\n",
    "3.  **Output Tensor**: The output of the `Detect` head is a set of tensors. For a single image, the predictions from all scales are concatenated. The final output tensor has a shape like `(batch_size, num_classes + 4, num_predictions)`, where `num_predictions` is the total number of prediction anchors across all scales.\n",
    "\n",
    "4.  **Decoding the Output**: This raw tensor output is then post-processed:\n",
    "    *   The bounding box values are scaled to the original image dimensions.\n",
    "    *   The class scores are passed through a softmax or sigmoid function to get final probabilities.\n",
    "    *   Non-Maximum Suppression (NMS) is applied to filter out overlapping bounding boxes for the same object, keeping only the one with the highest confidence score.\n",
    "\n",
    "This process allows the model to efficiently predict multiple objects of various sizes and classes in a single forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900029f",
   "metadata": {},
   "source": [
    "### YOLO Intermediate Layer Visualization\n",
    "\n",
    "Now, let's visualize the intermediate outputs of the YOLO model. We will add hooks to the model layers to capture the feature maps and then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af467d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# A list to store the feature maps\n",
    "feature_maps = []\n",
    "hooks = []\n",
    "\n",
    "# The hook function that saves the output of a layer\n",
    "def hook_fn(module, input, output):\n",
    "    feature_maps.append(output)\n",
    "\n",
    "# We will visualize the output of the first 10 layers of the YOLO model\n",
    "detection_model = model_yolo.model\n",
    "layers_to_hook = detection_model.model[:10]\n",
    "\n",
    "# Register a forward hook for each layer to be visualized\n",
    "for layer in layers_to_hook:\n",
    "    hooks.append(layer.register_forward_hook(hook_fn))\n",
    "\n",
    "# Run inference to trigger the hooks.\n",
    "# Make sure to clear feature_maps before running, as hooks are global.\n",
    "feature_maps = []\n",
    "results_yolo = model_yolo(image.copy())\n",
    "\n",
    "# Remove the hooks after inference\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Now, let's visualize the feature maps\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "num_layers = len(feature_maps)\n",
    "layer_names = [f\"Layer {i}: {type(layers_to_hook[i]).__name__}\" for i in range(num_layers)]\n",
    "\n",
    "# Plot the feature maps in a grid\n",
    "cols = 4\n",
    "rows = (num_layers + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (fm, name) in enumerate(zip(feature_maps, layer_names)):\n",
    "    # Detach the tensor from the computation graph and move it to the CPU\n",
    "    fm = fm.detach().cpu()\n",
    "    # Get the feature map for the first image in the batch\n",
    "    fm = fm[0]\n",
    "    \n",
    "    C, H, W = fm.shape\n",
    "    \n",
    "    ax = axes[i]\n",
    "\n",
    "    # Use PCA to visualize the feature map's channel dimension\n",
    "    if C >= 3:\n",
    "        # Reshape for PCA: from (C, H, W) to (H*W, C)\n",
    "        data = fm.permute(1, 2, 0).reshape(H * W, C).numpy()\n",
    "        \n",
    "        # Apply PCA to reduce to 3 components for RGB visualization\n",
    "        pca = PCA(n_components=3)\n",
    "        pca_result = pca.fit_transform(data)\n",
    "        \n",
    "        # Reshape back to an image (H, W, 3)\n",
    "        pca_image = pca_result.reshape(H, W, 3)\n",
    "        \n",
    "        # Normalize each of the 3 principal components to the range [0, 1]\n",
    "        for c in range(3):\n",
    "            channel = pca_image[:, :, c]\n",
    "            min_val, max_val = channel.min(), channel.max()\n",
    "            if max_val > min_val:\n",
    "                pca_image[:, :, c] = (channel - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                pca_image[:, :, c] = 0\n",
    "        \n",
    "        ax.imshow(pca_image)\n",
    "        ax.set_title(name)\n",
    "    else:\n",
    "        # Fallback for layers with fewer than 3 channels: show the first channel in grayscale\n",
    "        ax.imshow(fm[0], cmap='gray')\n",
    "        ax.set_title(name + \" (grayscale)\")\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3640ff38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34aad0f7",
   "metadata": {},
   "source": [
    "## Exploring the ENA24 Dataset\n",
    "\n",
    "Instead of fine-tuning on a custom dataset, we will explore the `ENA24` dataset directly from the local checkout. This involves loading the existing labels, visualizing the frequency of common names, and displaying sample images for each unique common name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define the base path to the locally checked out dataset\n",
    "base_data_path = '../data/IDLE-OO-Camera-Traps/'\n",
    "ena24_csv_path = os.path.join(base_data_path, 'ENA24-balanced.csv')\n",
    "\n",
    "# Load the ENA24-balanced.csv file\n",
    "ena24_df = pd.read_csv(ena24_csv_path)\n",
    "print(f\"Successfully loaded {ena24_csv_path}\")\n",
    "print(f\"Total images in ENA24 dataset: {len(ena24_df)}\")\n",
    "\n",
    "# --- Visualize \"common_name\" frequency ---\n",
    "print(\"\\nVisualizing 'common_name' frequency...\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "ena24_df['common_name'].value_counts().plot(kind='bar')\n",
    "plt.title('Frequency of Common Names in ENA24 Dataset')\n",
    "plt.xlabel('Common Name')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Collect true and predicted classes for confusion matrix ---\n",
    "print(\"\\nProcessing images to collect true and predicted classes...\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "unique_common_names = ena24_df['common_name'].unique()\n",
    "\n",
    "# Let's take a few images per class to build a more meaningful confusion matrix\n",
    "num_samples_per_class = 5 \n",
    "\n",
    "for name in unique_common_names:\n",
    "    sample_images = ena24_df[ena24_df['common_name'] == name].head(num_samples_per_class)\n",
    "    \n",
    "    if not sample_images.empty:\n",
    "        for index, row in sample_images.iterrows():\n",
    "            image_relative_path = row['filepath']\n",
    "            full_image_path = os.path.join(base_data_path, 'data/test/', image_relative_path)\n",
    "            \n",
    "            if os.path.exists(full_image_path):\n",
    "                try:\n",
    "                    img = Image.open(full_image_path)\n",
    "                    \n",
    "                    # Run YOLO detection\n",
    "                    results_yolo_sample = model_yolo(img.copy(), conf=0.25)\n",
    "\n",
    "                    y_true.append(name)\n",
    "\n",
    "                    if len(results_yolo_sample[0].boxes) > 0:\n",
    "                        # Get top prediction (results are sorted by confidence)\n",
    "                        top_prediction_cls_id = int(results_yolo_sample[0].boxes.cls[0].item())\n",
    "                        predicted_name = model_yolo.names[top_prediction_cls_id]\n",
    "                        y_pred.append(predicted_name)\n",
    "                    else:\n",
    "                        y_pred.append(\"No detection\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not process image {full_image_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"Image file not found: {full_image_path}\")\n",
    "\n",
    "print(\"Finished collecting predictions.\")\n",
    "\n",
    "# --- Create and display the confusion matrix ---\n",
    "print(\"\\nGenerating confusion matrix...\")\n",
    "\n",
    "# Use pandas crosstab for a straightforward confusion matrix\n",
    "df_cm = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\n",
    "confusion_crosstab = pd.crosstab(df_cm['y_true'], df_cm['y_pred'], rownames=['True Class (ENA24)'], colnames=['Predicted Class (COCO)'])\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(confusion_crosstab, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix: ENA24 True Class vs. YOLO Predicted Class')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
