{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f0a7e2",
   "metadata": {},
   "source": [
    "# Object Detection with YOLO\n",
    "\n",
    "This notebook demonstrates how to use the YOLO model for object detection on the IDLE-OO-Camera-Traps dataset.\n",
    "\n",
    "First, let's install the necessary libraries.\n",
    "\n",
    "```python\n",
    "!pip install -q transformers datasets torch torchvision Pillow ultralytics scikit-learn\n",
    "```\n",
    "\n",
    "Now, let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from IPython.display import display,HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ddd787",
   "metadata": {},
   "source": [
    "## YOLO Object Detection\n",
    "\n",
    "First, we'll use a pretrained YOLOv8 model to perform object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e28a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLO model\n",
    "model_yolo = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bb05f6",
   "metadata": {},
   "source": [
    "Next, we load the `imageomics/IDLE-OO-Camera-Traps` dataset. We'll just take one example from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b661b0",
   "metadata": {
    "label": "load-image-cell"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=\"./data/IDLE-OO-Camera-Traps\", split=\"test\")\n",
    "iterator = iter(dataset)\n",
    "sample = next(iterator)\n",
    "image = sample[\"image\"]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb48b3f",
   "metadata": {},
   "source": [
    "Now we can run the YOLO model on the image. We'll test three different confidence thresholds: 0.5, 0.1, and 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_thresholds = [0.5, 0.1, 0.001]\n",
    "\n",
    "for conf in confidence_thresholds:\n",
    "    print(f\"Running YOLO detection with confidence threshold: {conf}\")\n",
    "    # Run inference on a copy of the image\n",
    "    results_yolo = model_yolo(image.copy(), conf=conf)\n",
    "\n",
    "    # Plot results\n",
    "    im_array = results_yolo[0].plot()\n",
    "    im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c607a6",
   "metadata": {},
   "source": [
    "## Exploring the ENA24 Dataset\n",
    "\n",
    "Instead of fine-tuning on a custom dataset, we will explore the `ENA24` dataset directly from the local checkout. This involves loading the existing labels, visualizing the frequency of common names, and displaying sample images for each unique common name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d381f5e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define the base path to the locally checked out dataset\n",
    "base_data_path = 'data/IDLE-OO-Camera-Traps/'\n",
    "ena24_csv_path = os.path.join(base_data_path, 'ENA24-balanced.csv')\n",
    "\n",
    "# Load the ENA24-balanced.csv file\n",
    "try:\n",
    "    ena24_df = pd.read_csv(ena24_csv_path)\n",
    "    print(f\"Successfully loaded {ena24_csv_path}\")\n",
    "    print(f\"Total images in ENA24 dataset: {len(ena24_df)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {ena24_csv_path} not found. Please ensure the dataset is correctly checked out.\")\n",
    "    ena24_df = pd.DataFrame() # Create an empty DataFrame to avoid further errors\n",
    "\n",
    "if not ena24_df.empty:\n",
    "    # --- Visualize \"common_name\" frequency ---\n",
    "    print(\"\\nVisualizing 'common_name' frequency...\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ena24_df['common_name'].value_counts().plot(kind='bar')\n",
    "    plt.title('Frequency of Common Names in ENA24 Dataset')\n",
    "    plt.xlabel('Common Name')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Display 2 images from each unique \"common_name\" ---\n",
    "    print(\"\\nDisplaying 2 images from each unique 'common_name'...\")\n",
    "    unique_common_names = ena24_df['common_name'].unique()\n",
    "    \n",
    "    for name in unique_common_names:\n",
    "        # Get up to 2 image paths for the current common name\n",
    "        sample_images = ena24_df[ena24_df['common_name'] == name].head(2)\n",
    "        \n",
    "        if not sample_images.empty:\n",
    "            print(f\"\\n--- Common Name: {name} ---\")\n",
    "            for index, row in sample_images.iterrows():\n",
    "                # Construct the full image path\n",
    "                # The 'filepath' column contains paths like 'ENA24/image_uuid.png'\n",
    "                # The actual images are under data/IDLE-OO-Camera-Traps/data/test/ENA24/\n",
    "                image_relative_path = row['filepath']\n",
    "                # Assuming the images are in data/IDLE-OO-Camera-Traps/data/test/\n",
    "                full_image_path = os.path.join(base_data_path, 'data/test/', image_relative_path)\n",
    "                \n",
    "                if os.path.exists(full_image_path):\n",
    "                    try:\n",
    "                        img = Image.open(full_image_path)\n",
    "                        display(img)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not load image {full_image_path}: {e}\")\n",
    "                else:\n",
    "                    print(f\"Image file not found: {full_image_path}\")\n",
    "else:\n",
    "    print(\"No data loaded from ENA24-balanced.csv, skipping visualization and image display.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61503bb",
   "metadata": {},
   "source": [
    "### YOLO Intermediate Layer Visualization\n",
    "\n",
    "Now, let's visualize the intermediate outputs of the YOLO model. We will add hooks to the model layers to capture the feature maps and then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692be486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list to store the feature maps\n",
    "feature_maps = []\n",
    "hooks = []\n",
    "\n",
    "# The hook function that saves the output of a layer\n",
    "def hook_fn(module, input, output):\n",
    "    feature_maps.append(output)\n",
    "\n",
    "# We will visualize the output of the first 10 layers of the YOLO model\n",
    "detection_model = model_yolo.model\n",
    "layers_to_hook = detection_model.model[:10]\n",
    "\n",
    "# Register a forward hook for each layer to be visualized\n",
    "for layer in layers_to_hook:\n",
    "    hooks.append(layer.register_forward_hook(hook_fn))\n",
    "\n",
    "# Run inference to trigger the hooks.\n",
    "# Make sure to clear feature_maps before running, as hooks are global.\n",
    "feature_maps = []\n",
    "results_yolo = model_yolo(image.copy())\n",
    "\n",
    "# Remove the hooks after inference\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Now, let's visualize the feature maps\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "num_layers = len(feature_maps)\n",
    "layer_names = [f\"Layer {i}: {type(layers_to_hook[i]).__name__}\" for i in range(num_layers)]\n",
    "\n",
    "# Plot the feature maps in a grid\n",
    "cols = 4\n",
    "rows = (num_layers + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (fm, name) in enumerate(zip(feature_maps, layer_names)):\n",
    "    # Detach the tensor from the computation graph and move it to the CPU\n",
    "    fm = fm.detach().cpu()\n",
    "    # Get the feature map for the first image in the batch\n",
    "    fm = fm[0]\n",
    "    \n",
    "    C, H, W = fm.shape\n",
    "    \n",
    "    ax = axes[i]\n",
    "\n",
    "    # Use PCA to visualize the feature map's channel dimension\n",
    "    if C >= 3:\n",
    "        # Reshape for PCA: from (C, H, W) to (H*W, C)\n",
    "        data = fm.permute(1, 2, 0).reshape(H * W, C).numpy()\n",
    "        \n",
    "        # Apply PCA to reduce to 3 components for RGB visualization\n",
    "        pca = PCA(n_components=3)\n",
    "        pca_result = pca.fit_transform(data)\n",
    "        \n",
    "        # Reshape back to an image (H, W, 3)\n",
    "        pca_image = pca_result.reshape(H, W, 3)\n",
    "        \n",
    "        # Normalize each of the 3 principal components to the range [0, 1]\n",
    "        for c in range(3):\n",
    "            channel = pca_image[:, :, c]\n",
    "            min_val, max_val = channel.min(), channel.max()\n",
    "            if max_val > min_val:\n",
    "                pca_image[:, :, c] = (channel - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                pca_image[:, :, c] = 0\n",
    "        \n",
    "        ax.imshow(pca_image)\n",
    "        ax.set_title(name)\n",
    "    else:\n",
    "        # Fallback for layers with fewer than 3 channels: show the first channel in grayscale\n",
    "        ax.imshow(fm[0], cmap='gray')\n",
    "        ax.set_title(name + \" (grayscale)\")\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
