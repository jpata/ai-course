{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3308de0d",
   "metadata": {},
   "source": [
    "jupyter:\n",
    "  jupytext:\n",
    "    default_lexer: python\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.18.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc314109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mount the drive in colab to be able to share outputs across the notebooks\n",
    "import sys\n",
    "import os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    # Mount Google Drive to persist files and access data\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "    # Create a directory for the AI course in Google Drive\n",
    "    %mkdir -p /content/drive/MyDrive/ai-course\n",
    "    # Change the current working directory to the new course directory\n",
    "    %cd /content/drive/MyDrive/ai-course\n",
    "\n",
    "    # If the course repository doesn't exist, clone it from GitHub\n",
    "    if not os.path.exists('ai-course'):\n",
    "        !git clone https://github.com/jpata/ai-course\n",
    "    \n",
    "    # Change directory into the cloned repository\n",
    "    %cd ai-course\n",
    "    # Pull the latest changes from the repository to ensure it's up-to-date\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d1fb91",
   "metadata": {},
   "source": [
    "# Object Detection: A Comparison of YOLO and DETR\n",
    "\n",
    "This notebook introduces and compares two prominent object detection architectures: YOLO (You Only Look Once) and DETR (Real-Time DEtection TRansformer). We will use pretrained models to perform inference on the ENA24 dataset and analyze their performance, paying special attention to the challenges posed by the mismatch between the models' training classes (COCO) and the dataset's actual classes.\n",
    "\n",
    "Object detection is a computer vision task that involves identifying and locating objects within an image. A model performing this task returns a set of bounding boxes, each with a corresponding class label for the object it contains.\n",
    "\n",
    "We will explore:\n",
    "*   **YOLO**: A leading family of single-stage detectors known for its speed and efficiency.\n",
    "*   **DETR**: A modern, transformer-based, end-to-end detector that provides high accuracy without requiring complex post-processing steps like Non-Maximum Suppression (NMS).\n",
    "*   **The ENA24 Dataset**: We will use the `imageomics/IDLE-OO-Camera-Traps` dataset to evaluate how well these models, pretrained on general-purpose datasets, perform on specialized data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063a9c5",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the necessary libraries. `ultralytics` provides the YOLO model, while `transformers` gives us access to DETR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66836386",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics transformers timm datasets torch torchvision Pillow scikit-learn seaborn pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9127d",
   "metadata": {},
   "source": [
    "We can also download the data from huggingface:\n",
    "```\n",
    "!mkdir ../data\n",
    "!git clone https://huggingface.co/datasets/imageomics/IDLE-OO-Camera-Traps ../data/IDLE-OO-Camera-Traps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ed5d9",
   "metadata": {},
   "source": [
    "Now, let's import all the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6df2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import cv2\n",
    "import tqdm\n",
    "\n",
    "# YOLO imports\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# DETR imports\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5e924",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset and a Sample Image\n",
    "\n",
    "We'll load the `imageomics/IDLE-OO-Camera-Traps` dataset from a local path and select one example from the test split to use for our initial inference examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54754b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../data/IDLE-OO-Camera-Traps/data/test/desert-lion/8b0146e9-3117-4d76-b61c-a8ead22e5755.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "print(f\"Loaded image: {image_path}\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b40970",
   "metadata": {},
   "source": [
    "## 3. Part 1: YOLO (You Only Look Once)\n",
    "\n",
    "The YOLO family of models are \"single-stage\" detectors, meaning they predict bounding boxes and class probabilities directly from the image in a single pass. They are famously fast and have become an industry standard for real-time object detection.\n",
    "\n",
    "### 3.1. Inference with YOLOv8\n",
    "\n",
    "The `ultralytics` library makes it incredibly simple to load a pretrained YOLO model and run inference. We'll use `yolov8n.pt`, a nano-sized version of the model that is fast and lightweight.\n",
    "\n",
    "The model is pretrained on the COCO dataset, a large-scale object detection dataset with 80 common object classes like \"person,\" \"car,\" and \"dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bd24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLOv8 model\n",
    "model_yolo = YOLO('../yolov8n.pt').to(device)\n",
    "\n",
    "# Run inference on a copy of the image, with a confidence threshold of 0.5\n",
    "# This means only detections with a confidence score > 0.5 will be returned\n",
    "results_yolo = model_yolo(image.copy(), conf=0.5)\n",
    "\n",
    "# The `plot()` method conveniently draws the detected boxes on the image\n",
    "im_array = results_yolo[0].plot()\n",
    "im_yolo = Image.fromarray(im_array[..., ::-1])  # Convert to RGB PIL image\n",
    "\n",
    "print(\"YOLOv8 Detections (Confidence > 0.5):\")\n",
    "display(im_yolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a8446",
   "metadata": {},
   "source": [
    "### 3.2. YOLO Architectural Deep Dive\n",
    "\n",
    "YOLO's architecture is a masterclass in efficiency, designed to perform detection in a single forward pass. It consists of three primary components: the Backbone, the Neck, and the Head.\n",
    "\n",
    "*   **Backbone (CSPDarknet):** The backbone is a deep Convolutional Neural Network (CNN) responsible for extracting features from the input image at various scales. It starts with a `Stem` layer for initial downsampling, followed by a series of convolutional blocks (`C2f` in YOLOv8). As the image passes through the backbone, its spatial dimensions (height and width) are reduced, while the number of channels (feature depth) is increased. This process creates a hierarchy of feature maps: early layers capture low-level features like edges and textures, while deeper layers capture high-level semantic features like object parts.\n",
    "\n",
    "*   **Neck (PANet):** The neck's job is to fuse the feature maps from the backbone to create a feature pyramid that is rich in both semantic (what) and localization (where) information. YOLOv8 uses a Path Aggregation Network (PANet). It takes feature maps from different stages of the backbone and combines them through both a top-down path (bringing high-level context to lower-level maps) and a bottom-up path (bringing precise localization information from lower-level maps to higher-level ones). This allows the model to effectively detect objects of different sizes.\n",
    "\n",
    "*   **Head (YOLOv8 Head):** The head is the final stage, responsible for making predictions. It takes the fused feature maps from the neck and uses a series of convolutions to predict three things for each location on the feature grid:\n",
    "    1.  **Bounding Box:** The coordinates (x, y, width, height) of a potential object.\n",
    "    2.  **Objectness Score:** A confidence score indicating how likely it is that an object exists at this location.\n",
    "    3.  **Class Probabilities:** A set of probabilities for each of the 80 COCO classes.\n",
    "\n",
    "#### Output Interpretation and NMS\n",
    "\n",
    "The raw output of the YOLO head is a massive tensor containing thousands of potential detections at different scales. To produce a clean, final list of objects, a critical post-processing step is required: **Non-Maximum Suppression (NMS)**. NMS works by:\n",
    "1.  Filtering out boxes with low confidence scores.\n",
    "2.  For each class, finding groups of overlapping boxes that likely correspond to the same object.\n",
    "3.  Within each group, suppressing (discarding) all boxes except the one with the highest confidence score.\n",
    "\n",
    "The `ultralytics` library handles all of this automatically when you call the model.\n",
    "\n",
    "#### Visualizing Intermediate Features\n",
    "\n",
    "To better understand what the model \"sees,\" we can extract the feature maps from intermediate layers and visualize them. We will use Principal Component Analysis (PCA) to reduce the high-dimensional channel information of a feature map into 3 components (RGB) for visualization.\n",
    "\n",
    "We'll grab features from three different points in the network:\n",
    "1.  An early backbone layer (`C2f_2`)\n",
    "2.  A later backbone layer (`C2f_4`)\n",
    "3.  The output of a neck layer (`C2f_6`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f25e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize a feature map using PCA\n",
    "def visualize_features_pca(feature_map, title):\n",
    "    # Detach from graph and move to CPU\n",
    "    features = feature_map.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # Reshape for PCA: (H*W, C)\n",
    "    # The input shape is (C, H, W), so we transpose it to (H, W, C) first\n",
    "    print(\"feat\", features.shape)\n",
    "    features = features.transpose(1, 2, 0)\n",
    "    h, w, c = features.shape\n",
    "    reshaped_features = features.reshape(-1, c)\n",
    "    \n",
    "    # Apply PCA to reduce channels to 3 components\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_result = pca.fit_transform(reshaped_features)\n",
    "    \n",
    "    # Normalize and reshape back to an image (H, W, 3)\n",
    "    pca_img = (pca_result - pca_result.min()) / (pca_result.max() - pca_result.min())\n",
    "    pca_img = pca_img.reshape(h, w, 3)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(pca_img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# --- YOLO Feature Extraction ---\n",
    "\n",
    "# Dictionary to store intermediate features\n",
    "yolo_features = {}\n",
    "\n",
    "# Hook function to capture the output of a module\n",
    "def get_yolo_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        # For C2f modules, the output might be a tuple/list, we take the first tensor\n",
    "        if isinstance(output, (list, tuple)):\n",
    "            yolo_features[name] = output[0].detach()\n",
    "        else:\n",
    "            yolo_features[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Mapping friendly names to actual module names in the YOLOv8 model structure\n",
    "# The indices [2], [4], [6] correspond to early backbone, late backbone, and neck layers\n",
    "yolo_layer_map = {\n",
    "    \"Early Backbone (C2f_2)\": model_yolo.model.model[2],\n",
    "    \"Mid Backbone (C2f_4)\": model_yolo.model.model[4],\n",
    "    \"Neck Features (C2f_6)\": model_yolo.model.model[6],\n",
    "}\n",
    "\n",
    "# Register forward hooks on the target layers\n",
    "hooks = []\n",
    "for name, layer in yolo_layer_map.items():\n",
    "    hooks.append(layer.register_forward_hook(get_yolo_hook(name)))\n",
    "\n",
    "# Run inference on the sample image to trigger the hooks\n",
    "# We use the original `image` from the dataset\n",
    "results_yolo = model_yolo(image.copy(), verbose=False)\n",
    "\n",
    "# Remove the hooks now that we have the features\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Visualize the captured features\n",
    "print(\"PCA Visualization of YOLOv8 Intermediate Features:\")\n",
    "for name, features in yolo_features.items():\n",
    "    visualize_features_pca(features, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c5fcd",
   "metadata": {},
   "source": [
    "## 4. Part 2: DETR (DEtection TRansformer)\n",
    "\n",
    "DETR (DEtection TRansformer) models reframe object detection as a direct set prediction problem. They use a transformer-based architecture to produce a fixed-size set of predictions, eliminating the need for complex post-processing like NMS. DETR is an evolution of this idea, optimized for real-time performance.\n",
    "\n",
    "### 4.1. Inference with DETR\n",
    "\n",
    "We will use the `transformers` library to load a pretrained DETR model. Unlike YOLO, DETR models require a specific `processor` to resize and normalize the input image correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor and a pretrained DETR model from Hugging Face\n",
    "# The `revision=\"no_timm\"` flag is used to ensure compatibility and avoid potential conflicts with the timm library.\n",
    "processor_detr = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "model_detr = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\").to(device)\n",
    "\n",
    "# Prepare the image for the model by resizing and normalizing it\n",
    "inputs = processor_detr(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model_detr(**inputs)\n",
    "\n",
    "# Post-process the results to get bounding boxes and class labels.\n",
    "# The `target_sizes` tensor is needed to scale the bounding boxes back to the original image size.\n",
    "# We set a low threshold to get all potential detections, which we will filter later.\n",
    "target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "results_detr = processor_detr.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.1)[0]\n",
    "\n",
    "# Helper function to draw bounding boxes on an image\n",
    "def draw_boxes(image, boxes, labels, scores):\n",
    "    img_draw = image.copy()\n",
    "    draw = ImageDraw.Draw(img_draw)\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        label_text = f\"{label} {score:.2f}\"\n",
    "        \n",
    "        draw.rectangle(box, outline=\"red\", width=2)\n",
    "        draw.text((box[0], box[1]), label_text, fill=\"red\")\n",
    "    return img_draw\n",
    "\n",
    "# Get labels, scores, and boxes for the top 3 detections\n",
    "scores = results_detr[\"scores\"]\n",
    "labels = results_detr[\"labels\"]\n",
    "boxes = results_detr[\"boxes\"]\n",
    "\n",
    "# Define how many of the top detections to show\n",
    "TOP_K = 3\n",
    "k = min(len(scores), TOP_K)\n",
    "\n",
    "# Get the top k scores and their corresponding indices\n",
    "scores, indices = torch.topk(scores, k)\n",
    "# Filter the labels and boxes to keep only the top k\n",
    "labels = labels[indices]\n",
    "boxes = boxes[indices]\n",
    "\n",
    "# Convert label IDs to human-readable class names\n",
    "labels = [model_detr.config.id2label[i.item()] for i in labels]\n",
    "\n",
    "# Draw the final boxes on the image\n",
    "im_detr = draw_boxes(image, boxes, labels, scores)\n",
    "\n",
    "print(f\"DETR Top {k} Detections:\")\n",
    "display(im_detr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d2614",
   "metadata": {},
   "source": [
    "### 4.2. DETR Architectural Deep Dive\n",
    "\n",
    "The DETR architecture introduced a paradigm shift by framing object detection as a direct set prediction problem, removing the need for many hand-designed components like NMS.\n",
    "\n",
    "*   **Backbone (ResNet):** It begins with a standard CNN backbone (a ResNet-50 in this case) to extract a 2D feature map from the input image. This feature map captures the essential spatial features.\n",
    "\n",
    "*   **Transformer Encoder:** This is where DETR diverges significantly.\n",
    "    *   **Input:** The feature map from the backbone is flattened into a sequence of tokens. Crucially, these tokens are combined with **Positional Encodings**, which are vectors that give the model information about the original `(x, y)` position of each token. Without this, the transformer would be unaware of the image's spatial structure.\n",
    "    *   **Function:** The encoder processes this sequence using multiple layers of self-attention. This allows every feature token to attend to every other token, building a rich, context-aware representation. The output is an enriched sequence of image features.\n",
    "\n",
    "*   **Transformer Decoder:** The decoder is the core of the prediction mechanism.\n",
    "    *   **Input:** It takes two main inputs: the memory of enriched features from the encoder, and a small, fixed-size set of learnable embeddings called **Object Queries**.\n",
    "    *   **Function:** Each object query acts as a \"slot\" responsible for detecting a single object. Through layers of self-attention and cross-attention, the queries interact with each other (to avoid duplicates) and with the encoder's output (to find and localize objects). Each query \"asks\" the image features: \"Is there an object here that matches my pattern?\"\n",
    "    \n",
    "*   **Prediction Heads (FFNs):** After the final decoder layer, each output query embedding is passed to two separate Feed-Forward Networks (FFNs):\n",
    "    1.  A **classification head** predicts the class label for that query (e.g., 'bird', 'car', or 'no object').\n",
    "    2.  A **box head** predicts the bounding box coordinates `(center_x, center_y, width, height)`.\n",
    "\n",
    "#### End-to-End Philosophy\n",
    "\n",
    "This design is \"end-to-end\" because it directly outputs a sparse set of predictions. Since each query is encouraged to specialize on a different object, the model learns to avoid making duplicate predictions for the same object, thus eliminating the need for NMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1086bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: INFERENCE WITH ATTENTIONS ---\n",
    "# We must request output_attentions=True to get the internal weights\n",
    "print(\"Running inference with attentions...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model_detr(**inputs, output_attentions=True)\n",
    "    \n",
    "    # 1. Get Probabilities and Top K\n",
    "    # Convert logits to probabilities and remove the \"no object\" class\n",
    "    probs = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "    # For each of the 100 object queries, find the class with the highest probability\n",
    "    max_scores, class_ids = probs.max(-1)\n",
    "    # Select the top 3 queries with the highest overall confidence scores\n",
    "    TOP_K = 3\n",
    "    top_scores, top_idxs = torch.topk(max_scores, TOP_K)\n",
    "\n",
    "# --- STEP 2: PROCESS ATTENTION MAPS ---\n",
    "print(\"Generating Cross-Attention Maps...\")\n",
    "\n",
    "# Get the Cross-Attentions from the LAST decoder layer\n",
    "# Shape: (Batch, Num_Heads, Num_Queries, Sequence_Length)\n",
    "# Sequence_Length = Feature_Height * Feature_Width (usually H/32 * W/32)\n",
    "cross_attentions = outputs.cross_attentions[-1]\n",
    "\n",
    "# Get input image dimensions\n",
    "img_h, img_w = inputs['pixel_values'].shape[-2:]\n",
    "\n",
    "# Prepare background image for visualization by un-normalizing it\n",
    "img_tensor = inputs['pixel_values'][0].detach().cpu()\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "rgb_img_base = (img_tensor * std + mean).permute(1, 2, 0).numpy()\n",
    "rgb_img_base = np.clip(rgb_img_base, 0, 1)\n",
    "\n",
    "# Setup Plot\n",
    "fig, axes = plt.subplots(1, TOP_K, figsize=(20, 6))\n",
    "if TOP_K == 1: axes = [axes]\n",
    "\n",
    "# Iterate through the top K queries and visualize their attention maps\n",
    "for i, query_idx_tensor in enumerate(top_idxs):\n",
    "    query_idx = query_idx_tensor.item()\n",
    "    class_id = class_ids[query_idx].item()\n",
    "    score = top_scores[i].item()\n",
    "    label_name = model_detr.config.id2label[class_id]\n",
    "\n",
    "    print(f\"Rank {i+1}: {label_name} | Score: {score:.2f} | Query Index: {query_idx}\")\n",
    "\n",
    "    # 1. Extract Attention for this specific Query\n",
    "    # Shape: [Batch, Heads, Queries, Seq] -> [Heads, Seq]\n",
    "    attn_map = cross_attentions[0, :, query_idx, :]\n",
    "    \n",
    "    # 2. Average over Attention Heads to get a single map\n",
    "    # Shape: [Seq]\n",
    "    attn_map = attn_map.mean(dim=0).detach().cpu()\n",
    "\n",
    "    # 3. Reshape Sequence back to 2D Feature Map\n",
    "    # We infer the feature map size (H/32, W/32) from the original image size\n",
    "    feat_h = int(np.ceil(img_h / 32))\n",
    "    feat_w = int(np.ceil(img_w / 32))\n",
    "    \n",
    "    # Handle potential mismatch due to padding/rounding during feature extraction\n",
    "    num_patches = feat_h * feat_w\n",
    "    # DETR sometimes pads the mask, so we strictly reshape to what matches the sequence length\n",
    "    if attn_map.shape[0] != num_patches:\n",
    "       # Fallback for edge cases: assume roughly square aspect ratio\n",
    "       side = int(attn_map.shape[0]**0.5)\n",
    "       feat_h, feat_w = side, side\n",
    "\n",
    "    attn_map = attn_map.reshape(feat_h, feat_w).numpy()\n",
    "\n",
    "    # 4. Process for Visualization\n",
    "    # Upscale the small attention map to the original image size for overlay\n",
    "    attn_map = cv2.resize(attn_map, (img_w, img_h))\n",
    "    \n",
    "    # Normalize the map to a 0-1 range for visualization\n",
    "    attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
    "    \n",
    "    # Apply a colormap to create a heatmap\n",
    "    heatmap = np.uint8(255 * attn_map)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Overlay the heatmap on the original image\n",
    "    visualization = cv2.addWeighted(np.uint8(255 * rgb_img_base), 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    # 5. Draw the Predicted Bounding Box\n",
    "    pred_box = outputs.pred_boxes[0, query_idx].detach().cpu().numpy()\n",
    "    # Convert normalized box coordinates [cx, cy, w, h] to pixel coordinates [x1, y1, x2, y2]\n",
    "    cx, cy, w_box, h_box = pred_box\n",
    "    x1 = int((cx - w_box / 2) * img_w)\n",
    "    y1 = int((cy - h_box / 2) * img_h)\n",
    "    x2 = int((cx + w_box / 2) * img_w)\n",
    "    y2 = int((cy + h_box / 2) * img_h)\n",
    "\n",
    "    cv2.rectangle(visualization, (x1, y1), (x2, y2), (255, 255, 255), 2)\n",
    "    \n",
    "    # Add the class label and score to the visualization\n",
    "    label_text = f\"{label_name} {score:.2f}\"\n",
    "    cv2.putText(visualization, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "    axes[i].imshow(visualization)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Rank {i+1}: {label_name}\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c0026",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis on the ENA24 Dataset\n",
    "\n",
    "Both models are powerful, but how do they perform on a real-world, specialized dataset like ENA24? A key challenge is that they were trained on COCO's 80 classes, which do not directly align with the animal species in ENA24.\n",
    "\n",
    "### 5.1. The Class Mismatch Problem\n",
    "\n",
    "Let's look at the classes in the ENA24 dataset. We expect the models to either fail to detect anything or to classify an ENA24 animal as a \"related\" COCO class (e.g., classifying a \"deer\" as a \"cow\" or \"horse\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ac98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base path and load the ENA24 metadata\n",
    "base_data_path = '../data/IDLE-OO-Camera-Traps/'\n",
    "ena24_csv_path = os.path.join(base_data_path, 'ENA24-balanced.csv')\n",
    "ena24_df = pd.read_csv(ena24_csv_path)\n",
    "\n",
    "# --- Visualize \"common_name\" frequency ---\n",
    "print(\"Visualizing 'common_name' frequency in the ENA24 Dataset:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "ena24_df['common_name'].value_counts().plot(kind='bar')\n",
    "plt.title('Frequency of Common Names in ENA24 Dataset')\n",
    "plt.xlabel('Common Name')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aff3e2",
   "metadata": {},
   "source": [
    "### 5.2. Performance Evaluation\n",
    "\n",
    "We will now iterate through a sample of the ENA24 dataset, run both models on each image, and record the ground truth class and the top predicted COCO class. This will allow us to see which COCO classes the models associate with the ENA24 animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52903c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store true labels and predicted labels for both models\n",
    "y_true = []\n",
    "y_pred_yolo = []\n",
    "y_pred_detr = []\n",
    "\n",
    "# Let's take a few images per class for our analysis\n",
    "num_samples_per_class = 5\n",
    "unique_common_names = ena24_df['common_name'].unique()\n",
    "\n",
    "# Iterate over each unique animal class in the dataset\n",
    "for name in unique_common_names:\n",
    "    # Get a small sample of images for the current class\n",
    "    sample_images = ena24_df[ena24_df['common_name'] == name].head(num_samples_per_class)\n",
    "    \n",
    "    # Process each image in the sample\n",
    "    for index, row in sample_images.iterrows():\n",
    "        image_relative_path = row['filepath']\n",
    "        full_image_path = os.path.join(base_data_path, 'data/test/', image_relative_path)\n",
    "        \n",
    "        if os.path.exists(full_image_path):\n",
    "            try:\n",
    "                img = Image.open(full_image_path).convert(\"RGB\")\n",
    "                # Store the ground truth class name\n",
    "                y_true.append(name)\n",
    "\n",
    "                # --- Run YOLO detection ---\n",
    "                # Run inference with a confidence threshold of 0.25\n",
    "                results_yolo = model_yolo(img.copy(), conf=0.25, verbose=False)\n",
    "                # Check if any objects were detected\n",
    "                if len(results_yolo[0].boxes) > 0:\n",
    "                    # Get the class ID of the top prediction\n",
    "                    top_pred_id = int(results_yolo[0].boxes.cls[0].item())\n",
    "                    # Get the corresponding class name\n",
    "                    predicted_name = model_yolo.names[top_pred_id]\n",
    "                    y_pred_yolo.append(predicted_name)\n",
    "                else:\n",
    "                    # If no objects are detected, record it\n",
    "                    y_pred_yolo.append(\"No detection\")\n",
    "\n",
    "                # --- Run DETR detection ---\n",
    "                inputs = processor_detr(images=img, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model_detr(**inputs)\n",
    "                \n",
    "                # Post-process the results with a confidence threshold of 0.25\n",
    "                target_sizes = torch.tensor([img.size[::-1]]).to(device)\n",
    "                results_detr = processor_detr.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.25)[0]\n",
    "                \n",
    "                # Check if any objects were detected\n",
    "                if len(results_detr[\"scores\"]) > 0:\n",
    "                    # Get the class ID of the top prediction\n",
    "                    top_pred_id = results_detr[\"labels\"][0].item()\n",
    "                    # Get the corresponding class name\n",
    "                    predicted_name = model_detr.config.id2label[top_pred_id]\n",
    "                    y_pred_detr.append(predicted_name)\n",
    "                else:\n",
    "                    # If no objects are detected, record it\n",
    "                    y_pred_detr.append(\"No detection\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not process image {full_image_path}: {e}\")\n",
    "\n",
    "print(\"Finished collecting predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2f624",
   "metadata": {},
   "source": [
    "### 5.3. Visualization with a Confusion Matrix\n",
    "\n",
    "A standard confusion matrix compares predictions against true labels when the classes are the same. Since our classes are different, we'll use a **cross-tabulation** (crosstab) to visualize the relationship between the true ENA24 classes and the predicted COCO classes from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64769f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOLO Crosstab ---\n",
    "df_yolo = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred_yolo})\n",
    "yolo_crosstab = pd.crosstab(df_yolo['y_true'], df_yolo['y_pred'], rownames=['True Class (ENA24)'], colnames=['Predicted Class (YOLO/COCO)'])\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(yolo_crosstab, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('ENA24 True Class vs. YOLO Predicted COCO Class')\n",
    "plt.show()\n",
    "\n",
    "# --- DETR Crosstab ---\n",
    "df_detr = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred_detr})\n",
    "detr_crosstab = pd.crosstab(df_detr['y_true'], df_detr['y_pred'], rownames=['True Class (ENA24)'], colnames=['Predicted Class (DETR/COCO)'])\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(detr_crosstab, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('ENA24 True Class vs. DETR Predicted COCO Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d318554",
   "metadata": {},
   "source": [
    "### 5.4. Evaluating with a ROC Curve\n",
    "\n",
    "To dig deeper into model performance, we can generate a Receiver Operating Characteristic (ROC) curve. This will help us quantify how well each model can distinguish between a target class and other classes based on its confidence scores.\n",
    "\n",
    "Since the ENA24 `bird` class has a direct counterpart in the COCO dataset, we can formulate a binary classification problem:\n",
    "*   **Positive Class**: Images from ENA24 labeled as `bird`.\n",
    "*   **Negative Class**: Images of all other animals in our ENA24 sample.\n",
    "*   **Prediction Score**: For each image, we take the highest score that a model assigns to the COCO `bird` class. For DETR, this is across all its object queries. For YOLO, this is across all its potential detections.\n",
    "\n",
    "The resulting ROC curves will show the trade-off between the True Positive Rate (correctly identifying birds) and the False Positive Rate (incorrectly labeling other animals as birds) at various confidence thresholds for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions for ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# We will evaluate both models' ability to distinguish the 'bird' class from other animals.\n",
    "# Positives: Images of birds from ENA24\n",
    "# Negatives: Images of other animals from ENA24\n",
    "# Score: The model's maximum confidence score for the COCO 'bird' class.\n",
    "\n",
    "ground_truth = []\n",
    "prediction_scores_detr = []\n",
    "prediction_scores_yolo = []\n",
    "\n",
    "# --- Find class IDs for 'bird' in both models ---\n",
    "detr_bird_class_id = None\n",
    "# Iterate through the DETR model's class labels to find the ID for 'bird'\n",
    "for k, v in model_detr.config.id2label.items():\n",
    "    if v == 'bird':\n",
    "        detr_bird_class_id = k\n",
    "        break\n",
    "\n",
    "yolo_bird_class_id = None\n",
    "# Iterate through the YOLO model's class names to find the ID for 'bird'\n",
    "for k, v in model_yolo.names.items():\n",
    "    if v == 'bird':\n",
    "        yolo_bird_class_id = k\n",
    "        break\n",
    "\n",
    "print(f\"COCO 'bird' class ID (DETR): {detr_bird_class_id}\")\n",
    "print(f\"COCO 'bird' class ID (YOLO): {yolo_bird_class_id}\")\n",
    "\n",
    "if detr_bird_class_id is None or yolo_bird_class_id is None:\n",
    "    raise Exception(\"Bird class not found in one of the models\")\n",
    "\n",
    "print(\"Generating scores for ROC curve...\")\n",
    "# Use tqdm to show a progress bar while iterating through classes\n",
    "for name in tqdm.tqdm(unique_common_names):\n",
    "    # Using a larger sample for a more robust curve\n",
    "    sample_images = ena24_df[ena24_df['common_name'] == name].head(20)\n",
    "    \n",
    "    for index, row in sample_images.iterrows():\n",
    "        image_relative_path = row['filepath']\n",
    "        full_image_path = os.path.join(base_data_path, 'data/test/', image_relative_path)\n",
    "        \n",
    "        if os.path.exists(full_image_path):\n",
    "            try:\n",
    "                img = Image.open(full_image_path).convert(\"RGB\")\n",
    "                \n",
    "                # Define ground truth: 1 if the image contains a bird, 0 otherwise\n",
    "                is_bird = 1 if name in ['american crow', 'domestic chicken', 'wild turkey'] else 0\n",
    "                ground_truth.append(is_bird)\n",
    "\n",
    "                # --- Get DETR score for the 'bird' class ---\n",
    "                inputs = processor_detr(images=img.copy(), return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model_detr(**inputs)\n",
    "                \n",
    "                # Get probabilities for all classes and queries\n",
    "                probs = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "                # Specifically get the scores for the 'bird' class across all queries\n",
    "                bird_scores = probs[:, detr_bird_class_id]\n",
    "                # The final score is the maximum 'bird' score among all queries\n",
    "                max_bird_score_detr = bird_scores.max().item()\n",
    "                prediction_scores_detr.append(max_bird_score_detr)\n",
    "\n",
    "                # --- Get YOLO score for the 'bird' class ---\n",
    "                # We run with a very low confidence threshold to get all potential boxes\n",
    "                results_yolo = model_yolo(img.copy(), conf=0.01, verbose=False)\n",
    "                boxes = results_yolo[0].boxes\n",
    "                \n",
    "                # Find all detections corresponding to the 'bird' class\n",
    "                bird_detections = boxes[boxes.cls == yolo_bird_class_id]\n",
    "                \n",
    "                if len(bird_detections) > 0:\n",
    "                    # Get the highest confidence score among all 'bird' detections\n",
    "                    max_bird_score_yolo = bird_detections.conf.max().item()\n",
    "                else:\n",
    "                    # If no bird was detected, the score is 0\n",
    "                    max_bird_score_yolo = 0.0\n",
    "                prediction_scores_yolo.append(max_bird_score_yolo)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not process image {full_image_path}: {e}\")\n",
    "\n",
    "print(\"Finished generating scores.\")\n",
    "\n",
    "# --- Calculate ROC Curve for DETR ---\n",
    "# roc_curve computes the receiver operating characteristic curve, or ROC curve.\n",
    "# It returns the false positive rate (fpr), true positive rate (tpr), and the thresholds used to calculate them.\n",
    "fpr_detr, tpr_detr, thresholds_detr = roc_curve(ground_truth, prediction_scores_detr)\n",
    "# auc (Area Under Curve) computes the area under the ROC curve, a single value summarizing the model's performance.\n",
    "roc_auc_detr = auc(fpr_detr, tpr_detr)\n",
    "\n",
    "# --- Calculate ROC Curve for YOLO ---\n",
    "fpr_yolo, tpr_yolo, thresholds_yolo = roc_curve(ground_truth, prediction_scores_yolo)\n",
    "roc_auc_yolo = auc(fpr_yolo, tpr_yolo)\n",
    "\n",
    "# --- Find points at confidence 0.9 ---\n",
    "# Find the index where the threshold is closest to 0.9 for DETR\n",
    "idx_detr = np.abs(thresholds_detr - 0.9).argmin()\n",
    "fpr_at_0_9_detr = fpr_detr[idx_detr]\n",
    "tpr_at_0_9_detr = tpr_detr[idx_detr]\n",
    "\n",
    "# Find the index where the threshold is closest to 0.9 for YOLO\n",
    "idx_yolo = np.abs(thresholds_yolo - 0.9).argmin()\n",
    "fpr_at_0_9_yolo = fpr_yolo[idx_yolo]\n",
    "tpr_at_0_9_yolo = tpr_yolo[idx_yolo]\n",
    "\n",
    "\n",
    "# --- Plot Both ROC Curves ---\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr_detr, tpr_detr, color='darkorange', lw=2, label=f'DETR ROC curve (area = {roc_auc_detr:.2f})')\n",
    "plt.plot(fpr_yolo, tpr_yolo, color='cornflowerblue', lw=2, label=f'YOLO ROC curve (area = {roc_auc_yolo:.2f})')\n",
    "# Plot the diagonal line representing a random guesser\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "\n",
    "# Plot the specific points at confidence 0.9 for both models\n",
    "plt.scatter(fpr_at_0_9_detr, tpr_at_0_9_detr, color='darkorange', marker='o', s=100,\n",
    "            label=f'DETR @ 0.9 conf (FPR: {fpr_at_0_9_detr:.2f}, TPR: {tpr_at_0_9_detr:.2f})', zorder=5)\n",
    "plt.scatter(fpr_at_0_9_yolo, tpr_at_0_9_yolo, color='cornflowerblue', marker='o', s=100,\n",
    "            label=f'YOLO @ 0.9 conf (FPR: {fpr_at_0_9_yolo:.2f}, TPR: {tpr_at_0_9_yolo:.2f})', zorder=5)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for \"bird\" class detection on ENA24')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014da70",
   "metadata": {},
   "source": [
    "From the heatmaps, we can see how the models perform. For example, both models often correctly map the ENA24 `bird` class to the COCO `bird` class. However, for species not in COCO, like `red_deer` or `wild_boar`, the models might predict related classes like `cow`, `horse`, or `dog`, or simply fail to make a detection. This analysis highlights the limitations of using pretrained models on novel domains and underscores the need for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5c464",
   "metadata": {},
   "source": [
    "## 6. Head-to-Head Comparison: YOLO vs. DETR\n",
    "\n",
    "| Feature               | YOLOv8                                        | DETR                                                       |\n",
    "| --------------------- | --------------------------------------------- | ---------------------------------------------------------- |\n",
    "| **Architecture**      | CNN-based (CSPDarknet Backbone, PANet Neck)   | Hybrid (CNN Backbone + Transformer Encoder/Decoder)        |\n",
    "| **Prediction**        | Predicts on a dense grid across the image     | Predicts a sparse set of objects via object queries        |\n",
    "| **Post-processing**   | Requires Non-Maximum Suppression (NMS)        | End-to-end; minimal or no NMS required                     |\n",
    "| **Ease of Use**       | Very simple via `ultralytics` library         | More complex; requires manual processing via `transformers`|\n",
    "| **Core Idea**         | Fast, single-stage regression and classification | Direct set prediction, treating detection as a dictionary lookup |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb92ee",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we explored two state-of-the-art object detection models.\n",
    "*   **YOLOv8** is incredibly fast and easy to use, making it an excellent choice for real-time applications where speed is critical. Its reliance on NMS is a defining characteristic of single-stage detectors.\n",
    "*   **DETR** represents a newer paradigm, using a transformer architecture to perform end-to-end detection. This removes the need for hand-tuned components like NMS and can lead to better performance, though often at the cost of higher computational requirements and implementation complexity.\n",
    "\n",
    "Our analysis on the ENA24 dataset showed that while pretrained models are a fantastic starting point, their performance on specialized domains is limited by their training data. To achieve high accuracy on ENA24's specific animal classes, the clear next step is **fine-tuning**, where we would train these models further on the ENA24 data itself."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
