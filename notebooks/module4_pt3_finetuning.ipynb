{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mount the drive in colab to be able to share outputs across the notebooks\n",
    "import sys\n",
    "import os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "    %mkdir -p /content/drive/MyDrive/ai-course\n",
    "    %cd /content/drive/MyDrive/ai-course\n",
    "\n",
    "    if not os.path.exists('ai-course'):\n",
    "        !git clone https://github.com/jpata/ai-course\n",
    "    \n",
    "    %cd ai-course\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aad9f6",
   "metadata": {},
   "source": [
    "# YOLO Fine-Tuning\n",
    "\n",
    "This notebook demonstrates how to fine-tune a YOLO model on a custom dataset. The dataset was labeled automatically using the OWL2 model in the `module4_owl2_object_detection.md` notebook.\n",
    "\n",
    "First, let's install the necessary libraries.\n",
    "\n",
    "```python\n",
    "#!pip install -q ultralytics pandas pyyaml scikit-learn\n",
    "```\n",
    "\n",
    "Now, let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['YOLO_VERBOSE'] = 'False'\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013c631",
   "metadata": {},
   "source": [
    "## Create Dataset YAML\n",
    "\n",
    "YOLO models require a `dataset.yaml` file that specifies the dataset paths and class names. We will create this file now. The labels were generated by the OWL2 notebook and are located in `data/IDLE-OO-Camera-Traps/labels/test`. The corresponding images are in `data/IDLE-OO-Camera-Traps/data/test`.\n",
    "\n",
    "**Note:** The `module4_owl2_object_detection.md` notebook generates labels for a sample of 500 images. Fine-tuning on a small dataset (like 500 images, which is still relatively small for object detection) will not produce a robust model, but it demonstrates the process. For better results, you should generate labels for a larger portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ba4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Step 1: Find all labeled images and their corresponding labels ---\n",
    "# Define base paths for the YOLO dataset directory\n",
    "base_path = os.path.abspath('../data/IDLE-OO-Camera-Traps_yolo')\n",
    "labels_dir = os.path.join(base_path, 'labels')\n",
    "images_dir = os.path.join(base_path, 'images')\n",
    "train_file_path = os.path.join(base_path, 'train.txt')\n",
    "val_file_path = os.path.join(base_path, 'val.txt')\n",
    "image_files = []\n",
    "\n",
    "# Use glob to recursively find all .txt label files, excluding 'classes.txt'\n",
    "label_files = [f for f in glob.glob(os.path.join(labels_dir, '**/*.txt'), recursive=True) if os.path.basename(f) != 'classes.txt']\n",
    "\n",
    "# For each label file, find the corresponding image file\n",
    "for label_file in label_files:\n",
    "    # Construct the expected image path by replacing 'labels' with 'images' and '.txt' with '.png'\n",
    "    # This assumes a parallel directory structure and matching filenames.\n",
    "    image_path = label_file.replace(\"/labels/\", \"/images/\").replace(\".txt\", \".png\")\n",
    "    # If the corresponding image exists, add it to our list of available image files\n",
    "    if os.path.exists(image_path):\n",
    "        image_files.append(image_path)\n",
    "\n",
    "if not image_files:\n",
    "    raise Exception(\"No labeled images found. 'train.txt' and 'val.txt' were not created.\")\n",
    "\n",
    "# --- Step 2: Split data into training and validation sets ---\n",
    "# Use scikit-learn's train_test_split to randomly divide the image files.\n",
    "# 80% of the data will be for training, 20% for validation.\n",
    "# `random_state` ensures the split is the same every time the code is run.\n",
    "train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Step 3: Create train.txt and val.txt files ---\n",
    "# YOLO can use .txt files that list the absolute paths to training and validation images.\n",
    "# Write the paths of the training images to train.txt\n",
    "with open(train_file_path, 'w') as f:\n",
    "    for image_path in train_images:\n",
    "        f.write(f\"{image_path}\\n\")\n",
    "print(f\"Found {len(image_files)} labeled images.\")\n",
    "print(f\"Created '{train_file_path}' with {len(train_images)} images for training.\")\n",
    "\n",
    "# Write the paths of the validation images to val.txt\n",
    "with open(val_file_path, 'w') as f:\n",
    "    for image_path in val_images:\n",
    "        f.write(f\"{image_path}\\n\")\n",
    "print(f\"Created '{val_file_path}' with {len(val_images)} images for validation.\")\n",
    "\n",
    "\n",
    "# --- Step 4: Create the dataset.yaml file ---\n",
    "# This configuration file tells YOLO where to find the dataset and what the class names are.\n",
    "dataset_config = {\n",
    "    'path': os.path.abspath(base_path), # The root directory of the dataset\n",
    "    'train': 'train.txt',               # Path to the training images list, relative to 'path'\n",
    "    'val': 'val.txt',                   # Path to the validation images list, relative to 'path'\n",
    "    'names': {}                         # A dictionary mapping class indices to class names\n",
    "}\n",
    "# Read the class names from the 'classes.txt' file created in the previous notebook.\n",
    "classes_path = os.path.join(os.path.dirname(labels_dir), 'classes.txt')\n",
    "with open(classes_path, 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "    # Create the mapping {0: 'class_a', 1: 'class_b', ...}\n",
    "    dataset_config['names'] = {i: name for i, name in enumerate(classes)}\n",
    "\n",
    "# Write the configuration dictionary to a YAML file.\n",
    "with open('ena24_yolo_dataset.yaml', 'w') as f:\n",
    "    yaml.dump(dataset_config, f)\n",
    "\n",
    "# Print the contents of the created YAML file for verification.\n",
    "print(\"\\nena24_yolo_dataset.yaml created:\")\n",
    "with open('ena24_yolo_dataset.yaml', 'r') as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664afdce",
   "metadata": {},
   "source": [
    "## Fine-Tune YOLO Model\n",
    "\n",
    "Now we can load a pretrained YOLO model and fine-tune it on our custom dataset. We'll use the `yolov8n.pt` model.\n",
    "\n",
    "If the `ena24_yolo_dataset.yaml` was created successfully, we can proceed with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406275d6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load a pretrained YOLO model\n",
    "model = YOLO('../yolov8n.pt')\n",
    "\n",
    "# Train the model on our custom dataset\n",
    "results = model.train(\n",
    "    data='ena24_yolo_dataset.yaml',  # Path to our dataset configuration file\n",
    "    epochs=100,                     # Number of times to iterate over the entire dataset\n",
    "    imgsz=640,                      # Resize all images to 640x640 before feeding them to the model\n",
    "    batch=8,                        # Number of images to process in a single batch\n",
    "    fliplr=0.5,                     # Augmentation: randomly flip images horizontally 50% of the time\n",
    "    translate=0.1,                  # Augmentation: randomly translate images by up to 10%\n",
    "    scale=0.5                       # Augmentation: randomly scale (zoom in/out) images by up to 50%\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29039aae",
   "metadata": {},
   "source": [
    "## Visualize Training and Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4612ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest training directory in the 'runs/detect' folder\n",
    "train_dir = 'runs/detect'\n",
    "latest_train_run = max(os.listdir(train_dir), key=lambda d: os.path.getmtime(os.path.join(train_dir, d)))\n",
    "# Construct the path to the results.csv file, which contains training metrics for each epoch\n",
    "results_csv_path = os.path.join(train_dir, latest_train_run, 'results.csv')\n",
    "\n",
    "print(f\"Loading training results from: {results_csv_path}\")\n",
    "# Load the training results into a pandas DataFrame\n",
    "results_df = pd.read_csv(results_csv_path)\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "# The total loss is the sum of three components:\n",
    "# - box_loss: Bounding box regression loss (how well the model predicts box coordinates)\n",
    "# - cls_loss: Classification loss (how well the model predicts the correct class)\n",
    "# - dfl_loss: Distribution Focal Loss (a more advanced loss for box regression)\n",
    "plt.plot(results_df['epoch'], results_df['train/box_loss']+results_df['train/cls_loss']+results_df['train/dfl_loss'], label='Train Loss')\n",
    "plt.plot(results_df['epoch'], results_df['val/box_loss']+results_df['val/cls_loss']+results_df['val/dfl_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
