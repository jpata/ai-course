{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d01bf06",
   "metadata": {},
   "source": [
    "jupyter:\n",
    "  jupytext:\n",
    "    default_lexer: python\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.18.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151f268",
   "metadata": {},
   "source": [
    "# Object Detection: A Comparison of YOLO and RT-DETR\n",
    "\n",
    "This notebook introduces and compares two prominent object detection architectures: YOLO (You Only Look Once) and RT-DETR (Real-Time DEtection TRansformer). We will use pretrained models to perform inference on the ENA24 dataset and analyze their performance, paying special attention to the challenges posed by the mismatch between the models' training classes (COCO) and the dataset's actual classes.\n",
    "\n",
    "Object detection is a computer vision task that involves identifying and locating objects within an image. A model performing this task returns a set of bounding boxes, each with a corresponding class label for the object it contains.\n",
    "\n",
    "We will explore:\n",
    "*   **YOLO**: A leading family of single-stage detectors known for its speed and efficiency.\n",
    "*   **RT-DETR**: A modern, transformer-based, end-to-end detector that provides high accuracy without requiring complex post-processing steps like Non-Maximum Suppression (NMS).\n",
    "*   **The ENA24 Dataset**: We will use the `imageomics/IDLE-OO-Camera-Traps` dataset to evaluate how well these models, pretrained on general-purpose datasets, perform on specialized data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87190b",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the necessary libraries. `ultralytics` provides the YOLO model, while `transformers` gives us access to RT-DETR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics transformers timm datasets torch torchvision Pillow scikit-learn seaborn pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda8a05",
   "metadata": {},
   "source": [
    "Now, let's import all the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd702c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# YOLO imports\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# DETR imports\n",
    "from transformers import DetrImageProcessor, RTDetrForObjectDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f70aa4",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset and a Sample Image\n",
    "\n",
    "We'll load the `imageomics/IDLE-OO-Camera-Traps` dataset from a local path and select one example from the test split to use for our initial inference examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96cb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=\"../data/IDLE-OO-Camera-Traps\", split=\"test\")\n",
    "iterator = iter(dataset)\n",
    "sample = next(iterator)\n",
    "image = sample[\"image\"]\n",
    "print(\"A sample image from the ENA24 dataset:\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390ad6b",
   "metadata": {},
   "source": [
    "## 3. Part 1: YOLO (You Only Look Once)\n",
    "\n",
    "The YOLO family of models are \"single-stage\" detectors, meaning they predict bounding boxes and class probabilities directly from the image in a single pass. They are famously fast and have become an industry standard for real-time object detection.\n",
    "\n",
    "### 3.1. Inference with YOLOv8\n",
    "\n",
    "The `ultralytics` library makes it incredibly simple to load a pretrained YOLO model and run inference. We'll use `yolov8n.pt`, a nano-sized version of the model that is fast and lightweight.\n",
    "\n",
    "The model is pretrained on the COCO dataset, a large-scale object detection dataset with 80 common object classes like \"person,\" \"car,\" and \"dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a25a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLOv8 model\n",
    "model_yolo = YOLO('../yolov8n.pt')\n",
    "\n",
    "# Run inference on a copy of the image\n",
    "results_yolo = model_yolo(image.copy(), conf=0.5)\n",
    "\n",
    "# The `plot()` method conveniently draws the detected boxes on the image\n",
    "im_array = results_yolo[0].plot()\n",
    "im_yolo = Image.fromarray(im_array[..., ::-1])  # Convert to RGB PIL image\n",
    "\n",
    "print(\"YOLOv8 Detections (Confidence > 0.5):\")\n",
    "display(im_yolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b9108d",
   "metadata": {},
   "source": [
    "### 3.2. YOLO Architectural Deep Dive\n",
    "\n",
    "YOLO's architecture can be broken down into three key parts:\n",
    "*   **Backbone**: A deep convolutional neural network (like CSPDarknet53 in YOLOv8) that extracts image features at different scales.\n",
    "*   **Neck**: This part (e.g., a PANet) merges and refines the feature maps from the backbone, creating a rich feature pyramid that helps detect objects of various sizes.\n",
    "*   **Head**: The detection head takes the feature maps from the neck and predicts bounding boxes, class probabilities, and an \"objectness\" score for each location on a predefined grid.\n",
    "\n",
    "A critical post-processing step for YOLO is **Non-Maximum Suppression (NMS)**. Because the model predicts many potential bounding boxes for the same object, NMS is used to discard redundant, overlapping boxes, keeping only the one with the highest confidence score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b648e",
   "metadata": {},
   "source": [
    "## 4. Part 2: RT-DETR (Real-Time DEtection TRansformer)\n",
    "\n",
    "DETR (DEtection TRansformer) models reframe object detection as a direct set prediction problem. They use a transformer-based architecture to produce a fixed-size set of predictions, eliminating the need for complex post-processing like NMS. RT-DETR is an evolution of this idea, optimized for real-time performance.\n",
    "\n",
    "### 4.1. Inference with RT-DETR\n",
    "\n",
    "We will use the `transformers` library to load a pretrained RT-DETR model. Unlike YOLO, DETR models require a specific `processor` to resize and normalize the input image correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor and a pretrained RT-DETR model from Hugging Face\n",
    "processor_detr = DetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")\n",
    "model_detr = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd\")\n",
    "\n",
    "# Prepare the image for the model\n",
    "inputs = processor_detr(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model_detr(**inputs)\n",
    "\n",
    "# Post-process the results to get bounding boxes and class labels\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results_detr = processor_detr.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n",
    "\n",
    "# Helper function to draw bounding boxes\n",
    "def draw_boxes(image, boxes, labels, scores):\n",
    "    img_draw = image.copy()\n",
    "    draw = ImageDraw.Draw(img_draw)\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        label_text = f\"{label} {score:.2f}\"\n",
    "        \n",
    "        draw.rectangle(box, outline=\"red\", width=2)\n",
    "        draw.text((box[0], box[1]), label_text, fill=\"red\")\n",
    "    return img_draw\n",
    "\n",
    "# Get labels, scores, and boxes\n",
    "labels = [model_detr.config.id2label[i.item()] for i in results_detr[\"labels\"]]\n",
    "scores = results_detr[\"scores\"]\n",
    "boxes = results_detr[\"boxes\"]\n",
    "\n",
    "# Draw the boxes on the image\n",
    "im_detr = draw_boxes(image, boxes, labels, scores)\n",
    "\n",
    "print(\"RT-DETR Detections (Confidence > 0.7):\")\n",
    "display(im_detr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33052573",
   "metadata": {},
   "source": [
    "### 4.2. RT-DETR Architectural Deep Dive\n",
    "\n",
    "The DETR architecture is fundamentally different from YOLO:\n",
    "*   **Backbone**: Like YOLO, it uses a standard CNN backbone (e.g., ResNet) to extract a feature map from the image.\n",
    "*   **Transformer Encoder-Decoder**: This is the core of the model.\n",
    "    *   The **Encoder** takes the image features and enriches them using self-attention mechanisms.\n",
    "    *   The **Decoder** takes a small, fixed number of learnable embeddings called **object queries**. Each query is responsible for finding one object in the image. The decoder uses attention to compare the object queries to the image features and outputs the final set of predictions (class and bounding box) for each query.\n",
    "\n",
    "This end-to-end philosophy means that each object is detected exactly once by one of the object queries. In its original form, this completely removes the need for NMS, simplifying the detection pipeline. RT-DETR introduces some optimizations that re-introduce an optional, efficient NMS-like step, but the core principle remains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f15460",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis on the ENA24 Dataset\n",
    "\n",
    "Both models are powerful, but how do they perform on a real-world, specialized dataset like ENA24? A key challenge is that they were trained on COCO's 80 classes, which do not directly align with the animal species in ENA24.\n",
    "\n",
    "### 5.1. The Class Mismatch Problem\n",
    "\n",
    "Let's look at the classes in the ENA24 dataset. We expect the models to either fail to detect anything or to classify an ENA24 animal as a \"related\" COCO class (e.g., classifying a \"deer\" as a \"cow\" or \"horse\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base path and load the ENA24 metadata\n",
    "base_data_path = '../data/IDLE-OO-Camera-Traps/'\n",
    "ena24_csv_path = os.path.join(base_data_path, 'ENA24-balanced.csv')\n",
    "ena24_df = pd.read_csv(ena24_csv_path)\n",
    "\n",
    "# --- Visualize \"common_name\" frequency ---\n",
    "print(\"Visualizing 'common_name' frequency in the ENA24 Dataset:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "ena24_df['common_name'].value_counts().plot(kind='bar')\n",
    "plt.title('Frequency of Common Names in ENA24 Dataset')\n",
    "plt.xlabel('Common Name')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d29132",
   "metadata": {},
   "source": [
    "### 5.2. Performance Evaluation\n",
    "\n",
    "We will now iterate through a sample of the ENA24 dataset, run both models on each image, and record the ground truth class and the top predicted COCO class. This will allow us to see which COCO classes the models associate with the ENA24 animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred_yolo = []\n",
    "y_pred_detr = []\n",
    "\n",
    "# Let's take a few images per class for our analysis\n",
    "num_samples_per_class = 5\n",
    "unique_common_names = ena24_df['common_name'].unique()\n",
    "\n",
    "for name in unique_common_names:\n",
    "    sample_images = ena24_df[ena24_df['common_name'] == name].head(num_samples_per_class)\n",
    "    \n",
    "    for index, row in sample_images.iterrows():\n",
    "        image_relative_path = row['filepath']\n",
    "        full_image_path = os.path.join(base_data_path, 'data/test/', image_relative_path)\n",
    "        \n",
    "        if os.path.exists(full_image_path):\n",
    "            try:\n",
    "                img = Image.open(full_image_path).convert(\"RGB\")\n",
    "                y_true.append(name)\n",
    "\n",
    "                # --- Run YOLO detection ---\n",
    "                results_yolo = model_yolo(img.copy(), conf=0.25, verbose=False)\n",
    "                if len(results_yolo[0].boxes) > 0:\n",
    "                    top_pred_id = int(results_yolo[0].boxes.cls[0].item())\n",
    "                    predicted_name = model_yolo.names[top_pred_id]\n",
    "                    y_pred_yolo.append(predicted_name)\n",
    "                else:\n",
    "                    y_pred_yolo.append(\"No detection\")\n",
    "\n",
    "                # --- Run RT-DETR detection ---\n",
    "                inputs = processor_detr(images=img, return_tensors=\"pt\")\n",
    "                with torch.no_grad():\n",
    "                    outputs = model_detr(**inputs)\n",
    "                \n",
    "                target_sizes = torch.tensor([img.size[::-1]])\n",
    "                results_detr = processor_detr.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.25)[0]\n",
    "                \n",
    "                if len(results_detr[\"scores\"]) > 0:\n",
    "                    top_pred_id = results_detr[\"labels\"][0].item()\n",
    "                    predicted_name = model_detr.config.id2label[top_pred_id]\n",
    "                    y_pred_detr.append(predicted_name)\n",
    "                else:\n",
    "                    y_pred_detr.append(\"No detection\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not process image {full_image_path}: {e}\")\n",
    "\n",
    "print(\"Finished collecting predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30102628",
   "metadata": {},
   "source": [
    "### 5.3. Visualization with a Confusion Matrix\n",
    "\n",
    "A standard confusion matrix compares predictions against true labels when the classes are the same. Since our classes are different, we'll use a **cross-tabulation** (crosstab) to visualize the relationship between the true ENA24 classes and the predicted COCO classes from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb7a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOLO Crosstab ---\n",
    "df_yolo = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred_yolo})\n",
    "yolo_crosstab = pd.crosstab(df_yolo['y_true'], df_yolo['y_pred'], rownames=['True Class (ENA24)'], colnames=['Predicted Class (YOLO/COCO)'])\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(yolo_crosstab, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('ENA24 True Class vs. YOLO Predicted COCO Class')\n",
    "plt.show()\n",
    "\n",
    "# --- RT-DETR Crosstab ---\n",
    "df_detr = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred_detr})\n",
    "detr_crosstab = pd.crosstab(df_detr['y_true'], df_detr['y_pred'], rownames=['True Class (ENA24)'], colnames=['Predicted Class (RT-DETR/COCO)'])\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(detr_crosstab, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('ENA24 True Class vs. RT-DETR Predicted COCO Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dada58",
   "metadata": {},
   "source": [
    "From the heatmaps, we can see how the models perform. For example, both models often correctly map the ENA24 `bird` class to the COCO `bird` class. However, for species not in COCO, like `red_deer` or `wild_boar`, the models might predict related classes like `cow`, `horse`, or `dog`, or simply fail to make a detection. This analysis highlights the limitations of using pretrained models on novel domains and underscores the need for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb74794",
   "metadata": {},
   "source": [
    "## 6. Head-to-Head Comparison: YOLO vs. RT-DETR\n",
    "\n",
    "| Feature               | YOLOv8                                        | RT-DETR                                                    |\n",
    "| --------------------- | --------------------------------------------- | ---------------------------------------------------------- |\n",
    "| **Architecture**      | CNN-based (CSPDarknet Backbone, PANet Neck)   | Hybrid (CNN Backbone + Transformer Encoder/Decoder)        |\n",
    "| **Prediction**        | Predicts on a dense grid across the image     | Predicts a sparse set of objects via object queries        |\n",
    "| **Post-processing**   | Requires Non-Maximum Suppression (NMS)        | End-to-end; minimal or no NMS required                     |\n",
    "| **Ease of Use**       | Very simple via `ultralytics` library         | More complex; requires manual processing via `transformers`|\n",
    "| **Core Idea**         | Fast, single-stage regression and classification | Direct set prediction, treating detection as a dictionary lookup |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be9b25",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we explored two state-of-the-art object detection models.\n",
    "*   **YOLOv8** is incredibly fast and easy to use, making it an excellent choice for real-time applications where speed is critical. Its reliance on NMS is a defining characteristic of single-stage detectors.\n",
    "*   **RT-DETR** represents a newer paradigm, using a transformer architecture to perform end-to-end detection. This removes the need for hand-tuned components like NMS and can lead to better performance, though often at the cost of higher computational requirements and implementation complexity.\n",
    "\n",
    "Our analysis on the ENA24 dataset showed that while pretrained models are a fantastic starting point, their performance on specialized domains is limited by their training data. To achieve high accuracy on ENA24's specific animal classes, the clear next step is **fine-tuning**, where we would train these models further on the ENA24 data itself."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
