{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f154a5e",
   "metadata": {},
   "source": [
    "jupyter:\n",
    "  jupytext:\n",
    "    default_lexer: python\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.18.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a5d26",
   "metadata": {},
   "source": [
    "# Object Detection: A Comparison of YOLO and RT-DETR\n",
    "\n",
    "This notebook introduces and compares two prominent object detection architectures: YOLO (You Only Look Once) and RT-DETR (Real-Time DEtection TRansformer). We will use pretrained models to perform inference on the ENA24 dataset and analyze their performance, paying special attention to the challenges posed by the mismatch between the models' training classes (COCO) and the dataset's actual classes.\n",
    "\n",
    "Object detection is a computer vision task that involves identifying and locating objects within an image. A model performing this task returns a set of bounding boxes, each with a corresponding class label for the object it contains.\n",
    "\n",
    "We will explore:\n",
    "*   **YOLO**: A leading family of single-stage detectors known for its speed and efficiency.\n",
    "*   **RT-DETR**: A modern, transformer-based, end-to-end detector that provides high accuracy without requiring complex post-processing steps like Non-Maximum Suppression (NMS).\n",
    "*   **The ENA24 Dataset**: We will use the `imageomics/IDLE-OO-Camera-Traps` dataset to evaluate how well these models, pretrained on general-purpose datasets, perform on specialized data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42498565",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the necessary libraries. `ultralytics` provides the YOLO model, while `transformers` gives us access to RT-DETR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08a925",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics transformers timm datasets torch torchvision Pillow scikit-learn seaborn pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29758a0",
   "metadata": {},
   "source": [
    "Now, let's import all the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3597a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# YOLO imports\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# DETR imports\n",
    "from transformers import DetrImageProcessor, RTDetrForObjectDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2182b2",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset and a Sample Image\n",
    "\n",
    "We'll load the `imageomics/IDLE-OO-Camera-Traps` dataset from a local path and select one example from the test split to use for our initial inference examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b43197",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=\"../data/IDLE-OO-Camera-Traps\", split=\"test\")\n",
    "iterator = iter(dataset)\n",
    "sample = next(iterator)\n",
    "image = sample[\"image\"]\n",
    "print(\"A sample image from the ENA24 dataset:\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3682ae19",
   "metadata": {},
   "source": [
    "## 3. Part 1: YOLO (You Only Look Once)\n",
    "\n",
    "The YOLO family of models are \"single-stage\" detectors, meaning they predict bounding boxes and class probabilities directly from the image in a single pass. They are famously fast and have become an industry standard for real-time object detection.\n",
    "\n",
    "### 3.1. Inference with YOLOv8\n",
    "\n",
    "The `ultralytics` library makes it incredibly simple to load a pretrained YOLO model and run inference. We'll use `yolov8n.pt`, a nano-sized version of the model that is fast and lightweight.\n",
    "\n",
    "The model is pretrained on the COCO dataset, a large-scale object detection dataset with 80 common object classes like \"person,\" \"car,\" and \"dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e404f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLOv8 model\n",
    "model_yolo = YOLO('../yolov8n.pt')\n",
    "\n",
    "# Run inference on a copy of the image\n",
    "results_yolo = model_yolo(image.copy(), conf=0.5)\n",
    "\n",
    "# The `plot()` method conveniently draws the detected boxes on the image\n",
    "im_array = results_yolo[0].plot()\n",
    "im_yolo = Image.fromarray(im_array[..., ::-1])  # Convert to RGB PIL image\n",
    "\n",
    "print(\"YOLOv8 Detections (Confidence > 0.5):\")\n",
    "display(im_yolo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2396bb7",
   "metadata": {},
   "source": [
    "### 3.2. YOLO Architectural Deep Dive\n",
    "\n",
    "YOLO's architecture is a masterclass in efficiency, designed to perform detection in a single forward pass. It consists of three primary components: the Backbone, the Neck, and the Head.\n",
    "\n",
    "*   **Backbone (CSPDarknet):** The backbone is a deep Convolutional Neural Network (CNN) responsible for extracting features from the input image at various scales. It starts with a `Stem` layer for initial downsampling, followed by a series of convolutional blocks (`C2f` in YOLOv8). As the image passes through the backbone, its spatial dimensions (height and width) are reduced, while the number of channels (feature depth) is increased. This process creates a hierarchy of feature maps: early layers capture low-level features like edges and textures, while deeper layers capture high-level semantic features like object parts.\n",
    "\n",
    "*   **Neck (PANet):** The neck's job is to fuse the feature maps from the backbone to create a feature pyramid that is rich in both semantic (what) and localization (where) information. YOLOv8 uses a Path Aggregation Network (PANet). It takes feature maps from different stages of the backbone and combines them through both a top-down path (bringing high-level context to lower-level maps) and a bottom-up path (bringing precise localization information from lower-level maps to higher-level ones). This allows the model to effectively detect objects of different sizes.\n",
    "\n",
    "*   **Head (YOLOv8 Head):** The head is the final stage, responsible for making predictions. It takes the fused feature maps from the neck and uses a series of convolutions to predict three things for each location on the feature grid:\n",
    "    1.  **Bounding Box:** The coordinates (x, y, width, height) of a potential object.\n",
    "    2.  **Objectness Score:** A confidence score indicating how likely it is that an object exists at this location.\n",
    "    3.  **Class Probabilities:** A set of probabilities for each of the 80 COCO classes.\n",
    "\n",
    "#### Output Interpretation and NMS\n",
    "\n",
    "The raw output of the YOLO head is a massive tensor containing thousands of potential detections at different scales. To produce a clean, final list of objects, a critical post-processing step is required: **Non-Maximum Suppression (NMS)**. NMS works by:\n",
    "1.  Filtering out boxes with low confidence scores.\n",
    "2.  For each class, finding groups of overlapping boxes that likely correspond to the same object.\n",
    "3.  Within each group, suppressing (discarding) all boxes except the one with the highest confidence score.\n",
    "\n",
    "The `ultralytics` library handles all of this automatically when you call the model.\n",
    "\n",
    "#### Visualizing Intermediate Features\n",
    "\n",
    "To better understand what the model \"sees,\" we can extract the feature maps from intermediate layers and visualize them. We will use Principal Component Analysis (PCA) to reduce the high-dimensional channel information of a feature map into 3 components (RGB) for visualization.\n",
    "\n",
    "We'll grab features from three different points in the network:\n",
    "1.  An early backbone layer (`C2f_2`)\n",
    "2.  A later backbone layer (`C2f_4`)\n",
    "3.  The output of a neck layer (`C2f_6`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d027890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize a feature map using PCA\n",
    "def visualize_features_pca(feature_map, title):\n",
    "    # Detach from graph and move to CPU\n",
    "    features = feature_map.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # Reshape for PCA: (H*W, C)\n",
    "    # The input shape is (C, H, W), so we transpose it to (H, W, C) first\n",
    "    features = features.transpose(1, 2, 0)\n",
    "    h, w, c = features.shape\n",
    "    reshaped_features = features.reshape(-1, c)\n",
    "    \n",
    "    # Apply PCA to reduce channels to 3 components\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_result = pca.fit_transform(reshaped_features)\n",
    "    \n",
    "    # Normalize and reshape back to an image (H, W, 3)\n",
    "    pca_img = (pca_result - pca_result.min()) / (pca_result.max() - pca_result.min())\n",
    "    pca_img = pca_img.reshape(h, w, 3)\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(pca_img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# --- YOLO Feature Extraction ---\n",
    "\n",
    "# Dictionary to store intermediate features\n",
    "yolo_features = {}\n",
    "\n",
    "# Hook function to capture the output of a module\n",
    "def get_yolo_hook(name):\n",
    "    def hook(model, input, output):\n",
    "        # For C2f modules, the output might be a tuple/list, we take the first tensor\n",
    "        if isinstance(output, (list, tuple)):\n",
    "            yolo_features[name] = output[0].detach()\n",
    "        else:\n",
    "            yolo_features[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Mapping friendly names to actual module names in the YOLOv8 model structure\n",
    "# The indices [2], [4], [6] correspond to early backbone, late backbone, and neck layers\n",
    "yolo_layer_map = {\n",
    "    \"Early Backbone (C2f_2)\": model_yolo.model.model[2],\n",
    "    \"Mid Backbone (C2f_4)\": model_yolo.model.model[4],\n",
    "    \"Neck Features (C2f_6)\": model_yolo.model.model[6],\n",
    "}\n",
    "\n",
    "# Register forward hooks on the target layers\n",
    "hooks = []\n",
    "for name, layer in yolo_layer_map.items():\n",
    "    hooks.append(layer.register_forward_hook(get_yolo_hook(name)))\n",
    "\n",
    "# Run inference on the sample image to trigger the hooks\n",
    "# We use the original `image` from the dataset\n",
    "results_yolo = model_yolo(image.copy(), verbose=False)\n",
    "\n",
    "# Remove the hooks now that we have the features\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Visualize the captured features\n",
    "print(\"PCA Visualization of YOLOv8 Intermediate Features:\")\n",
    "for name, features in yolo_features.items():\n",
    "    visualize_features_pca(features, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91bd145",
   "metadata": {},
   "source": [
    "## 4. Part 2: RT-DETR (Real-Time DEtection TRansformer)\n",
    "\n",
    "DETR (DEtection TRansformer) models reframe object detection as a direct set prediction problem. They use a transformer-based architecture to produce a fixed-size set of predictions, eliminating the need for complex post-processing like NMS. RT-DETR is an evolution of this idea, optimized for real-time performance.\n",
    "\n",
    "### 4.1. Inference with RT-DETR\n",
    "\n",
    "We will use the `transformers` library to load a pretrained RT-DETR model. Unlike YOLO, DETR models require a specific `processor` to resize and normalize the input image correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor and a pretrained RT-DETR model from Hugging Face\n",
    "processor_detr = DetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")\n",
    "model_detr = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd\")\n",
    "\n",
    "# Prepare the image for the model\n",
    "inputs = processor_detr(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model_detr(**inputs)\n",
    "\n",
    "# Post-process the results to get bounding boxes and class labels\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results_detr = processor_detr.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n",
    "\n",
    "# Helper function to draw bounding boxes\n",
    "def draw_boxes(image, boxes, labels, scores):\n",
    "    img_draw = image.copy()\n",
    "    draw = ImageDraw.Draw(img_draw)\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        label_text = f\"{label} {score:.2f}\"\n",
    "        \n",
    "        draw.rectangle(box, outline=\"red\", width=2)\n",
    "        draw.text((box[0], box[1]), label_text, fill=\"red\")\n",
    "    return img_draw\n",
    "\n",
    "# Get labels, scores, and boxes\n",
    "labels = [model_detr.config.id2label[i.item()] for i in results_detr[\"labels\"]]\n",
    "scores = results_detr[\"scores\"]\n",
    "boxes = results_detr[\"boxes\"]\n",
    "\n",
    "# Draw the boxes on the image\n",
    "im_detr = draw_boxes(image, boxes, labels, scores)\n",
    "\n",
    "print(\"RT-DETR Detections (Confidence > 0.7):\")\n",
    "display(im_detr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc4c3c0",
   "metadata": {},
   "source": [
    "### 4.2. RT-DETR Architectural Deep Dive\n",
    "\n",
    "The DETR architecture introduced a paradigm shift by framing object detection as a direct set prediction problem, removing the need for many hand-designed components like NMS.\n",
    "\n",
    "*   **Backbone (ResNet):** Like YOLO, it begins with a standard CNN backbone (a modified ResNet-50 in this case) to extract a 2D feature map from the input image. This feature map captures the essential spatial features.\n",
    "\n",
    "*   **Transformer Encoder:** This is where DETR diverges significantly.\n",
    "    *   **Input:** The feature map from the backbone is flattened into a sequence of tokens. Crucially, these tokens are combined with **Positional Encodings**, which are vectors that give the model information about the original `(x, y)` position of each token. Without this, the transformer would be unaware of the image's spatial structure.\n",
    "    *   **Function:** The encoder processes this sequence using multiple layers of self-attention. This allows every feature token to attend to every other token, building a rich, context-aware representation. The output is an enriched sequence of image features.\n",
    "\n",
    "*   **Transformer Decoder:** The decoder is the core of the prediction mechanism.\n",
    "    *   **Input:** It takes two main inputs: the memory of enriched features from the encoder, and a small, fixed-size set of learnable embeddings called **Object Queries**.\n",
    "    *   **Function:** Each object query acts as a \"slot\" responsible for detecting a single object. Through layers of self-attention and cross-attention, the queries interact with each other (to avoid duplicates) and with the encoder's output (to find and localize objects). Each query \"asks\" the image features: \"Is there an object here that matches my pattern?\"\n",
    "    \n",
    "*   **Prediction Heads (FFNs):** After the final decoder layer, each output query embedding is passed to two separate Feed-Forward Networks (FFNs):\n",
    "    1.  A **classification head** predicts the class label for that query (e.g., 'bird', 'car', or 'no object').\n",
    "    2.  A **box head** predicts the bounding box coordinates `(center_x, center_y, width, height)`.\n",
    "\n",
    "#### End-to-End Philosophy\n",
    "\n",
    "This design is \"end-to-end\" because it directly outputs a sparse set of predictions. Since each query is encouraged to specialize on a different object, the model learns to avoid making duplicate predictions for the same object, thus eliminating the need for NMS in the original DETR paper. RT-DETR, for performance reasons, re-introduces an efficient, optional NMS-like step but the core philosophy of direct set prediction remains.\n",
    "\n",
    "#### Visualizing Intermediate Features\n",
    "\n",
    "We can visualize the output of the Transformer's encoder and decoder to see how the model refines its understanding. We'll look at:\n",
    "1.  **Encoder Output:** The contextually-rich image features before the decoder sees them.\n",
    "2.  **Decoder Output:** The final object-focused embeddings produced by the object queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc15226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RT-DETR Feature Extraction ---\n",
    "\n",
    "# Run inference, asking the model to output hidden states from all layers\n",
    "inputs = processor_detr(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model_detr(**inputs, output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "# The final encoder output tensor has shape (batch, channels, height, width)\n",
    "encoder_features = outputs.encoder_hidden_states[-1] \n",
    "\n",
    "# The decoder output is the final set of object query embeddings\n",
    "decoder_features = outputs.last_hidden_state\n",
    "\n",
    "# We can visualize the encoder output directly\n",
    "print(\"\\nPCA Visualization of RT-DETR Intermediate Features:\")\n",
    "visualize_features_pca(encoder_features, \"RT-DETR Encoder Output\")\n",
    "\n",
    "# --- Visualize Cross-Attention Maps ---\n",
    "# The cross-attentions tensor shows what part of the image each object query is \"looking at\".\n",
    "# The shape is (batch, queries, heads, dim1, dim2).\n",
    "cross_attentions = outputs.cross_attentions #Shape: (batch, num_queries, num_heads, height_keypoint, width_keypoint)\n",
    "\n",
    "# Get attentions for the first image, and average across the heads dimension.\n",
    "attention_maps = cross_attentions[0].mean(dim=2)[0]  # Shape: (num_queries, height_keypoint, width_keypoint)\n",
    "\n",
    "# Find the top 5 queries with the highest confidence score (excluding \"no object\").\n",
    "logits = outputs.logits\n",
    "probs = logits.softmax(-1)[0, :, :-1]  # Shape: (num_queries, num_classes - 1)\n",
    "scores, _ = probs.max(-1)\n",
    "top_scores, top_indices = torch.topk(scores, 5)\n",
    "\n",
    "print(\"\\nVisualization of RT-DETR Cross-Attention Weights for Top 5 Queries:\")\n",
    "fig, axs = plt.subplots(5, 1, figsize=(10, 12))\n",
    "fig.suptitle(\"Attention weights per query over 12 selected encoder keypoints\", fontsize=16)\n",
    "\n",
    "for i, query_idx in enumerate(top_indices):\n",
    "    score = top_scores[i]\n",
    "\n",
    "    # Get the attention map for this query and flatten it to a 1D vector.\n",
    "    attn_map_2d = attention_maps[query_idx]  # Shape: (3, 4)\n",
    "    attn_weights = attn_map_2d.flatten().cpu().numpy() # Shape: (12,)\n",
    "\n",
    "    # Find the predicted class label for this query.\n",
    "    pred_class_idx = probs[query_idx].argmax()\n",
    "    pred_class_label = model_detr.config.id2label[pred_class_idx.item()]\n",
    "\n",
    "    # Plot the attention weights as a bar chart.\n",
    "    ax = axs[i]\n",
    "    ax.bar(range(len(attn_weights)), attn_weights)\n",
    "    ax.set_title(f\"Query {query_idx} | Pred: {pred_class_label} ({score:.2f})\")\n",
    "    ax.set_ylabel(\"Attention Weight\")\n",
    "    ax.set_xticks(range(len(attn_weights)))\n",
    "    ax.set_xticklabels(range(len(attn_weights)))\n",
    "\n",
    "axs[-1].set_xlabel(\"Encoder Keypoint Index\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# --- NEW: Visualize Cross-Attention Maps on the Image ---\n",
    "\n",
    "# Helper function to visualize attention map overlaid on the original image\n",
    "def visualize_attention_map_on_image(original_image, attention_map_2d, title, alpha=0.5):\n",
    "    # Normalize attention map to 0-1\n",
    "    norm_attention_map = (attention_map_2d - attention_map_2d.min()) / (attention_map_2d.max() - attention_map_2d.min())\n",
    "\n",
    "    # Convert to PIL Image, resize, and convert back to numpy\n",
    "    # Ensure attention_map_2d is on CPU and is a numpy array before processing\n",
    "    attention_map_np = norm_attention_map.cpu().numpy() if isinstance(norm_attention_map, torch.Tensor) else norm_attention_map\n",
    "    attention_img = Image.fromarray(np.uint8(255 * attention_map_np))\n",
    "    attention_img = attention_img.resize(original_image.size, Image.Resampling.LANCZOS)\n",
    "    attention_img = np.array(attention_img)\n",
    "\n",
    "    # Apply colormap\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    attention_heatmap = cmap(attention_img / 255.0)[:, :, :3] # Take only RGB channels\n",
    "\n",
    "    # Convert original image to numpy array\n",
    "    original_img_np = np.array(original_image) / 255.0 # Normalize to 0-1\n",
    "\n",
    "    # Blend images\n",
    "    blended_img = original_img_np * (1 - alpha) + attention_heatmap * alpha\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(blended_img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nVisualization of RT-DETR Cross-Attention Maps Overlaid on Image for Top 5 Queries:\")\n",
    "for i, query_idx in enumerate(top_indices):\n",
    "    score = top_scores[i]\n",
    "    attn_map_2d = attention_maps[query_idx] # Shape: (3, 4)\n",
    "\n",
    "    # Find the predicted class label for this query.\n",
    "    pred_class_idx = probs[query_idx].argmax()\n",
    "    pred_class_label = model_detr.config.id2label[pred_class_idx.item()]\n",
    "\n",
    "    visualize_attention_map_on_image(\n",
    "        image, # The original PIL image\n",
    "        attn_map_2d,\n",
    "        title=f\"Query {query_idx} | Pred: {pred_class_label} ({score:.2f}) Attention Map\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e586d50",
   "metadata": {},
   "source": [
    "## 5. Comparative Analysis on the ENA24 Dataset\n",
    "\n",
    "Both models are powerful, but how do they perform on a real-world, specialized dataset like ENA24? A key challenge is that they were trained on COCO's 80 classes, which do not directly align with the animal species in ENA24.\n",
    "\n",
    "### 5.1. The Class Mismatch Problem\n",
    "\n",
    "Let's look at the classes in the ENA24 dataset. We expect the models to either fail to detect anything or to classify an ENA24 animal as a \"related\" COCO class (e.g., classifying a \"deer\" as a \"cow\" or \"horse\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base path and load the ENA24 metadata\n",
    "base_data_path = '../data/IDLE-OO-Camera-Traps/'\n",
    "ena24_csv_path = os.path.join(base_data_path, 'ENA24-balanced.csv')\n",
    "ena24_df = pd.read_csv(ena24_csv_path)\n",
    "\n",
    "# --- Visualize \"common_name\" frequency ---\n",
    "print(\"Visualizing 'common_name' frequency in the ENA24 Dataset:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "ena24_df['common_name'].value_counts().plot(kind='bar')\n",
    "plt.title('Frequency of Common Names in ENA24 Dataset')\n",
    "plt.xlabel('Common Name')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877ea90",
   "metadata": {},
   "source": [
    "### 5.2. Performance Evaluation\n",
    "\n",
    "We will now iterate through a sample of the ENA24 dataset, run both models on each image, and record the ground truth class and the top predicted COCO class. This will allow us to see which COCO classes the models associate with the ENA24 animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b025f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred_yolo = []\n",
    "y_pred_detr = []\n",
    "\n",
    "# Let's take a few images per class for our analysis\n",
    "num_samples_per_class = 5\n",
    "unique_common_names = ena24_df['common_name'].unique()\n",
    "\n",
    "for name in unique_common_names:\n",
    "    sample_images = ena24_df[ena24_df['common_name'] == name].head(num_samples_per_class)\n",
    "    \n",
    "    for index, row in sample_images.iterrows():\n",
    "        image_relative_path = row['filepath']\n",
    "        full_image_path = os.path.join(base_data_path, 'data/test/', image_relative_path)\n",
    "        \n",
    "        if os.path.exists(full_image_path):\n",
    "            try:\n",
    "                img = Image.open(full_image_path).convert(\"RGB\")\n",
    "                y_true.append(name)\n",
    "\n",
    "                # --- Run YOLO detection ---\n",
    "                results_yolo = model_yolo(img.copy(), conf=0.25, verbose=False)\n",
    "                if len(results_yolo[0].boxes) > 0:\n",
    "                    top_pred_id = int(results_yolo[0].boxes.cls[0].item())\n",
    "                    predicted_name = model_yolo.names[top_pred_id]\n",
    "                    y_pred_yolo.append(predicted_name)\n",
    "                else:\n",
    "                    y_pred_yolo.append(\"No detection\")\n",
    "\n",
    "                # --- Run RT-DETR detection ---\n",
    "                inputs = processor_detr(images=img, return_tensors=\"pt\")\n",
    "                with torch.no_grad():\n",
    "                    outputs = model_detr(**inputs)\n",
    "                \n",
    "                target_sizes = torch.tensor([img.size[::-1]])\n",
    "                results_detr = processor_detr.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.25)[0]\n",
    "                \n",
    "                if len(results_detr[\"scores\"]) > 0:\n",
    "                    top_pred_id = results_detr[\"labels\"][0].item()\n",
    "                    predicted_name = model_detr.config.id2label[top_pred_id]\n",
    "                    y_pred_detr.append(predicted_name)\n",
    "                else:\n",
    "                    y_pred_detr.append(\"No detection\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not process image {full_image_path}: {e}\")\n",
    "\n",
    "print(\"Finished collecting predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74813f61",
   "metadata": {},
   "source": [
    "### 5.3. Visualization with a Confusion Matrix\n",
    "\n",
    "A standard confusion matrix compares predictions against true labels when the classes are the same. Since our classes are different, we'll use a **cross-tabulation** (crosstab) to visualize the relationship between the true ENA24 classes and the predicted COCO classes from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f02a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOLO Crosstab ---\n",
    "df_yolo = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred_yolo})\n",
    "yolo_crosstab = pd.crosstab(df_yolo['y_true'], df_yolo['y_pred'], rownames=['True Class (ENA24)'], colnames=['Predicted Class (YOLO/COCO)'])\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(yolo_crosstab, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('ENA24 True Class vs. YOLO Predicted COCO Class')\n",
    "plt.show()\n",
    "\n",
    "# --- RT-DETR Crosstab ---\n",
    "df_detr = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred_detr})\n",
    "detr_crosstab = pd.crosstab(df_detr['y_true'], df_detr['y_pred'], rownames=['True Class (ENA24)'], colnames=['Predicted Class (RT-DETR/COCO)'])\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(detr_crosstab, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('ENA24 True Class vs. RT-DETR Predicted COCO Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a46fe",
   "metadata": {},
   "source": [
    "From the heatmaps, we can see how the models perform. For example, both models often correctly map the ENA24 `bird` class to the COCO `bird` class. However, for species not in COCO, like `red_deer` or `wild_boar`, the models might predict related classes like `cow`, `horse`, or `dog`, or simply fail to make a detection. This analysis highlights the limitations of using pretrained models on novel domains and underscores the need for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a155ae69",
   "metadata": {},
   "source": [
    "## 6. Head-to-Head Comparison: YOLO vs. RT-DETR\n",
    "\n",
    "| Feature               | YOLOv8                                        | RT-DETR                                                    |\n",
    "| --------------------- | --------------------------------------------- | ---------------------------------------------------------- |\n",
    "| **Architecture**      | CNN-based (CSPDarknet Backbone, PANet Neck)   | Hybrid (CNN Backbone + Transformer Encoder/Decoder)        |\n",
    "| **Prediction**        | Predicts on a dense grid across the image     | Predicts a sparse set of objects via object queries        |\n",
    "| **Post-processing**   | Requires Non-Maximum Suppression (NMS)        | End-to-end; minimal or no NMS required                     |\n",
    "| **Ease of Use**       | Very simple via `ultralytics` library         | More complex; requires manual processing via `transformers`|\n",
    "| **Core Idea**         | Fast, single-stage regression and classification | Direct set prediction, treating detection as a dictionary lookup |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2515b32",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we explored two state-of-the-art object detection models.\n",
    "*   **YOLOv8** is incredibly fast and easy to use, making it an excellent choice for real-time applications where speed is critical. Its reliance on NMS is a defining characteristic of single-stage detectors.\n",
    "*   **RT-DETR** represents a newer paradigm, using a transformer architecture to perform end-to-end detection. This removes the need for hand-tuned components like NMS and can lead to better performance, though often at the cost of higher computational requirements and implementation complexity.\n",
    "\n",
    "Our analysis on the ENA24 dataset showed that while pretrained models are a fantastic starting point, their performance on specialized domains is limited by their training data. To achieve high accuracy on ENA24's specific animal classes, the clear next step is **fine-tuning**, where we would train these models further on the ENA24 data itself."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
