{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ec8676",
   "metadata": {},
   "source": [
    "jupyter:\n",
    "  jupytext:\n",
    "    default_lexer: python\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.18.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42141d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mount the drive in colab to be able to share outputs across the notebooks\n",
    "import sys\n",
    "import os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "    %mkdir -p /content/drive/MyDrive/ai-course\n",
    "    %cd /content/drive/MyDrive/ai-course\n",
    "\n",
    "    if not os.path.exists('ai-course'):\n",
    "        !git clone https://github.com/jpata/ai-course\n",
    "    \n",
    "    %cd ai-course\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba8ec7",
   "metadata": {},
   "source": [
    "# Segmenting Objects with SAM using YOLO Prompts\n",
    "\n",
    "This notebook combines our fine-tuned YOLO object detector with the Segment Anything Model (SAM) to generate precise segmentation masks for animals in the ENA24 dataset.\n",
    "\n",
    "First, we will use our custom-trained YOLO model to predict a bounding box for an animal. Then, we will feed that bounding box as a \"prompt\" to SAM, which will return a high-quality segmentation mask for the object within the box.\n",
    "\n",
    "### Motivation: Why Segment Animals?\n",
    "\n",
    "While a bounding box tells us *where* an animal is, a segmentation mask provides a much richer understanding of the object.\n",
    "*   **Precise Shape and Size**: Masks outline the exact shape of an animal, allowing for more accurate measurements of size, length, and potentially biomass estimation.\n",
    "*   **Detailed Analysis**: With a precise silhouette, we can perform more detailed analyses, such as pose estimation, identifying specific body parts, or assessing animal health (e.g., whether it looks thin or well-fed).\n",
    "*   **Ecological Monitoring**: Segmentation masks are crucial for large-scale ecological studies. They enable tracking individual animals across different camera trap sightings, which is essential for estimating population density, understanding territory ranges, and studying migration patterns. The precise outline helps in re-identifying individuals based on unique markings (like stripe or spot patterns).\n",
    "*   **Occlusion and Crowds**: Segmentation can help distinguish between individual animals that are overlapping or close together, which is difficult with bounding boxes alone.\n",
    "*   **Improved Data Quality**: Using masks instead of boxes to train downstream models (like species classifiers) can improve their accuracy by removing noisy background pixels.\n",
    "\n",
    "This combination of a fast, specialized detector (YOLO) and a powerful, generalist segmentation model (SAM) creates an efficient and highly effective pipeline for advanced image analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0a420",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the necessary libraries. `ultralytics` provides our YOLO model, and we'll install `segment-anything` for the SAM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics 'segment-anything' torch torchvision matplotlib opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986b7e2",
   "metadata": {},
   "source": [
    "Now, let's import all the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7467d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4441e93",
   "metadata": {},
   "source": [
    "## 2. Load Models\n",
    "\n",
    "We need two models for this pipeline: our fine-tuned YOLO model to find the animals, and a pretrained SAM to segment them.\n",
    "\n",
    "### 2.1. Load Fine-Tuned YOLO Model\n",
    "\n",
    "We will load the `best.pt` weights from the latest YOLO training run performed in the `module4_yolo_finetuning.md` notebook. The code below automatically finds the latest run directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory where training runs are saved\n",
    "train_dir = 'runs/detect'\n",
    "\n",
    "# Find all subdirectories in the training directory that correspond to training runs\n",
    "train_dirs = [d for d in os.listdir(train_dir) if \"train\" in d]\n",
    "# Find the latest training run directory by sorting them by modification time\n",
    "latest_train_run = max(train_dirs, key=lambda d: os.path.getmtime(os.path.join(train_dir, d)))\n",
    "# Construct the full path to the best model weights from that training run\n",
    "best_model_path = os.path.join(train_dir, latest_train_run, 'weights/best.pt')\n",
    "\n",
    "print(f\"Loading fine-tuned YOLO model from: {best_model_path}\")\n",
    "# Load the fine-tuned YOLO model\n",
    "yolo_model = YOLO(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff189df",
   "metadata": {},
   "source": [
    "### 2.2. Load Segment Anything Model (SAM)\n",
    "\n",
    "SAM is a foundation model from Meta AI designed for promptable image segmentation. It can generate high-quality masks from various input prompts, including points, boxes, and text.\n",
    "\n",
    "We will use the large ViT-H SAM model. The code below will download the model checkpoint (a ~2.4GB file) if it's not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c94b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the SAM checkpoint, the model type, and the download URL\n",
    "sam_checkpoint_path = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\" # \"vit_h\" is the largest and most accurate SAM model\n",
    "sam_checkpoint_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "\n",
    "# Download the SAM checkpoint file if it doesn't already exist\n",
    "if not os.path.exists(sam_checkpoint_path):\n",
    "    print(\"Downloading SAM checkpoint...\")\n",
    "    # Stream the download to handle large files efficiently\n",
    "    response = requests.get(sam_checkpoint_url, stream=True)\n",
    "    with open(sam_checkpoint_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Register the SAM model from the checkpoint file and model type\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint_path)\n",
    "# Move the model to the specified device (GPU if available)\n",
    "sam.to(device=device)\n",
    "\n",
    "# Create the SamPredictor object. This object handles the image encoding and mask prediction.\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbe077",
   "metadata": {},
   "source": [
    "## 3. Load Validation Data\n",
    "\n",
    "We'll run our pipeline on a few images from the validation set that our YOLO model was evaluated on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e73c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.abspath('../data/IDLE-OO-Camera-Traps_yolo')\n",
    "val_file_path = os.path.join(base_path, 'val.txt')\n",
    "\n",
    "with open(val_file_path, 'r') as f:\n",
    "    val_images = [line.strip() for line in f.readlines()]\n",
    "print(f\"Found {len(val_images)} validation images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb529f",
   "metadata": {},
   "source": [
    "## 4. Run the YOLO-SAM Pipeline\n",
    "\n",
    "Now we'll tie everything together. For each image, we will:\n",
    "1.  Run our fine-tuned YOLO model to get bounding boxes.\n",
    "2.  Take the top three highest-confidence bounding boxes as prompts.\n",
    "3.  For each box, use the SAM Predictor to generate a mask.\n",
    "4.  Visualize the original image, the YOLO boxes, and the final SAM masks.\n",
    "\n",
    "First, let's define a couple of helper functions for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to show a mask on the image\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "# Helper function to show a bounding box\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d08a9d",
   "metadata": {},
   "source": [
    "Now, let's process a few sample images and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2055a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select up to 10 images from the validation set to display\n",
    "num_display_images = min(len(val_images), 10)\n",
    "display_images = val_images[:num_display_images]\n",
    "\n",
    "# Create a figure with two columns of subplots: one for YOLO boxes, one for SAM masks\n",
    "fig, axs = plt.subplots(num_display_images, 2, figsize=(15, 5 * num_display_images))\n",
    "\n",
    "# Loop through the selected images\n",
    "for i, image_path in enumerate(display_images):\n",
    "    print(f\"Processing: {os.path.basename(image_path)}\")\n",
    "\n",
    "    # --- Load Image ---\n",
    "    # Read the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    # Convert the image from BGR (OpenCV's default) to RGB, as required by SAM and Matplotlib\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- 1. Get YOLO Detections ---\n",
    "    # Run the fine-tuned YOLO model on the image path.\n",
    "    # `verbose=False` reduces console output. `conf=0.2` sets the confidence threshold.\n",
    "    yolo_results = yolo_model(image_path, verbose=False, conf=0.2)\n",
    "    \n",
    "    # --- Setup Visualization ---\n",
    "    # Select the first subplot for showing the original image with YOLO boxes\n",
    "    ax1 = axs[i, 0] if num_display_images > 1 else axs[0]\n",
    "    ax1.imshow(image_rgb)\n",
    "    ax1.set_title(f\"YOLO Detection: {os.path.basename(image_path)}\")\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Select the second subplot for showing the final SAM segmentation masks\n",
    "    ax2 = axs[i, 1] if num_display_images > 1 else axs[1]\n",
    "    ax2.imshow(image_rgb)\n",
    "    ax2.set_title(f\"SAM Segmentation\")\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Proceed only if YOLO detected at least one object\n",
    "    if len(yolo_results[0].boxes) > 0:\n",
    "        # --- Prepare SAM ---\n",
    "        # Set the image for the SAM predictor. This pre-processes the image to create an embedding.\n",
    "        # This only needs to be done once per image.\n",
    "        predictor.set_image(image_rgb)\n",
    "\n",
    "        # Get the detected boxes object from the YOLO results\n",
    "        boxes = yolo_results[0].boxes\n",
    "        # Get the confidence scores for all detected boxes\n",
    "        confidences = boxes.conf\n",
    "        # Sort the detections by confidence in descending order\n",
    "        indices = torch.argsort(confidences, descending=True)\n",
    "        # Select the indices of the top 3 most confident detections\n",
    "        top_indices = indices[:3]\n",
    "        \n",
    "        print(f\"  Found {len(boxes)} objects. Segmenting top {len(top_indices)}.\")\n",
    "\n",
    "        # Loop through the top 3 detections\n",
    "        for box_index in top_indices:\n",
    "            box = boxes[box_index] # Get the box object for the current index\n",
    "            # Extract bounding box coordinates (xyxy format)\n",
    "            box_coords = box.xyxy[0].cpu().numpy()\n",
    "            # Extract class ID, confidence score, and class name\n",
    "            class_id = int(box.cls[0].cpu().numpy())\n",
    "            confidence = float(box.conf[0].cpu().numpy())\n",
    "            class_name = yolo_model.names[class_id]\n",
    "            label = f\"{class_name} {confidence:.2f}\"\n",
    "            \n",
    "            # Draw the YOLO bounding box on both plots for comparison\n",
    "            show_box(box_coords, ax1)\n",
    "            show_box(box_coords, ax2)\n",
    "\n",
    "            # Add the class label above the box on both plots\n",
    "            x0, y0 = box_coords[0], box_coords[1]\n",
    "            ax1.text(x0, y0 - 10, label, color='white', fontsize=8, backgroundcolor='green')\n",
    "            ax2.text(x0, y0 - 10, label, color='white', fontsize=8, backgroundcolor='green')\n",
    "\n",
    "            # --- 2. Use Box as Prompt for SAM ---\n",
    "            # The input box needs to be a numpy array of shape (1, 4)\n",
    "            input_box = box_coords.astype(int)[None, :]\n",
    "\n",
    "            # Predict the mask for the object within the bounding box\n",
    "            masks, scores, logits = predictor.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=input_box,\n",
    "                multimask_output=False, # We want only the single best mask\n",
    "            )\n",
    "\n",
    "            # --- 3. Visualize the Mask ---\n",
    "            # `masks` is a (1, H, W) array, so we take the first and only mask\n",
    "            # Use a random color for each distinct object in the image\n",
    "            show_mask(masks[0], ax2, random_color=True)\n",
    "    else:\n",
    "        # If YOLO found no objects, print a message\n",
    "        print(f\"  No objects detected by YOLO in {os.path.basename(image_path)}\")\n",
    "        ax1.set_title(f\"YOLO: No Detections in {os.path.basename(image_path)}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c121b",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we successfully created a powerful pipeline combining a fine-tuned YOLO model with the generalist Segment Anything Model.\n",
    "\n",
    "*   **YOLO** acted as a fast and efficient \"object finder,\" leveraging its specialized training on our ENA24 dataset to locate animals with high accuracy.\n",
    "*   **SAM** acted as a \"precision tool,\" taking the rough bounding box from YOLO and producing a highly detailed and accurate segmentation mask without needing to be trained on our specific data.\n",
    "\n",
    "This approach demonstrates the power of foundation models in modern computer vision. Instead of training a complex segmentation model from scratch, which can be data-intensive and time-consuming, we can link together smaller, more specialized components and large, pre-trained models to achieve state-of-the-art results with significantly less effort. This YOLO-SAM pipeline could be used to rapidly generate a high-quality segmentation dataset, which could then be used to train a more lightweight, custom segmentation model if real-time performance on-device were a final goal."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
