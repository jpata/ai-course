{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db79137",
   "metadata": {},
   "source": [
    "jupyter:\n",
    "  jupytext:\n",
    "    default_lexer: python\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.18.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ac90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mount the drive in colab to be able to share outputs across the notebooks\n",
    "import sys\n",
    "import os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "    %mkdir -p /content/drive/MyDrive/ai-course\n",
    "    %cd /content/drive/MyDrive/ai-course\n",
    "\n",
    "    if not os.path.exists('ai-course'):\n",
    "        !git clone https://github.com/jpata/ai-course\n",
    "    \n",
    "    %cd ai-course\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be239196",
   "metadata": {},
   "source": [
    "# Segmenting Objects with SAM using YOLO Prompts\n",
    "\n",
    "This notebook combines our fine-tuned YOLO object detector with the Segment Anything Model (SAM) to generate precise segmentation masks for animals in the ENA24 dataset.\n",
    "\n",
    "First, we will use our custom-trained YOLO model to predict a bounding box for an animal. Then, we will feed that bounding box as a \"prompt\" to SAM, which will return a high-quality segmentation mask for the object within the box.\n",
    "\n",
    "### Motivation: Why Segment Animals?\n",
    "\n",
    "While a bounding box tells us *where* an animal is, a segmentation mask provides a much richer understanding of the object.\n",
    "*   **Precise Shape and Size**: Masks outline the exact shape of an animal, allowing for more accurate measurements of size, length, and potentially biomass estimation.\n",
    "*   **Detailed Analysis**: With a precise silhouette, we can perform more detailed analyses, such as pose estimation, identifying specific body parts, or assessing animal health (e.g., whether it looks thin or well-fed).\n",
    "*   **Ecological Monitoring**: Segmentation masks are crucial for large-scale ecological studies. They enable tracking individual animals across different camera trap sightings, which is essential for estimating population density, understanding territory ranges, and studying migration patterns. The precise outline helps in re-identifying individuals based on unique markings (like stripe or spot patterns).\n",
    "*   **Occlusion and Crowds**: Segmentation can help distinguish between individual animals that are overlapping or close together, which is difficult with bounding boxes alone.\n",
    "*   **Improved Data Quality**: Using masks instead of boxes to train downstream models (like species classifiers) can improve their accuracy by removing noisy background pixels.\n",
    "\n",
    "This combination of a fast, specialized detector (YOLO) and a powerful, generalist segmentation model (SAM) creates an efficient and highly effective pipeline for advanced image analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437c0dac",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the necessary libraries. `ultralytics` provides our YOLO model, and we'll install `segment-anything` for the SAM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics 'segment-anything' torch torchvision matplotlib opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb1d3b",
   "metadata": {},
   "source": [
    "Now, let's import all the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ddff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e83bb0",
   "metadata": {},
   "source": [
    "## 2. Load Models\n",
    "\n",
    "We need two models for this pipeline: our fine-tuned YOLO model to find the animals, and a pretrained SAM to segment them.\n",
    "\n",
    "### 2.1. Load Fine-Tuned YOLO Model\n",
    "\n",
    "We will load the `best.pt` weights from the latest YOLO training run performed in the `module4_yolo_finetuning.md` notebook. The code below automatically finds the latest run directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory where training runs are saved\n",
    "train_dir = 'runs/detect'\n",
    "\n",
    "# Find the latest training directory\n",
    "train_dirs = [d for d in os.listdir(train_dir) if \"train\" in d]\n",
    "latest_train_run = max(train_dirs, key=lambda d: os.path.getmtime(os.path.join(train_dir, d)))\n",
    "best_model_path = os.path.join(train_dir, latest_train_run, 'weights/best.pt')\n",
    "\n",
    "print(f\"Loading fine-tuned YOLO model from: {best_model_path}\")\n",
    "yolo_model = YOLO(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7e3cc",
   "metadata": {},
   "source": [
    "### 2.2. Load Segment Anything Model (SAM)\n",
    "\n",
    "SAM is a foundation model from Meta AI designed for promptable image segmentation. It can generate high-quality masks from various input prompts, including points, boxes, and text.\n",
    "\n",
    "We will use the large ViT-H SAM model. The code below will download the model checkpoint (a ~2.4GB file) if it's not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_checkpoint_path = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "sam_checkpoint_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "\n",
    "# Download the SAM checkpoint if it doesn't exist\n",
    "if not os.path.exists(sam_checkpoint_path):\n",
    "    print(\"Downloading SAM checkpoint...\")\n",
    "    response = requests.get(sam_checkpoint_url, stream=True)\n",
    "    with open(sam_checkpoint_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Load the SAM model\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint_path)\n",
    "sam.to(device=device)\n",
    "\n",
    "# Create the SAM predictor\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd10a5",
   "metadata": {},
   "source": [
    "## 3. Load Validation Data\n",
    "\n",
    "We'll run our pipeline on a few images from the validation set that our YOLO model was evaluated on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9eec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.abspath('../data/IDLE-OO-Camera-Traps_yolo')\n",
    "val_file_path = os.path.join(base_path, 'val.txt')\n",
    "\n",
    "with open(val_file_path, 'r') as f:\n",
    "    val_images = [line.strip() for line in f.readlines()]\n",
    "print(f\"Found {len(val_images)} validation images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7396d5",
   "metadata": {},
   "source": [
    "## 4. Run the YOLO-SAM Pipeline\n",
    "\n",
    "Now we'll tie everything together. For each image, we will:\n",
    "1.  Run our fine-tuned YOLO model to get bounding boxes.\n",
    "2.  Take the top three highest-confidence bounding boxes as prompts.\n",
    "3.  For each box, use the SAM Predictor to generate a mask.\n",
    "4.  Visualize the original image, the YOLO boxes, and the final SAM masks.\n",
    "\n",
    "First, let's define a couple of helper functions for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to show a mask on the image\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "# Helper function to show a bounding box\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c67dd1",
   "metadata": {},
   "source": [
    "Now, let's process a few sample images and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select up to 10 images to display\n",
    "num_display_images = min(len(val_images), 10)\n",
    "display_images = val_images[:num_display_images]\n",
    "\n",
    "fig, axs = plt.subplots(num_display_images, 2, figsize=(15, 5 * num_display_images))\n",
    "\n",
    "for i, image_path_relative in enumerate(display_images):\n",
    "    image_path_abs = os.path.join(base_path, image_path_relative)\n",
    "    print(f\"Processing: {os.path.basename(image_path_abs)}\")\n",
    "\n",
    "    # --- Load Image ---\n",
    "    # SAM expects the image in RGB format\n",
    "    image = cv2.imread(image_path_abs)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- 1. Get YOLO Detections ---\n",
    "    yolo_results = yolo_model(image_path_abs, verbose=False, conf=0.2)\n",
    "    \n",
    "    # Plot original image and YOLO box on the first subplot\n",
    "    ax1 = axs[i, 0]\n",
    "    ax1.imshow(image_rgb)\n",
    "    ax1.set_title(f\"YOLO Detection: {os.path.basename(image_path_abs)}\")\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Plot final mask on the second subplot\n",
    "    ax2 = axs[i, 1]\n",
    "    ax2.imshow(image_rgb)\n",
    "    ax2.set_title(f\"SAM Segmentation\")\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    if len(yolo_results[0].boxes) > 0:\n",
    "        # Set the image for the predictor once\n",
    "        predictor.set_image(image_rgb)\n",
    "\n",
    "        # Get top 3 boxes by confidence\n",
    "        boxes = yolo_results[0].boxes\n",
    "        confidences = boxes.conf\n",
    "        indices = torch.argsort(confidences, descending=True)\n",
    "        top_indices = indices[:3]\n",
    "        \n",
    "        print(f\"  Found {len(boxes)} objects. Segmenting top {len(top_indices)}.\")\n",
    "\n",
    "        for i in top_indices:\n",
    "            box = boxes[i] # Get the box object for the current index\n",
    "            box_coords = box.xyxy[0].cpu().numpy()\n",
    "            class_id = int(box.cls[0].cpu().numpy())\n",
    "            confidence = float(box.conf[0].cpu().numpy())\n",
    "            class_name = yolo_model.names[class_id]\n",
    "            label = f\"{class_name} {confidence:.2f}\"\n",
    "            \n",
    "            # Draw the YOLO box on both plots for comparison\n",
    "            show_box(box_coords, ax1)\n",
    "            show_box(box_coords, ax2)\n",
    "\n",
    "            # Add label to the boxes\n",
    "            x0, y0 = box_coords[0], box_coords[1]\n",
    "            ax1.text(x0, y0 - 10, label, color='white', fontsize=8, backgroundcolor='green')\n",
    "            ax2.text(x0, y0 - 10, label, color='white', fontsize=8, backgroundcolor='green')\n",
    "\n",
    "            # --- 2. Use Box as Prompt for SAM ---\n",
    "            # The input box needs to be a numpy array\n",
    "            input_box = box_coords.astype(int)\n",
    "\n",
    "            # Predict the mask\n",
    "            masks, scores, logits = predictor.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=input_box[None, :],\n",
    "                multimask_output=False,\n",
    "            )\n",
    "\n",
    "            # --- 3. Visualize the Mask ---\n",
    "            # masks is a (1, H, W) array, so we take the first one\n",
    "            show_mask(masks[0], ax2, random_color=True)\n",
    "    else:\n",
    "        print(f\"  No objects detected by YOLO in {os.path.basename(image_path_abs)}\")\n",
    "        ax1.set_title(f\"YOLO: No Detections in {os.path.basename(image_path_abs)}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce2ef2",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we successfully created a powerful pipeline combining a fine-tuned YOLO model with the generalist Segment Anything Model.\n",
    "\n",
    "*   **YOLO** acted as a fast and efficient \"object finder,\" leveraging its specialized training on our ENA24 dataset to locate animals with high accuracy.\n",
    "*   **SAM** acted as a \"precision tool,\" taking the rough bounding box from YOLO and producing a highly detailed and accurate segmentation mask without needing to be trained on our specific data.\n",
    "\n",
    "This approach demonstrates the power of foundation models in modern computer vision. Instead of training a complex segmentation model from scratch, which can be data-intensive and time-consuming, we can link together smaller, more specialized components and large, pre-trained models to achieve state-of-the-art results with significantly less effort. This YOLO-SAM pipeline could be used to rapidly generate a high-quality segmentation dataset, which could then be used to train a more lightweight, custom segmentation model if real-time performance on-device were a final goal."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
