{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092aa92e",
   "metadata": {},
   "source": [
    "jupyter:\n",
    "  jupytext:\n",
    "    default_lexer: python\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.18.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mount the drive in colab to be able to share outputs across the notebooks\n",
    "import sys\n",
    "import os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "    %mkdir -p /content/drive/MyDrive/ai-course\n",
    "    %cd /content/drive/MyDrive/ai-course\n",
    "\n",
    "    if not os.path.exists('ai-course'):\n",
    "        !git clone https://github.com/jpata/ai-course\n",
    "    \n",
    "    %cd ai-course\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141144d",
   "metadata": {},
   "source": [
    "# Object Detection with OWL2\n",
    "\n",
    "This notebook demonstrates how to use the OWL2 model for object detection on the IDLE-OO-Camera-Traps dataset.\n",
    "\n",
    "First, let's install the necessary libraries. We need `transformers` for the OWL2 model, `datasets` to handle the data, `torch` and `torchvision` as the backend for the model, `Pillow` for image manipulation, `ultralytics` for potential YOLO format compatibility, and `scikit-learn` for utilities like PCA.\n",
    "\n",
    "```python\n",
    "!pip install -q transformers datasets torch torchvision Pillow ultralytics scikit-learn\n",
    "```\n",
    "\n",
    "Now, let's import the required libraries. These include `torch` for tensor operations, components from `transformers` and `datasets` for the model and data, `PIL` (Pillow) for image processing, and `matplotlib` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "from datasets import load_dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display,HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f99425",
   "metadata": {},
   "source": [
    "Next, we load the `imageomics/IDLE-OO-Camera-Traps` dataset. We'll just take one example from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5ffc2",
   "metadata": {
    "label": "load-image-cell"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=\"../data/IDLE-OO-Camera-Traps\", split=\"test\")\n",
    "iterator = iter(dataset)\n",
    "sample = next(iterator)\n",
    "print(sample)\n",
    "image = sample[\"image\"]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa276f2",
   "metadata": {},
   "source": [
    "## Object Detection with OWL2\n",
    "\n",
    "Now, we will use the OWL2 model for object detection.\n",
    "\n",
    "We will load the OWL2 model and processor from Hugging Face. The `Owlv2Processor` is responsible for preparing the inputs for the model (both image and text), and the `Owlv2ForObjectDetection` is the model itself. We are using the `google/owlv2-base-patch16-ensemble` checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee4979",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92418b",
   "metadata": {},
   "source": [
    "Now, let's define the objects we want to detect. We can see a cheetah in the image, so let's try to detect that. The model is zero-shot, so we can provide arbitrary text queries. Here, we are providing a few example queries. Note that these queries are not ideal for detecting a cheetah, but they will demonstrate the model's ability to distinguish between different objects. The processor then tokenizes the text and preprocesses the image to create the model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[\"a photo of a leopard\", \"a photo of a tiger\", \"a photo of a rock\"]]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbcb076",
   "metadata": {},
   "source": [
    "Now we run the model to get the object detection outputs. We use `torch.no_grad()` as we are doing inference and don't need to compute gradients. The model returns a dictionary of outputs, including logits and predicted boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc5a5e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a3a55",
   "metadata": {},
   "source": [
    "The model outputs logits and bounding boxes in a raw format. We use the `processor.post_process_object_detection` function to convert these into human-readable predictions. This function filters detections based on a `threshold`, and rescales the bounding boxes to the original image size. We then iterate through the detections and draw them on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45caf16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "# The model outputs normalized box coordinates. `target_sizes` is used to scale these back to the original image dimensions.\n",
    "target_sizes = torch.Tensor([image.size[::-1]])\n",
    "# `post_process_object_detection` converts the raw model outputs (logits and boxes) into final predictions.\n",
    "# It filters detections based on the confidence `threshold` and rescales the boxes.\n",
    "results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "\n",
    "i = 0  # Retrieve predictions for the first (and only) image in the batch\n",
    "text = texts[i]\n",
    "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "# Draw bounding boxes on a copy of the image\n",
    "image_with_boxes = image.copy()\n",
    "draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "# Define a list of colors to use for different labels\n",
    "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\"]\n",
    "\n",
    "# Iterate over each detected object\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    # Assign a color based on the label index\n",
    "    color = colors[label.item() % len(colors)]\n",
    "    print(\n",
    "        f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\"\n",
    "    )\n",
    "    # Draw the bounding box rectangle\n",
    "    draw.rectangle(box, outline=color, width=3)\n",
    "    # Draw the label and confidence score above the box\n",
    "    draw.text((box[0], box[1]), f\"{text[label]} {round(score.item(), 3)}\", fill=color)\n",
    "\n",
    "image_with_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2d1ee",
   "metadata": {},
   "source": [
    "## Investigating OWL2 Internals\n",
    "\n",
    "OWL2 predicts for each patch whether or not it contains an object, and its bounding box.\n",
    "It also gives an agnostic class embedding for each patch, which contains information about the contents of the box.\n",
    "\n",
    "This code block dives into the model's internal workings.\n",
    "1.  `image_embedder`: We first pass the image through the `image_embedder` (the Vision Transformer backbone) to get a `feature_map`. This map represents the image as a grid of feature vectors, one for each image patch.\n",
    "2.  `class_predictor`: We get `source_class_embeddings` which are class-agnostic embeddings for each patch.\n",
    "3.  `objectness_predictor`: This gives a score for each patch indicating how likely it is to contain any object.\n",
    "4.  `box_predictor`: This predicts the bounding box for each patch.\n",
    "5.  **PCA Visualization**: To visualize the rich information in the `source_class_embeddings`, we use Principal Component Analysis (PCA) to reduce their dimensionality from 768 to 3. This allows us to view the embeddings as an RGB image, where different colors represent different semantic features detected in the patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713faffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature map from the image encoder (Vision Transformer backbone)\n",
    "# Shape: (batch_size, height, width, hidden_size) e.g., (1, 48, 48, 768)\n",
    "feature_map = model.image_embedder(inputs.pixel_values)[0]\n",
    "batch_size, height, width, hidden_size = feature_map.shape\n",
    "# Reshape the feature map into a sequence of patch features\n",
    "# Shape: (batch_size, num_patches, hidden_size) e.g., (1, 2304, 768)\n",
    "image_features = feature_map.reshape(batch_size, height * width, hidden_size)\n",
    "# Get class-agnostic embeddings for each patch\n",
    "# Shape: (batch_size, num_patches, hidden_size)\n",
    "source_class_embeddings = model.class_predictor(image_features)[1]\n",
    "# Get the \"objectness\" score for each patch (how likely it contains any object)\n",
    "# Shape: (batch_size, num_patches)\n",
    "objectnesses = model.objectness_predictor(image_features).sigmoid()\n",
    "# Predict a bounding box for each patch\n",
    "# Shape: (batch_size, num_patches, 4)\n",
    "boxes = model.box_predictor(image_features, feature_map=feature_map)\n",
    "\n",
    "source_class_embeddings.shape\n",
    "from sklearn.decomposition import PCA\n",
    "# Use PCA to reduce the 768-dimensional embeddings to 3 dimensions for visualization\n",
    "pca = PCA(n_components=3)\n",
    "# The number of patches along one dimension (e.g., 768/16=48)\n",
    "num_patches = model.config.vision_config.image_size // model.config.vision_config.patch_size\n",
    "H, W = num_patches, num_patches\n",
    "# Fit PCA and transform the embeddings\n",
    "pca_result = pca.fit_transform(source_class_embeddings[0].detach().numpy())\n",
    "# Reshape the 3D PCA result into an image-like format (H, W, 3)\n",
    "pca_image = pca_result.reshape(H, W, 3)\n",
    "# Normalize each of the 3 channels (R, G, B) to be in the [0, 1] range for correct display\n",
    "for c in range(3):\n",
    "    channel = pca_image[:, :, c]\n",
    "    min_val, max_val = channel.min(), channel.max()\n",
    "    if max_val > min_val:\n",
    "        pca_image[:, :, c] = (channel - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        pca_image[:, :, c] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40788a8a",
   "metadata": {},
   "source": [
    "Here we visualize the PCA-reduced class embeddings. The resulting image shows how the model groups different parts of the image semantically. Areas with similar colors are considered similar by the model. You can often see the main object (the cheetah) being segmented from the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b18d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Class embeddings\")\n",
    "plt.imshow(pca_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c4d980",
   "metadata": {},
   "source": [
    "This plot shows the objectness scores for each patch. Brighter areas indicate a higher probability that the patch contains an object. Notice how the model highlights the area where the cheetah is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Objectness scores\")\n",
    "plt.imshow(objectnesses.detach().numpy().reshape(H,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d6823",
   "metadata": {},
   "source": [
    "This is a helper function to reverse the preprocessing steps applied by the `Owlv2Processor`. The processor normalizes the image (e.g., by subtracting the mean and dividing by the standard deviation of the training data). This function \"un-normalizes\" the image so we can visualize it correctly with `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e35cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_image(pixel_values):\n",
    "    pixel_values = pixel_values.squeeze().numpy()\n",
    "    unnormalized_image = (pixel_values * np.array(processor.image_processor.image_std)[:, None, None]) + np.array(processor.image_processor.image_mean)[:, None, None]\n",
    "    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "    unnormalized_image = Image.fromarray(unnormalized_image)\n",
    "    return unnormalized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac74acd",
   "metadata": {},
   "source": [
    "Here, we identify the top 3 patches with the highest objectness scores. We then take the raw bounding box predictions from the `box_predictor` for these patches and draw them on the \"un-normalized\" image. This gives us a glimpse into the model's raw output before any post-processing like non-maximum suppression. The boxes are predicted as `(center_x, center_y, width, height)` in a normalized format, so we need to scale them to the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the top 3 patches with the highest objectness scores\n",
    "top_scores = np.argsort(objectnesses.detach().numpy()[0])[-3:]\n",
    "# Plot the original image, and the boxes of the top 3 objects.\n",
    "plt.figure(figsize=(10, 10))\n",
    "img_preprocessed = get_preprocessed_image(inputs.pixel_values)\n",
    "plt.imshow(img_preprocessed)\n",
    "plt.title(\"Original Image with Top 3 Objectness Boxes (Raw Model Output)\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Create a drawing object on a copy of the image to avoid modifying the original\n",
    "image_with_raw_boxes = img_preprocessed.copy()\n",
    "draw = ImageDraw.Draw(image_with_raw_boxes)\n",
    "\n",
    "img_width, img_height = img_preprocessed.size\n",
    "\n",
    "# Iterate over the top 3 objectness scores and their corresponding boxes\n",
    "for patch_idx in top_scores:\n",
    "    # Get the raw box coordinates for this patch\n",
    "    raw_box = boxes[0, patch_idx].detach().numpy()\n",
    "\n",
    "    # The raw box is [center_x, center_y, width, height] in a normalized format (0-1).\n",
    "    # We need to scale these coordinates to the original image dimensions.\n",
    "    cx, cy, w, h = raw_box\n",
    "    cx = cx * img_width\n",
    "    cy = cy * img_height\n",
    "    w = w * img_width\n",
    "    h = h * img_height\n",
    "\n",
    "    # Convert from [cx, cy, w, h] to [x_min, y_min, x_max, y_max] for drawing\n",
    "    box_coords_pixel = [cx-w/2, cy-h/2, cx+w/2, cy+h/2]\n",
    "\n",
    "    # Get the objectness score for this patch for display\n",
    "    objectness_score = objectnesses.detach().numpy()[0, patch_idx]\n",
    "\n",
    "    # Draw the rectangle on the image\n",
    "    draw.rectangle(box_coords_pixel, outline=\"lime\", width=3)\n",
    "    # Draw the objectness score near the box\n",
    "    draw.text((box_coords_pixel[0], box_coords_pixel[1]), f\"Confidence: {objectness_score:.3f}\", fill=\"lime\")\n",
    "\n",
    "plt.imshow(image_with_raw_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b6738",
   "metadata": {},
   "source": [
    "## Image guided prompting\n",
    "\n",
    "Instead of text, we can also use an image or part of an image as a prompt. This is called image-guided or one-shot object detection.\n",
    "\n",
    "First, let's get a few more images from the dataset to search in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca07a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "target_images = [next(iterator)[\"image\"] for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c50bd9",
   "metadata": {},
   "source": [
    "Now, we'll take the class embedding of the object we detected with the highest confidence in the previous steps (which happened to be a cheetah-like figure in the rock). This embedding will serve as our \"query\". We then loop through the new `target_images`. For each target image, we compute its class embeddings and compare them with our query embedding. The `class_predictor` can take a query embedding to condition its output. We then find the patch in the target image whose embedding is most similar to our query embedding and draw its bounding box. This allows us to find \"more objects like this one\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sigmoid function to convert logits to probabilities\n",
    "from scipy.special import expit as sigmoid\n",
    "# Use the class embedding of the highest-scoring object from the previous step as our query\n",
    "query_embedding = source_class_embeddings[0][top_scores[-1]]\n",
    "# Loop through the new set of target images to find objects similar to our query\n",
    "for target_image in target_images:\n",
    "    # Preprocess the target image\n",
    "    target_pixel_values = processor(images=target_image, return_tensors=\"pt\").pixel_values\n",
    "    # Un-normalize the image for visualization later\n",
    "    unnormalized_target_image = get_preprocessed_image(target_pixel_values)\n",
    "    \n",
    "    # Get the feature map for the target image\n",
    "    with torch.no_grad():\n",
    "      feature_map = model.image_embedder(target_pixel_values)[0]\n",
    "    \n",
    "    # Reshape the feature map into a sequence of patch features\n",
    "    b, h, w, d = feature_map.shape\n",
    "    target_image_features = feature_map.reshape(b, h * w, d)\n",
    "\n",
    "    # Predict bounding boxes for all patches in the target image\n",
    "    target_boxes = model.box_predictor(\n",
    "        target_image_features, feature_map=feature_map\n",
    "    )\n",
    "    \n",
    "    # Get class predictions, but this time conditioned on our `query_embedding`.\n",
    "    # The model will output similarity scores between each patch in the target image and the query.\n",
    "    target_class_predictions = model.class_predictor(\n",
    "        target_image_features,\n",
    "        torch.tensor(query_embedding[None, None, ...]),  # Shape must be [batch, num_queries, hidden_dim]\n",
    "    )[0]\n",
    "    \n",
    "    # Remove batch dimension and convert tensors to numpy arrays\n",
    "    target_boxes = np.array(target_boxes[0].detach())\n",
    "    target_logits = np.array(target_class_predictions[0].detach())\n",
    "    \n",
    "    # Find the patch in the target image with the highest similarity score to our query\n",
    "    top_ind = np.argmax(target_logits[:, 0], axis=0)\n",
    "    # Convert the top logit to a confidence score using the sigmoid function\n",
    "    score = sigmoid(target_logits[top_ind, 0])\n",
    "    \n",
    "    # --- Visualization ---\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(unnormalized_target_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # Get the bounding box corresponding to the best-matching patch\n",
    "    cx, cy, w, h = target_boxes[top_ind]\n",
    "    # Draw the bounding box on the image\n",
    "    ax.plot(\n",
    "        [cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "        [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2],\n",
    "        color='lime',\n",
    "    )\n",
    "    \n",
    "    # Display the similarity score\n",
    "    ax.text(\n",
    "        cx - w / 2 + 0.015,\n",
    "        cy + h / 2 - 0.015,\n",
    "        f'Score: {score:1.2f}',\n",
    "        ha='left',\n",
    "        va='bottom',\n",
    "        color='lime',\n",
    "    )\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(1, 0)\n",
    "    ax.set_title(f'Closest match')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98270480",
   "metadata": {},
   "source": [
    "## Automatic data labelling\n",
    "\n",
    "One powerful application of zero-shot object detectors like OWL2 is automatic data labeling. We can use the model to generate initial labels for a dataset, which can then be manually reviewed and corrected, saving a lot of time.\n",
    "\n",
    "This code block demonstrates this process:\n",
    "1.  **Setup**: We set the device to GPU if available, define paths, and load a CSV file (`ENA24-balanced.csv`) that contains file paths and the true `common_name` for images in the ENA24 dataset.\n",
    "2.  **Class Mapping**: We create a mapping from the string `common_name` to an integer `class_id`, which is standard for training object detection models. We save these class names to `classes.txt`.\n",
    "3.  **Image Sampling**: We take a sample of images to label.\n",
    "4.  **Loop and Detect**: We loop through each sampled image.\n",
    "5.  **Run OWL2**: For each image, we run OWL2 with general prompts like \"a photo of an animal\".\n",
    "6.  **Process Detections**: If the model detects an object, we take the one with the highest confidence score.\n",
    "7.  **Create YOLO Label**: We convert the predicted bounding box into the YOLO format (`<class_id> <x_center> <y_center> <width> <height>`, all normalized).\n",
    "8.  **Save Files**: We save the YOLO label to a `.txt` file and copy both the image and the label file into a new directory structure (`images/` and `labels/`) suitable for training a YOLO model.\n",
    "9.  **Track Accuracy**: We keep track of how many images for each class had at least one object detected versus how many were missed. This gives us a rough idea of OWL2's performance on this dataset with our general prompts.\n",
    "10. **Visualize**: For the first few images, we display the image with the predicted bounding boxes for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df89deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Move the model to the GPU if available, for faster processing\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the base path to the locally checked out dataset\n",
    "base_data_path = '../data/IDLE-OO-Camera-Traps/'\n",
    "base_data_path_yolo = '../data/IDLE-OO-Camera-Traps_yolo'\n",
    "ena24_csv_path = os.path.join(base_data_path, 'ENA24-balanced.csv')\n",
    "\n",
    "# Load the ENA24-balanced.csv file which contains image paths and true labels\n",
    "ena24_df = pd.read_csv(ena24_csv_path)\n",
    "print(f\"Successfully loaded {ena24_csv_path}\")\n",
    "print(f\"Total images in ENA24 dataset: {len(ena24_df)}\")\n",
    "\n",
    "# Create a mapping from class name string to an integer ID\n",
    "class_names = sorted(ena24_df['common_name'].unique())\n",
    "class_map = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "# Define the output directory for the YOLO dataset and save the class names to 'classes.txt'\n",
    "labels_base_dir = os.path.join(base_data_path_yolo)\n",
    "os.makedirs(labels_base_dir, exist_ok=True)\n",
    "with open(os.path.join(labels_base_dir, 'classes.txt'), 'w') as f:\n",
    "    for name in class_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "print(f\"Saved {len(class_names)} class names to {os.path.join(labels_base_dir, 'classes.txt')}\")\n",
    "\n",
    "\n",
    "# Take a sample of images for demonstration\n",
    "NUM_IMAGES_LABEL=1000\n",
    "sample_images = ena24_df.sample(NUM_IMAGES_LABEL, random_state=42) # Use a random state for reproducibility\n",
    "\n",
    "# Initialize a dictionary to track detection recall for each class\n",
    "accuracy_tracker = {name: {'detected': 0, 'missed': 0, 'total': 0} for name in sorted(sample_images['common_name'].unique())}\n",
    "\n",
    "# Use a general prompt for object detection. OWL2 will try to find objects matching any of these descriptions.\n",
    "texts = [[\"a photo of an animal\", \"a photo of a bird\", \"a photo of a dog\"]]\n",
    "\n",
    "index_img = 0\n",
    "# Loop through the sampled images and generate labels, with a progress bar\n",
    "for index, row in tqdm.tqdm(sample_images.iterrows()):\n",
    "    image_relative_path = row['filepath']\n",
    "    full_image_path = os.path.join(base_data_path, 'data/test/', image_relative_path)\n",
    "    common_name = row['common_name']\n",
    "    \n",
    "    if os.path.exists(full_image_path):\n",
    "        try:\n",
    "            image = Image.open(full_image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Increment the total count for this class\n",
    "            accuracy_tracker[common_name]['total'] += 1\n",
    "            \n",
    "            # Prepare inputs for the OWL2 model\n",
    "            inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "            # Post-process the outputs to get filtered detections\n",
    "            target_sizes = torch.Tensor([image.size[::-1]])\n",
    "            results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "\n",
    "            i = 0  # Predictions for the first (and only) image\n",
    "            boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "            # If any objects are detected, process the highest-confidence one\n",
    "            if len(scores) > 0:\n",
    "                accuracy_tracker[common_name]['detected'] += 1\n",
    "                \n",
    "                # Find the detection with the highest confidence score\n",
    "                best_score_index = scores.argmax()\n",
    "                best_box = boxes[best_score_index]\n",
    "                \n",
    "                # Get the ground truth class ID for this image from our class map\n",
    "                class_id = class_map[common_name]\n",
    "\n",
    "                # Convert the bounding box to YOLO format (normalized coordinates)\n",
    "                img_width, img_height = image.size\n",
    "                x_min, y_min, x_max, y_max = best_box.tolist()\n",
    "                \n",
    "                # Calculate center, width, and height\n",
    "                x_center = (x_min + x_max) / 2\n",
    "                y_center = (y_min + y_max) / 2\n",
    "                box_width = x_max - x_min\n",
    "                box_height = y_max - y_min\n",
    "\n",
    "                # Normalize the coordinates by the image dimensions\n",
    "                norm_x_center = x_center / img_width\n",
    "                norm_y_center = y_center / img_height\n",
    "                norm_width = box_width / img_width\n",
    "                norm_height = box_height / img_height\n",
    "\n",
    "                # Define the path for the YOLO label file, mirroring the image path structure\n",
    "                label_relative_path = os.path.splitext(image_relative_path)[0] + '.txt'\n",
    "                label_full_path = os.path.join(labels_base_dir, label_relative_path)\n",
    "                os.makedirs(os.path.dirname(label_full_path), exist_ok=True)\n",
    "\n",
    "                # Write the YOLO label file in the format: <class_id> <x_center> <y_center> <width> <height>\n",
    "                with open(label_full_path, 'w') as f:\n",
    "                    f.write(f\"{class_id} {norm_x_center:.6f} {norm_y_center:.6f} {norm_width:.6f} {norm_height:.6f}\\n\")\n",
    "                \n",
    "                # --- Organize files for YOLO training ---\n",
    "                yolo_train_images_dir = os.path.join(labels_base_dir, 'images')\n",
    "                yolo_train_labels_dir = os.path.join(labels_base_dir, 'labels')\n",
    "                os.makedirs(yolo_train_images_dir, exist_ok=True)\n",
    "                os.makedirs(yolo_train_labels_dir, exist_ok=True)\n",
    "\n",
    "                # Copy the image to the YOLO 'images' directory\n",
    "                image_name = os.path.basename(full_image_path)\n",
    "                destination_image_path = os.path.join(yolo_train_images_dir, image_name)\n",
    "                shutil.copyfile(full_image_path, destination_image_path)\n",
    "\n",
    "                # Copy the new label file to the YOLO 'labels' directory\n",
    "                label_name = os.path.basename(label_full_path)\n",
    "                destination_label_path = os.path.join(yolo_train_labels_dir, label_name)\n",
    "                shutil.copyfile(label_full_path, destination_label_path)\n",
    "            else:\n",
    "                # If no object was detected, increment the 'missed' counter\n",
    "                accuracy_tracker[common_name]['missed'] += 1\n",
    "\n",
    "            # Visualize the detections on the first few images for a quick visual check\n",
    "            if index_img < 10:\n",
    "                image_with_boxes = image.copy()\n",
    "                draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    box = [round(i, 2) for i in box.tolist()]\n",
    "                    detected_text = texts[0][label.item()]\n",
    "                    draw.rectangle(box, outline=\"red\", width=3)\n",
    "                    draw.text((box[0], box[1]), f\"{detected_text} {round(score.item(), 3)}\", fill=\"red\")\n",
    "\n",
    "                display(image_with_boxes)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process image {full_image_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Image file not found: {full_image_path}\")\n",
    "    index_img += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cda543",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Automatic Labelling Detection Rate\n",
    "\n",
    "Finally, we print the summary of our quick analysis. This shows, for each animal class, how many images we processed, and what percentage of them had at least one object detected by OWL2. This is not a measure of classification accuracy, but rather \"detection recall\" with a very general prompt. It helps to understand which animals are more easily detected by the model out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3faf31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the loop, print the accuracy summary\n",
    "print(\"\\n--- OWL2 Detection Accuracy Summary ---\")\n",
    "for common_name, stats in accuracy_tracker.items():\n",
    "    total = stats['total']\n",
    "    if total > 0:\n",
    "        detected_fraction = stats['detected'] / total\n",
    "        missed_fraction = stats['missed'] / total\n",
    "        print(f\"Class: {common_name}, total: {total}, detected: {stats['detected']} ({detected_fraction:.2%}), missed: {stats['missed']} ({missed_fraction:.2%})\")\n",
    "print(\"-------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0bd65d",
   "metadata": {},
   "source": [
    "## Challenge: How Would You Measure Detection Precision?\n",
    "\n",
    "The \"Automatic Labelling Detection Rate\" we calculated gives us a sense of **Recall** in a specific way: out of all the images that *truly contain* an animal (based on our dataset's `common_name` label), what percentage of them did our model successfully detect *at least one object*? This metric tells us how good the model is at finding *something* when an animal is present.\n",
    "\n",
    "However, it doesn't tell us about the model's **Precision** regarding the detections themselves. In this context, precision would address: of all the bounding boxes the model *generated*, how many were genuinely identifying the animal, rather than some other object or a spurious detection?\n",
    "\n",
    "For example, our current script might flag an image as \"detected\" if OWL2 puts a bounding box around a tree, a rock, or some other background element, even though the image is known to contain an animal. This would be a **False Positive** detection. The model's task here is not to identify the animal's species, but simply to localize the animal.\n",
    "\n",
    "**Your challenge:** How would you modify the automatic labeling script to measure this form of detection precision? Think about:\n",
    "1.  Currently, the `texts` prompts are quite general (e.g., `\"a photo of an animal\"`, `\"a photo of a bird\"`, `\"a photo of a dog\"`). How could you use the `label` returned by OWL2 (which corresponds to one of these prompts) to infer if the detection is likely a false positive?\n",
    "2.  If the image is known to contain an animal, and the model's highest-scoring detection corresponds to a prompt like `\"a photo of a rock\"`, how would you count that?\n",
    "3.  What counters would you need to track (e.g., `true_positive_detections`, `false_positive_detections`) to calculate precision for localization?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "label,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
