{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b809771",
   "metadata": {},
   "source": [
    "jupyter:\n",
    "  jupytext:\n",
    "    default_lexer: python\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.18.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2bbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mount the drive in colab to be able to share outputs across the notebooks\n",
    "import sys\n",
    "import os\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "    %mkdir -p /content/drive/MyDrive/ai-course\n",
    "    %cd /content/drive/MyDrive/ai-course\n",
    "\n",
    "    if not os.path.exists('ai-course'):\n",
    "        !git clone https://github.com/jpata/ai-course\n",
    "    \n",
    "    %cd ai-course\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30eca9",
   "metadata": {},
   "source": [
    "# Object Detection with OWL2\n",
    "\n",
    "This notebook demonstrates how to use the OWL2 model for object detection on the IDLE-OO-Camera-Traps dataset.\n",
    "\n",
    "First, let's install the necessary libraries. We need `transformers` for the OWL2 model, `datasets` to handle the data, `torch` and `torchvision` as the backend for the model, `Pillow` for image manipulation, `ultralytics` for potential YOLO format compatibility, and `scikit-learn` for utilities like PCA.\n",
    "\n",
    "```python\n",
    "!pip install -q transformers datasets torch torchvision Pillow ultralytics scikit-learn\n",
    "```\n",
    "\n",
    "Now, let's import the required libraries. These include `torch` for tensor operations, components from `transformers` and `datasets` for the model and data, `PIL` (Pillow) for image processing, and `matplotlib` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "from datasets import load_dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display,HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2403ffc",
   "metadata": {},
   "source": [
    "Next, we load the `imageomics/IDLE-OO-Camera-Traps` dataset. We'll just take one example from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a84b5b",
   "metadata": {
    "label": "load-image-cell"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=\"../data/IDLE-OO-Camera-Traps\", split=\"test\")\n",
    "iterator = iter(dataset)\n",
    "sample = next(iterator)\n",
    "print(sample)\n",
    "image = sample[\"image\"]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833ca40",
   "metadata": {},
   "source": [
    "## Object Detection with OWL2\n",
    "\n",
    "Now, we will use the OWL2 model for object detection.\n",
    "\n",
    "We will load the OWL2 model and processor from Hugging Face. The `Owlv2Processor` is responsible for preparing the inputs for the model (both image and text), and the `Owlv2ForObjectDetection` is the model itself. We are using the `google/owlv2-base-patch16-ensemble` checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9055c56",
   "metadata": {},
   "source": [
    "Now, let's define the objects we want to detect. We can see a cheetah in the image, so let's try to detect that. The model is zero-shot, so we can provide arbitrary text queries. Here, we are providing a few example queries. Note that these queries are not ideal for detecting a cheetah, but they will demonstrate the model's ability to distinguish between different objects. The processor then tokenizes the text and preprocesses the image to create the model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[\"a photo of a leopard\", \"a photo of a tiger\", \"a photo of a rock\"]]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1ea5d",
   "metadata": {},
   "source": [
    "Now we run the model to get the object detection outputs. We use `torch.no_grad()` as we are doing inference and don't need to compute gradients. The model returns a dictionary of outputs, including logits and predicted boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea6deb1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3aa857",
   "metadata": {},
   "source": [
    "The model outputs logits and bounding boxes in a raw format. We use the `processor.post_process_object_detection` function to convert these into human-readable predictions. This function filters detections based on a `threshold`, and rescales the bounding boxes to the original image size. We then iterate through the detections and draw them on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "target_sizes = torch.Tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "\n",
    "i = 0  # Retrieve predictions for the first image\n",
    "text = texts[i]\n",
    "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "# Draw bounding boxes on a copy of the image\n",
    "image_with_boxes = image.copy()\n",
    "draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "# Define a list of colors to use for different labels\n",
    "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\"]\n",
    "\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    # Assign a color based on the label\n",
    "    color = colors[label.item() % len(colors)]\n",
    "    print(\n",
    "        f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\"\n",
    "    )\n",
    "    draw.rectangle(box, outline=color, width=3)\n",
    "    # Draw the label and confidence score\n",
    "    draw.text((box[0], box[1]), f\"{text[label]} {round(score.item(), 3)}\", fill=color)\n",
    "\n",
    "image_with_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd241f5b",
   "metadata": {},
   "source": [
    "## Investigating OWL2 Internals\n",
    "\n",
    "OWL2 predicts for each patch whether or not it contains an object, and its bounding box.\n",
    "It also gives an agnostic class embedding for each patch, which contains information about the contents of the box.\n",
    "\n",
    "This code block dives into the model's internal workings.\n",
    "1.  `image_embedder`: We first pass the image through the `image_embedder` (the Vision Transformer backbone) to get a `feature_map`. This map represents the image as a grid of feature vectors, one for each image patch.\n",
    "2.  `class_predictor`: We get `source_class_embeddings` which are class-agnostic embeddings for each patch.\n",
    "3.  `objectness_predictor`: This gives a score for each patch indicating how likely it is to contain any object.\n",
    "4.  `box_predictor`: This predicts the bounding box for each patch.\n",
    "5.  **PCA Visualization**: To visualize the rich information in the `source_class_embeddings`, we use Principal Component Analysis (PCA) to reduce their dimensionality from 768 to 3. This allows us to view the embeddings as an RGB image, where different colors represent different semantic features detected in the patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ef414",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = model.image_embedder(inputs.pixel_values)[0]\n",
    "batch_size, height, width, hidden_size = feature_map.shape\n",
    "image_features = feature_map.reshape(batch_size, height * width, hidden_size)\n",
    "source_class_embeddings = model.class_predictor(image_features)[1]\n",
    "objectnesses = model.objectness_predictor(image_features).sigmoid()\n",
    "boxes = model.box_predictor(image_features, feature_map=feature_map)\n",
    "\n",
    "source_class_embeddings.shape\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "num_patches = model.config.vision_config.image_size // model.config.vision_config.patch_size\n",
    "H, W = num_patches, num_patches\n",
    "pca_result = pca.fit_transform(source_class_embeddings[0].detach().numpy())\n",
    "pca_image = pca_result.reshape(H, W, 3)\n",
    "for c in range(3):\n",
    "    channel = pca_image[:, :, c]\n",
    "    min_val, max_val = channel.min(), channel.max()\n",
    "    if max_val > min_val:\n",
    "        pca_image[:, :, c] = (channel - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        pca_image[:, :, c] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8f016",
   "metadata": {},
   "source": [
    "Here we visualize the PCA-reduced class embeddings. The resulting image shows how the model groups different parts of the image semantically. Areas with similar colors are considered similar by the model. You can often see the main object (the cheetah) being segmented from the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0856eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Class embeddings\")\n",
    "plt.imshow(pca_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e00fe23",
   "metadata": {},
   "source": [
    "This plot shows the objectness scores for each patch. Brighter areas indicate a higher probability that the patch contains an object. Notice how the model highlights the area where the cheetah is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8428a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Objectness scores\")\n",
    "plt.imshow(objectnesses.detach().numpy().reshape(H,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72c19e",
   "metadata": {},
   "source": [
    "This is a helper function to reverse the preprocessing steps applied by the `Owlv2Processor`. The processor normalizes the image (e.g., by subtracting the mean and dividing by the standard deviation of the training data). This function \"un-normalizes\" the image so we can visualize it correctly with `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b100a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_image(pixel_values):\n",
    "    pixel_values = pixel_values.squeeze().numpy()\n",
    "    unnormalized_image = (pixel_values * np.array(processor.image_processor.image_std)[:, None, None]) + np.array(processor.image_processor.image_mean)[:, None, None]\n",
    "    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "    unnormalized_image = Image.fromarray(unnormalized_image)\n",
    "    return unnormalized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e8e96",
   "metadata": {},
   "source": [
    "Here, we identify the top 3 patches with the highest objectness scores. We then take the raw bounding box predictions from the `box_predictor` for these patches and draw them on the \"un-normalized\" image. This gives us a glimpse into the model's raw output before any post-processing like non-maximum suppression. The boxes are predicted as `(center_x, center_y, width, height)` in a normalized format, so we need to scale them to the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38826c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores = np.argsort(objectnesses.detach().numpy()[0])[-3:]\n",
    "# Plot the original image, and the boxes of the top 3 objects.\n",
    "plt.figure(figsize=(10, 10))\n",
    "img_preprocessed = get_preprocessed_image(inputs.pixel_values)\n",
    "plt.imshow(img_preprocessed)\n",
    "plt.title(\"Original Image with Top 3 Objectness Boxes (Raw Model Output)\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Create a drawing object on a copy of the image to avoid modifying the original\n",
    "image_with_raw_boxes = img_preprocessed.copy()\n",
    "draw = ImageDraw.Draw(image_with_raw_boxes)\n",
    "\n",
    "img_width, img_height = img_preprocessed.size\n",
    "\n",
    "# Iterate over the top 3 objectness scores and their corresponding boxes\n",
    "for patch_idx in top_scores:\n",
    "    # Get the raw box coordinates for this patch\n",
    "    raw_box = boxes[0, patch_idx].detach().numpy()\n",
    "\n",
    "    # Assuming raw_box is [x_min_norm, y_min_norm, x_max_norm, y_max_norm] relative to feature map (0-1)\n",
    "    # Scale to original image dimensions\n",
    "    cx, cy, w, h = raw_box\n",
    "    cx = cx * img_width\n",
    "    cy = cy * img_height\n",
    "    w = w * img_width\n",
    "    h = h * img_height\n",
    "\n",
    "    box_coords_pixel = [cx-w/2, cy-h/2, cx+w/2, cy+h/2]\n",
    "\n",
    "    # Get the objectness score for this patch\n",
    "    objectness_score = objectnesses.detach().numpy()[0, patch_idx]\n",
    "\n",
    "    # Draw the rectangle\n",
    "    draw.rectangle(box_coords_pixel, outline=\"lime\", width=3)\n",
    "    # Draw the objectness score\n",
    "    draw.text((box_coords_pixel[0], box_coords_pixel[1]), f\"Confidence: {objectness_score:.3f}\", fill=\"lime\")\n",
    "\n",
    "plt.imshow(image_with_raw_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f395d78",
   "metadata": {},
   "source": [
    "## Image guided prompting\n",
    "\n",
    "Instead of text, we can also use an image or part of an image as a prompt. This is called image-guided or one-shot object detection.\n",
    "\n",
    "First, let's get a few more images from the dataset to search in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "target_images = [next(iterator)[\"image\"] for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9e081",
   "metadata": {},
   "source": [
    "Now, we'll take the class embedding of the object we detected with the highest confidence in the previous steps (which happened to be a cheetah-like figure in the rock). This embedding will serve as our \"query\". We then loop through the new `target_images`. For each target image, we compute its class embeddings and compare them with our query embedding. The `class_predictor` can take a query embedding to condition its output. We then find the patch in the target image whose embedding is most similar to our query embedding and draw its bounding box. This allows us to find \"more objects like this one\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "query_embedding = source_class_embeddings[0][top_scores[-1]]\n",
    "for target_image in target_images:\n",
    "    target_pixel_values = processor(images=target_image, return_tensors=\"pt\").pixel_values\n",
    "    unnormalized_target_image = get_preprocessed_image(target_pixel_values)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      feature_map = model.image_embedder(target_pixel_values)[0]\n",
    "    \n",
    "    # Get boxes and class embeddings (the latter conditioned on query embedding)\n",
    "    b, h, w, d = feature_map.shape\n",
    "    target_boxes = model.box_predictor(\n",
    "        feature_map.reshape(b, h * w, d), feature_map=feature_map\n",
    "    )\n",
    "    \n",
    "    target_class_predictions = model.class_predictor(\n",
    "        feature_map.reshape(b, h * w, d),\n",
    "        torch.tensor(query_embedding[None, None, ...]),  # [batch, queries, d]\n",
    "    )[0]\n",
    "    \n",
    "    # Remove batch dimension and convert to numpy:\n",
    "    target_boxes = np.array(target_boxes[0].detach())\n",
    "    target_logits = np.array(target_class_predictions[0].detach())\n",
    "    \n",
    "    # Take the highest scoring logit\n",
    "    top_ind = np.argmax(target_logits[:, 0], axis=0)\n",
    "    score = sigmoid(target_logits[top_ind, 0])\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(unnormalized_target_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # Get the corresponding bounding box\n",
    "    cx, cy, w, h = target_boxes[top_ind]\n",
    "    ax.plot(\n",
    "        [cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "        [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2],\n",
    "        color='lime',\n",
    "    )\n",
    "    \n",
    "    ax.text(\n",
    "        cx - w / 2 + 0.015,\n",
    "        cy + h / 2 - 0.015,\n",
    "        f'Score: {score:1.2f}',\n",
    "        ha='left',\n",
    "        va='bottom',\n",
    "        color='lime',\n",
    "        # bbox={\n",
    "        #     #'facecolor': 'white',\n",
    "        #     'edgecolor': 'lime',\n",
    "        #     'boxstyle': 'square,pad=.3',\n",
    "        # },\n",
    "    )\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(1, 0)\n",
    "    ax.set_title(f'Closest match')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408055a7",
   "metadata": {},
   "source": [
    "## Automatic data labelling\n",
    "\n",
    "One powerful application of zero-shot object detectors like OWL2 is automatic data labeling. We can use the model to generate initial labels for a dataset, which can then be manually reviewed and corrected, saving a lot of time.\n",
    "\n",
    "This code block demonstrates this process:\n",
    "1.  **Setup**: We set the device to GPU if available, define paths, and load a CSV file (`ENA24-balanced.csv`) that contains file paths and the true `common_name` for images in the ENA24 dataset.\n",
    "2.  **Class Mapping**: We create a mapping from the string `common_name` to an integer `class_id`, which is standard for training object detection models. We save these class names to `classes.txt`.\n",
    "3.  **Image Sampling**: We take a sample of images to label.\n",
    "4.  **Loop and Detect**: We loop through each sampled image.\n",
    "5.  **Run OWL2**: For each image, we run OWL2 with general prompts like \"a photo of an animal\".\n",
    "6.  **Process Detections**: If the model detects an object, we take the one with the highest confidence score.\n",
    "7.  **Create YOLO Label**: We convert the predicted bounding box into the YOLO format (`<class_id> <x_center> <y_center> <width> <height>`, all normalized).\n",
    "8.  **Save Files**: We save the YOLO label to a `.txt` file and copy both the image and the label file into a new directory structure (`images/` and `labels/`) suitable for training a YOLO model.\n",
    "9.  **Track Accuracy**: We keep track of how many images for each class had at least one object detected versus how many were missed. This gives us a rough idea of OWL2's performance on this dataset with our general prompts.\n",
    "10. **Visualize**: For the first few images, we display the image with the predicted bounding boxes for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf55e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the base path to the locally checked out dataset\n",
    "base_data_path = '../data/IDLE-OO-Camera-Traps/'\n",
    "base_data_path_yolo = '../data/IDLE-OO-Camera-Traps_yolo'\n",
    "ena24_csv_path = os.path.join(base_data_path, 'ENA24-balanced.csv')\n",
    "\n",
    "# Load the ENA24-balanced.csv file\n",
    "ena24_df = pd.read_csv(ena24_csv_path)\n",
    "print(f\"Successfully loaded {ena24_csv_path}\")\n",
    "print(f\"Total images in ENA24 dataset: {len(ena24_df)}\")\n",
    "\n",
    "# Create class mapping from the 'common_name' column\n",
    "class_names = sorted(ena24_df['common_name'].unique())\n",
    "class_map = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "# Define the output directory for labels and save the class names\n",
    "labels_base_dir = os.path.join(base_data_path_yolo)\n",
    "os.makedirs(labels_base_dir, exist_ok=True)\n",
    "with open(os.path.join(labels_base_dir, 'classes.txt'), 'w') as f:\n",
    "    for name in class_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "print(f\"Saved {len(class_names)} class names to {os.path.join(labels_base_dir, 'classes.txt')}\")\n",
    "\n",
    "\n",
    "# Take a sample of images for demonstration\n",
    "NUM_IMAGES_LABEL=1000\n",
    "sample_images = ena24_df.sample(NUM_IMAGES_LABEL, random_state=42) # Use a random state for reproducibility\n",
    "\n",
    "# Initialize accuracy tracker\n",
    "accuracy_tracker = {name: {'detected': 0, 'missed': 0, 'total': 0} for name in sorted(sample_images['common_name'].unique())}\n",
    "\n",
    "# Use a general prompt for object detection\n",
    "texts = [[\"a photo of an animal\", \"a photo of a bird\", \"a photo of a dog\"]]\n",
    "\n",
    "index_img = 0\n",
    "for index, row in tqdm.tqdm(sample_images.iterrows()):\n",
    "    image_relative_path = row['filepath']\n",
    "    full_image_path = os.path.join(base_data_path, 'data/test/', image_relative_path)\n",
    "    common_name = row['common_name']\n",
    "    \n",
    "    if os.path.exists(full_image_path):\n",
    "        try:\n",
    "            # print(f\"Processing image: {full_image_path}\")\n",
    "            image = Image.open(full_image_path).convert(\"RGB\")\n",
    "            \n",
    "            accuracy_tracker[common_name]['total'] += 1\n",
    "            \n",
    "            # Prepare inputs for OWL2 model\n",
    "            inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "            # Post-process the outputs\n",
    "            target_sizes = torch.Tensor([image.size[::-1]])\n",
    "            results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "\n",
    "            i = 0  # Predictions for the first (and only) image\n",
    "            boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "            # If any objects are detected, process the highest-confidence one\n",
    "            if len(scores) > 0:\n",
    "                accuracy_tracker[common_name]['detected'] += 1\n",
    "                \n",
    "                # Find the detection with the highest score\n",
    "                best_score_index = scores.argmax()\n",
    "                best_box = boxes[best_score_index]\n",
    "                \n",
    "                # Get the ground truth class ID from the CSV\n",
    "                class_id = class_map[common_name]\n",
    "\n",
    "                # Convert bounding box to YOLO format (normalized)\n",
    "                img_width, img_height = image.size\n",
    "                x_min, y_min, x_max, y_max = best_box.tolist()\n",
    "                \n",
    "                x_center = (x_min + x_max) / 2\n",
    "                y_center = (y_min + y_max) / 2\n",
    "                box_width = x_max - x_min\n",
    "                box_height = y_max - y_min\n",
    "\n",
    "                norm_x_center = x_center / img_width\n",
    "                norm_y_center = y_center / img_height\n",
    "                norm_width = box_width / img_width\n",
    "                norm_height = box_height / img_height\n",
    "\n",
    "                # Define the path for the YOLO label file\n",
    "                label_relative_path = os.path.splitext(image_relative_path)[0] + '.txt'\n",
    "                label_full_path = os.path.join(labels_base_dir, label_relative_path)\n",
    "                os.makedirs(os.path.dirname(label_full_path), exist_ok=True)\n",
    "\n",
    "                # Write the YOLO label file\n",
    "                with open(label_full_path, 'w') as f:\n",
    "                    f.write(f\"{class_id} {norm_x_center:.6f} {norm_y_center:.6f} {norm_width:.6f} {norm_height:.6f}\\n\")\n",
    "                \n",
    "                # print(f\"Saved YOLO label for '{common_name}' to {label_full_path}\")\n",
    "\n",
    "                # Define paths for YOLO training data\n",
    "                yolo_train_images_dir = os.path.join(labels_base_dir, 'images')\n",
    "                yolo_train_labels_dir = os.path.join(labels_base_dir, 'labels')\n",
    "                os.makedirs(yolo_train_images_dir, exist_ok=True)\n",
    "                os.makedirs(yolo_train_labels_dir, exist_ok=True)\n",
    "\n",
    "                # Copy image to YOLO training images directory\n",
    "                image_name = os.path.basename(full_image_path)\n",
    "                destination_image_path = os.path.join(yolo_train_images_dir, image_name)\n",
    "                shutil.copyfile(full_image_path, destination_image_path)\n",
    "                # print(f\"Copied image to {destination_image_path}\")\n",
    "\n",
    "                # Copy label file to YOLO training labels directory\n",
    "                label_name = os.path.basename(label_full_path)\n",
    "                destination_label_path = os.path.join(yolo_train_labels_dir, label_name)\n",
    "                shutil.copyfile(label_full_path, destination_label_path)\n",
    "                # print(f\"Copied label to {destination_label_path}\")\n",
    "            else:\n",
    "                accuracy_tracker[common_name]['missed'] += 1\n",
    "                # print(f\"No animal detected in image for '{common_name}'\")\n",
    "\n",
    "            # Visualize the detections on the image for verification\n",
    "            if index_img < 10:\n",
    "                image_with_boxes = image.copy()\n",
    "                draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    box = [round(i, 2) for i in box.tolist()]\n",
    "                    detected_text = texts[0][label.item()]\n",
    "                    # print(\n",
    "                    #     f\"Detected '{detected_text}' with confidence {round(score.item(), 3)} at location {box}\"\n",
    "                    # )\n",
    "                    draw.rectangle(box, outline=\"red\", width=3)\n",
    "                    draw.text((box[0], box[1]), f\"{detected_text} {round(score.item(), 3)}\", fill=\"red\")\n",
    "\n",
    "                display(image_with_boxes)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process image {full_image_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Image file not found: {full_image_path}\")\n",
    "    index_img += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcc71f6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Automatic Labelling Detection Rate\n",
    "\n",
    "Finally, we print the summary of our quick analysis. This shows, for each animal class, how many images we processed, and what percentage of them had at least one object detected by OWL2. This is not a measure of classification accuracy, but rather \"detection recall\" with a very general prompt. It helps to understand which animals are more easily detected by the model out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the loop, print the accuracy summary\n",
    "print(\"\\n--- OWL2 Detection Accuracy Summary ---\")\n",
    "for common_name, stats in accuracy_tracker.items():\n",
    "    total = stats['total']\n",
    "    if total > 0:\n",
    "        detected_fraction = stats['detected'] / total\n",
    "        missed_fraction = stats['missed'] / total\n",
    "        print(f\"Class: {common_name}, total: {total}, detected: {stats['detected']} ({detected_fraction:.2%}), missed: {stats['missed']} ({missed_fraction:.2%})\")\n",
    "print(\"-------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1329458",
   "metadata": {},
   "source": [
    "## Challenge: How Would You Measure Detection Precision?\n",
    "\n",
    "The \"Automatic Labelling Detection Rate\" we calculated gives us a sense of **Recall** in a specific way: out of all the images that *truly contain* an animal (based on our dataset's `common_name` label), what percentage of them did our model successfully detect *at least one object*? This metric tells us how good the model is at finding *something* when an animal is present.\n",
    "\n",
    "However, it doesn't tell us about the model's **Precision** regarding the detections themselves. In this context, precision would address: of all the bounding boxes the model *generated*, how many were genuinely identifying the animal, rather than some other object or a spurious detection?\n",
    "\n",
    "For example, our current script might flag an image as \"detected\" if OWL2 puts a bounding box around a tree, a rock, or some other background element, even though the image is known to contain an animal. This would be a **False Positive** detection. The model's task here is not to identify the animal's species, but simply to localize the animal.\n",
    "\n",
    "**Your challenge:** How would you modify the automatic labeling script to measure this form of detection precision? Think about:\n",
    "1.  Currently, the `texts` prompts are quite general (e.g., `\"a photo of an animal\"`, `\"a photo of a bird\"`, `\"a photo of a dog\"`). How could you use the `label` returned by OWL2 (which corresponds to one of these prompts) to infer if the detection is likely a false positive?\n",
    "2.  If the image is known to contain an animal, and the model's highest-scoring detection corresponds to a prompt like `\"a photo of a rock\"`, how would you count that?\n",
    "3.  What counters would you need to track (e.g., `true_positive_detections`, `false_positive_detections`) to calculate precision for localization?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "label,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
