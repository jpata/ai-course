{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0c9622",
   "metadata": {},
   "source": [
    "jupyter:\n",
    "  jupytext:\n",
    "    default_lexer: python\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.18.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04789a1b",
   "metadata": {},
   "source": [
    "# Object Detection with OWL2\n",
    "\n",
    "This notebook demonstrates how to use the OWL2 model for object detection on the IDLE-OO-Camera-Traps dataset.\n",
    "\n",
    "First, let's install the necessary libraries.\n",
    "\n",
    "```python\n",
    "!pip install -q transformers datasets torch torchvision Pillow ultralytics scikit-learn\n",
    "```\n",
    "\n",
    "Now, let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "from datasets import load_dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display,HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430011d",
   "metadata": {},
   "source": [
    "Next, we load the `imageomics/IDLE-OO-Camera-Traps` dataset. We'll just take one example from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c4f01",
   "metadata": {
    "label": "load-image-cell"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=\"../data/IDLE-OO-Camera-Traps\", split=\"test\")\n",
    "iterator = iter(dataset)\n",
    "sample = next(iterator)\n",
    "print(sample)\n",
    "image = sample[\"image\"]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97c7f3",
   "metadata": {},
   "source": [
    "## Object Detection with OWL2\n",
    "\n",
    "Now, we will use the OWL2 model for object detection.\n",
    "\n",
    "We will load the OWL2 model and processor from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ad008",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a73722",
   "metadata": {},
   "source": [
    "Now, let's define the objects we want to detect. We can see a deer in the image, so let's try to detect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[\"a photo of a leopard\", \"a photo of a tiger\", \"a photo of a rock\"]]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32607a",
   "metadata": {},
   "source": [
    "Now we run the model to get the object detection outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3b5cc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e309b4",
   "metadata": {},
   "source": [
    "The model outputs logits and bounding boxes. We need to post-process these to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "target_sizes = torch.Tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "\n",
    "i = 0  # Retrieve predictions for the first image\n",
    "text = texts[i]\n",
    "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "# Draw bounding boxes on a copy of the image\n",
    "image_with_boxes = image.copy()\n",
    "draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "# Define a list of colors to use for different labels\n",
    "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\"]\n",
    "\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    # Assign a color based on the label\n",
    "    color = colors[label.item() % len(colors)]\n",
    "    print(\n",
    "        f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\"\n",
    "    )\n",
    "    draw.rectangle(box, outline=color, width=3)\n",
    "    # Draw the label and confidence score\n",
    "    draw.text((box[0], box[1]), f\"{text[label]} {round(score.item(), 3)}\", fill=color)\n",
    "\n",
    "image_with_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e29639",
   "metadata": {},
   "source": [
    "## Investigating OWL2 Internals\n",
    "\n",
    "OWL2 predicts for each patch whether or not it contains an object, and its bounding box.\n",
    "It also gives an agnostic class embedding for each patch, which contains information about the contents of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = model.image_embedder(inputs.pixel_values)[0]\n",
    "batch_size, height, width, hidden_size = feature_map.shape\n",
    "image_features = feature_map.reshape(batch_size, height * width, hidden_size)\n",
    "source_class_embeddings = model.class_predictor(image_features)[1]\n",
    "objectnesses = model.objectness_predictor(image_features).sigmoid()\n",
    "boxes = model.box_predictor(image_features, feature_map=feature_map)\n",
    "\n",
    "source_class_embeddings.shape\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "num_patches = model.config.vision_config.image_size // model.config.vision_config.patch_size\n",
    "H, W = num_patches, num_patches\n",
    "pca_result = pca.fit_transform(source_class_embeddings[0].detach().numpy())\n",
    "pca_image = pca_result.reshape(H, W, 3)\n",
    "for c in range(3):\n",
    "    channel = pca_image[:, :, c]\n",
    "    min_val, max_val = channel.min(), channel.max()\n",
    "    if max_val > min_val:\n",
    "        pca_image[:, :, c] = (channel - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        pca_image[:, :, c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed034d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Class embeddings\")\n",
    "plt.imshow(pca_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16022489",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Objectness scores\")\n",
    "plt.imshow(objectnesses.detach().numpy().reshape(H,W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee284849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_image(pixel_values):\n",
    "    pixel_values = pixel_values.squeeze().numpy()\n",
    "    unnormalized_image = (pixel_values * np.array(processor.image_processor.image_std)[:, None, None]) + np.array(processor.image_processor.image_mean)[:, None, None]\n",
    "    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "    unnormalized_image = Image.fromarray(unnormalized_image)\n",
    "    return unnormalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores = np.argsort(objectnesses.detach().numpy()[0])[-3:]\n",
    "# Plot the original image, and the boxes of the top 3 objects.\n",
    "plt.figure(figsize=(10, 10))\n",
    "img_preprocessed = get_preprocessed_image(inputs.pixel_values)\n",
    "plt.imshow(img_preprocessed)\n",
    "plt.title(\"Original Image with Top 3 Objectness Boxes (Raw Model Output)\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Create a drawing object on a copy of the image to avoid modifying the original\n",
    "image_with_raw_boxes = img_preprocessed.copy()\n",
    "draw = ImageDraw.Draw(image_with_raw_boxes)\n",
    "\n",
    "img_width, img_height = img_preprocessed.size\n",
    "\n",
    "# Iterate over the top 3 objectness scores and their corresponding boxes\n",
    "for patch_idx in top_scores:\n",
    "    # Get the raw box coordinates for this patch\n",
    "    raw_box = boxes[0, patch_idx].detach().numpy()\n",
    "\n",
    "    # Assuming raw_box is [x_min_norm, y_min_norm, x_max_norm, y_max_norm] relative to feature map (0-1)\n",
    "    # Scale to original image dimensions\n",
    "    cx, cy, w, h = raw_box\n",
    "    cx = cx * img_width\n",
    "    cy = cy * img_height\n",
    "    w = w * img_width\n",
    "    h = h * img_height\n",
    "\n",
    "    box_coords_pixel = [cx-w/2, cy-h/2, cx+w/2, cy+h/2]\n",
    "\n",
    "    # Get the objectness score for this patch\n",
    "    objectness_score = objectnesses.detach().numpy()[0, patch_idx]\n",
    "\n",
    "    # Draw the rectangle\n",
    "    draw.rectangle(box_coords_pixel, outline=\"lime\", width=3)\n",
    "    # Draw the objectness score\n",
    "    draw.text((box_coords_pixel[0], box_coords_pixel[1]), f\"Confidence: {objectness_score:.3f}\", fill=\"lime\")\n",
    "\n",
    "plt.imshow(image_with_raw_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295f2c9",
   "metadata": {},
   "source": [
    "## Image guided prompting\n",
    "Let's reset the dataset iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "target_images = [next(iterator)[\"image\"] for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68448247",
   "metadata": {},
   "source": [
    "Now let's use the cheetah embedding to condition the class embeddings, and visualize the top one per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "query_embedding = source_class_embeddings[0][top_scores[-1]]\n",
    "for target_image in target_images:\n",
    "    target_pixel_values = processor(images=target_image, return_tensors=\"pt\").pixel_values\n",
    "    unnormalized_target_image = get_preprocessed_image(target_pixel_values)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      feature_map = model.image_embedder(target_pixel_values)[0]\n",
    "    \n",
    "    # Get boxes and class embeddings (the latter conditioned on query embedding)\n",
    "    b, h, w, d = feature_map.shape\n",
    "    target_boxes = model.box_predictor(\n",
    "        feature_map.reshape(b, h * w, d), feature_map=feature_map\n",
    "    )\n",
    "    \n",
    "    target_class_predictions = model.class_predictor(\n",
    "        feature_map.reshape(b, h * w, d),\n",
    "        torch.tensor(query_embedding[None, None, ...]),  # [batch, queries, d]\n",
    "    )[0]\n",
    "    \n",
    "    # Remove batch dimension and convert to numpy:\n",
    "    target_boxes = np.array(target_boxes[0].detach())\n",
    "    target_logits = np.array(target_class_predictions[0].detach())\n",
    "    \n",
    "    # Take the highest scoring logit\n",
    "    top_ind = np.argmax(target_logits[:, 0], axis=0)\n",
    "    score = sigmoid(target_logits[top_ind, 0])\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(unnormalized_target_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # Get the corresponding bounding box\n",
    "    cx, cy, w, h = target_boxes[top_ind]\n",
    "    ax.plot(\n",
    "        [cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "        [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2],\n",
    "        color='lime',\n",
    "    )\n",
    "    \n",
    "    ax.text(\n",
    "        cx - w / 2 + 0.015,\n",
    "        cy + h / 2 - 0.015,\n",
    "        f'Score: {score:1.2f}',\n",
    "        ha='left',\n",
    "        va='bottom',\n",
    "        color='lime',\n",
    "        # bbox={\n",
    "        #     #'facecolor': 'white',\n",
    "        #     'edgecolor': 'lime',\n",
    "        #     'boxstyle': 'square,pad=.3',\n",
    "        # },\n",
    "    )\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(1, 0)\n",
    "    ax.set_title(f'Closest match')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e97aac",
   "metadata": {},
   "source": [
    "## Automatic data labelling\n",
    "\n",
    "Here we will loop over images from the ENA24 dataset and apply prompts to automatically label them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e7b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the base path to the locally checked out dataset\n",
    "base_data_path = '../data/IDLE-OO-Camera-Traps/'\n",
    "base_data_path_yolo = '../data/IDLE-OO-Camera-Traps_yolo'\n",
    "ena24_csv_path = os.path.join(base_data_path, 'ENA24-balanced.csv')\n",
    "\n",
    "# Load the ENA24-balanced.csv file\n",
    "ena24_df = pd.read_csv(ena24_csv_path)\n",
    "print(f\"Successfully loaded {ena24_csv_path}\")\n",
    "print(f\"Total images in ENA24 dataset: {len(ena24_df)}\")\n",
    "\n",
    "# Create class mapping from the 'common_name' column\n",
    "class_names = sorted(ena24_df['common_name'].unique())\n",
    "class_map = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "# Define the output directory for labels and save the class names\n",
    "labels_base_dir = os.path.join(base_data_path_yolo)\n",
    "os.makedirs(labels_base_dir, exist_ok=True)\n",
    "with open(os.path.join(labels_base_dir, 'classes.txt'), 'w') as f:\n",
    "    for name in class_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "print(f\"Saved {len(class_names)} class names to {os.path.join(labels_base_dir, 'classes.txt')}\")\n",
    "\n",
    "\n",
    "# Take a sample of images for demonstration\n",
    "NUM_IMAGES_LABEL=500\n",
    "sample_images = ena24_df.sample(NUM_IMAGES_LABEL, random_state=42) # Use a random state for reproducibility\n",
    "\n",
    "# Initialize accuracy tracker\n",
    "accuracy_tracker = {name: {'detected': 0, 'missed': 0, 'total': 0} for name in sorted(sample_images['common_name'].unique())}\n",
    "\n",
    "# Use a general prompt for object detection\n",
    "texts = [[\"a photo of an animal\", \"a photo of a bird\", \"a photo of a dog\"]]\n",
    "\n",
    "index_img = 0\n",
    "for index, row in tqdm.tqdm(sample_images.iterrows()):\n",
    "    image_relative_path = row['filepath']\n",
    "    full_image_path = os.path.join(base_data_path, 'data/test/', image_relative_path)\n",
    "    common_name = row['common_name']\n",
    "    \n",
    "    if os.path.exists(full_image_path):\n",
    "        try:\n",
    "            # print(f\"Processing image: {full_image_path}\")\n",
    "            image = Image.open(full_image_path).convert(\"RGB\")\n",
    "            \n",
    "            accuracy_tracker[common_name]['total'] += 1\n",
    "            \n",
    "            # Prepare inputs for OWL2 model\n",
    "            inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "            # Post-process the outputs\n",
    "            target_sizes = torch.Tensor([image.size[::-1]])\n",
    "            results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "\n",
    "            i = 0  # Predictions for the first (and only) image\n",
    "            boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "            # If any objects are detected, process the highest-confidence one\n",
    "            if len(scores) > 0:\n",
    "                accuracy_tracker[common_name]['detected'] += 1\n",
    "                \n",
    "                # Find the detection with the highest score\n",
    "                best_score_index = scores.argmax()\n",
    "                best_box = boxes[best_score_index]\n",
    "                \n",
    "                # Get the ground truth class ID from the CSV\n",
    "                class_id = class_map[common_name]\n",
    "\n",
    "                # Convert bounding box to YOLO format (normalized)\n",
    "                img_width, img_height = image.size\n",
    "                x_min, y_min, x_max, y_max = best_box.tolist()\n",
    "                \n",
    "                x_center = (x_min + x_max) / 2\n",
    "                y_center = (y_min + y_max) / 2\n",
    "                box_width = x_max - x_min\n",
    "                box_height = y_max - y_min\n",
    "\n",
    "                norm_x_center = x_center / img_width\n",
    "                norm_y_center = y_center / img_height\n",
    "                norm_width = box_width / img_width\n",
    "                norm_height = box_height / img_height\n",
    "\n",
    "                # Define the path for the YOLO label file\n",
    "                label_relative_path = os.path.splitext(image_relative_path)[0] + '.txt'\n",
    "                label_full_path = os.path.join(labels_base_dir, label_relative_path)\n",
    "                os.makedirs(os.path.dirname(label_full_path), exist_ok=True)\n",
    "\n",
    "                # Write the YOLO label file\n",
    "                with open(label_full_path, 'w') as f:\n",
    "                    f.write(f\"{class_id} {norm_x_center:.6f} {norm_y_center:.6f} {norm_width:.6f} {norm_height:.6f}\\n\")\n",
    "                \n",
    "                # print(f\"Saved YOLO label for '{common_name}' to {label_full_path}\")\n",
    "\n",
    "                # Define paths for YOLO training data\n",
    "                yolo_train_images_dir = os.path.join(labels_base_dir, 'images')\n",
    "                yolo_train_labels_dir = os.path.join(labels_base_dir, 'labels')\n",
    "                os.makedirs(yolo_train_images_dir, exist_ok=True)\n",
    "                os.makedirs(yolo_train_labels_dir, exist_ok=True)\n",
    "\n",
    "                # Copy image to YOLO training images directory\n",
    "                image_name = os.path.basename(full_image_path)\n",
    "                destination_image_path = os.path.join(yolo_train_images_dir, image_name)\n",
    "                shutil.copyfile(full_image_path, destination_image_path)\n",
    "                # print(f\"Copied image to {destination_image_path}\")\n",
    "\n",
    "                # Copy label file to YOLO training labels directory\n",
    "                label_name = os.path.basename(label_full_path)\n",
    "                destination_label_path = os.path.join(yolo_train_labels_dir, label_name)\n",
    "                shutil.copyfile(label_full_path, destination_label_path)\n",
    "                # print(f\"Copied label to {destination_label_path}\")\n",
    "            else:\n",
    "                accuracy_tracker[common_name]['missed'] += 1\n",
    "                # print(f\"No animal detected in image for '{common_name}'\")\n",
    "\n",
    "            # Visualize the detections on the image for verification\n",
    "            if index_img < 10:\n",
    "                image_with_boxes = image.copy()\n",
    "                draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    box = [round(i, 2) for i in box.tolist()]\n",
    "                    detected_text = texts[0][label.item()]\n",
    "                    # print(\n",
    "                    #     f\"Detected '{detected_text}' with confidence {round(score.item(), 3)} at location {box}\"\n",
    "                    # )\n",
    "                    draw.rectangle(box, outline=\"red\", width=3)\n",
    "                    draw.text((box[0], box[1]), f\"{detected_text} {round(score.item(), 3)}\", fill=\"red\")\n",
    "\n",
    "                display(image_with_boxes)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process image {full_image_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Image file not found: {full_image_path}\")\n",
    "    index_img += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83adfe",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Automatic Labelling Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9290a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the loop, print the accuracy summary\n",
    "print(\"\\n--- OWL2 Detection Accuracy Summary ---\")\n",
    "for common_name, stats in accuracy_tracker.items():\n",
    "    total = stats['total']\n",
    "    if total > 0:\n",
    "        detected_fraction = stats['detected'] / total\n",
    "        missed_fraction = stats['missed'] / total\n",
    "        print(f\"Class: {common_name}, total: {total}, detected: {stats['detected']} ({detected_fraction:.2%}), missed: {stats['missed']} ({missed_fraction:.2%})\")\n",
    "print(\"-------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "label,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
