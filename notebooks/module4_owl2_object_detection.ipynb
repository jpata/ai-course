{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7097cac4",
   "metadata": {},
   "source": [
    "jupyter:\n",
    "  jupytext:\n",
    "    default_lexer: python\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.18.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52795ce4",
   "metadata": {},
   "source": [
    "# Object Detection with OWL2\n",
    "\n",
    "This notebook demonstrates how to use the OWL2 model for object detection on the IDLE-OO-Camera-Traps dataset.\n",
    "\n",
    "First, let's install the necessary libraries.\n",
    "\n",
    "```python\n",
    "!pip install -q transformers datasets torch torchvision Pillow ultralytics scikit-learn\n",
    "```\n",
    "\n",
    "Now, let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "from datasets import load_dataset\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display,HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9cf4e3",
   "metadata": {},
   "source": [
    "Next, we load the `imageomics/IDLE-OO-Camera-Traps` dataset. We'll just take one example from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9caac7c",
   "metadata": {
    "label": "load-image-cell"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=\"../data/IDLE-OO-Camera-Traps\", split=\"test\")\n",
    "iterator = iter(dataset)\n",
    "sample = next(iterator)\n",
    "image = sample[\"image\"]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9e923",
   "metadata": {},
   "source": [
    "## Object Detection with OWL2\n",
    "\n",
    "Now, we will use the OWL2 model for object detection.\n",
    "\n",
    "We will load the OWL2 model and processor from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee5f9a",
   "metadata": {},
   "source": [
    "Now, let's define the objects we want to detect. We can see a deer in the image, so let's try to detect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[\"a photo of a leopard\", \"a photo of a tiger\", \"a photo of a rock\"]]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01074d7",
   "metadata": {},
   "source": [
    "Now we run the model to get the object detection outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb1191",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efc5a7",
   "metadata": {},
   "source": [
    "The model outputs logits and bounding boxes. We need to post-process these to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ece77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "target_sizes = torch.Tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "\n",
    "i = 0  # Retrieve predictions for the first image\n",
    "text = texts[i]\n",
    "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "# Draw bounding boxes on a copy of the image\n",
    "image_with_boxes = image.copy()\n",
    "draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "# Define a list of colors to use for different labels\n",
    "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\"]\n",
    "\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    # Assign a color based on the label\n",
    "    color = colors[label.item() % len(colors)]\n",
    "    print(\n",
    "        f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\"\n",
    "    )\n",
    "    draw.rectangle(box, outline=color, width=3)\n",
    "    # Draw the label and confidence score\n",
    "    draw.text((box[0], box[1]), f\"{text[label]} {round(score.item(), 3)}\", fill=color)\n",
    "\n",
    "image_with_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732e1dc",
   "metadata": {},
   "source": [
    "## Automatic data labelling\n",
    "\n",
    "Here we will loop over images from the ENA24 dataset and apply prompts to automatically label them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5230e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the base path to the locally checked out dataset\n",
    "base_data_path = '../data/IDLE-OO-Camera-Traps/'\n",
    "base_data_path_yolo = '../data/IDLE-OO-Camera-Traps_yolo'\n",
    "ena24_csv_path = os.path.join(base_data_path, 'ENA24-balanced.csv')\n",
    "\n",
    "# Load the ENA24-balanced.csv file\n",
    "ena24_df = pd.read_csv(ena24_csv_path)\n",
    "print(f\"Successfully loaded {ena24_csv_path}\")\n",
    "print(f\"Total images in ENA24 dataset: {len(ena24_df)}\")\n",
    "\n",
    "# Create class mapping from the 'common_name' column\n",
    "class_names = sorted(ena24_df['common_name'].unique())\n",
    "class_map = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "# Define the output directory for labels and save the class names\n",
    "labels_base_dir = os.path.join(base_data_path_yolo)\n",
    "os.makedirs(labels_base_dir, exist_ok=True)\n",
    "with open(os.path.join(labels_base_dir, 'classes.txt'), 'w') as f:\n",
    "    for name in class_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "print(f\"Saved {len(class_names)} class names to {os.path.join(labels_base_dir, 'classes.txt')}\")\n",
    "\n",
    "\n",
    "# Take a sample of images for demonstration\n",
    "NUM_IMAGES_LABEL=500\n",
    "sample_images = ena24_df.sample(NUM_IMAGES_LABEL, random_state=42) # Use a random state for reproducibility\n",
    "\n",
    "# Initialize accuracy tracker\n",
    "accuracy_tracker = {name: {'detected': 0, 'missed': 0, 'total': 0} for name in sorted(sample_images['common_name'].unique())}\n",
    "\n",
    "# Use a general prompt for object detection\n",
    "texts = [[\"a photo of an animal\", \"a photo of a bird\", \"a photo of a dog\"]]\n",
    "\n",
    "index_img = 0\n",
    "for index, row in tqdm.tqdm(sample_images.iterrows()):\n",
    "    image_relative_path = row['filepath']\n",
    "    full_image_path = os.path.join(base_data_path, 'data/test/', image_relative_path)\n",
    "    common_name = row['common_name']\n",
    "    \n",
    "    if os.path.exists(full_image_path):\n",
    "        try:\n",
    "            # print(f\"Processing image: {full_image_path}\")\n",
    "            image = Image.open(full_image_path).convert(\"RGB\")\n",
    "            \n",
    "            accuracy_tracker[common_name]['total'] += 1\n",
    "            \n",
    "            # Prepare inputs for OWL2 model\n",
    "            inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "            # Post-process the outputs\n",
    "            target_sizes = torch.Tensor([image.size[::-1]])\n",
    "            results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "\n",
    "            i = 0  # Predictions for the first (and only) image\n",
    "            boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "            # If any objects are detected, process the highest-confidence one\n",
    "            if len(scores) > 0:\n",
    "                accuracy_tracker[common_name]['detected'] += 1\n",
    "                \n",
    "                # Find the detection with the highest score\n",
    "                best_score_index = scores.argmax()\n",
    "                best_box = boxes[best_score_index]\n",
    "                \n",
    "                # Get the ground truth class ID from the CSV\n",
    "                class_id = class_map[common_name]\n",
    "\n",
    "                # Convert bounding box to YOLO format (normalized)\n",
    "                img_width, img_height = image.size\n",
    "                x_min, y_min, x_max, y_max = best_box.tolist()\n",
    "                \n",
    "                x_center = (x_min + x_max) / 2\n",
    "                y_center = (y_min + y_max) / 2\n",
    "                box_width = x_max - x_min\n",
    "                box_height = y_max - y_min\n",
    "\n",
    "                norm_x_center = x_center / img_width\n",
    "                norm_y_center = y_center / img_height\n",
    "                norm_width = box_width / img_width\n",
    "                norm_height = box_height / img_height\n",
    "\n",
    "                # Define the path for the YOLO label file\n",
    "                label_relative_path = os.path.splitext(image_relative_path)[0] + '.txt'\n",
    "                label_full_path = os.path.join(labels_base_dir, label_relative_path)\n",
    "                os.makedirs(os.path.dirname(label_full_path), exist_ok=True)\n",
    "\n",
    "                # Write the YOLO label file\n",
    "                with open(label_full_path, 'w') as f:\n",
    "                    f.write(f\"{class_id} {norm_x_center:.6f} {norm_y_center:.6f} {norm_width:.6f} {norm_height:.6f}\\n\")\n",
    "                \n",
    "                # print(f\"Saved YOLO label for '{common_name}' to {label_full_path}\")\n",
    "\n",
    "                # Define paths for YOLO training data\n",
    "                yolo_train_images_dir = os.path.join(labels_base_dir, 'images')\n",
    "                yolo_train_labels_dir = os.path.join(labels_base_dir, 'labels')\n",
    "                os.makedirs(yolo_train_images_dir, exist_ok=True)\n",
    "                os.makedirs(yolo_train_labels_dir, exist_ok=True)\n",
    "\n",
    "                # Copy image to YOLO training images directory\n",
    "                image_name = os.path.basename(full_image_path)\n",
    "                destination_image_path = os.path.join(yolo_train_images_dir, image_name)\n",
    "                shutil.copyfile(full_image_path, destination_image_path)\n",
    "                # print(f\"Copied image to {destination_image_path}\")\n",
    "\n",
    "                # Copy label file to YOLO training labels directory\n",
    "                label_name = os.path.basename(label_full_path)\n",
    "                destination_label_path = os.path.join(yolo_train_labels_dir, label_name)\n",
    "                shutil.copyfile(label_full_path, destination_label_path)\n",
    "                # print(f\"Copied label to {destination_label_path}\")\n",
    "            else:\n",
    "                accuracy_tracker[common_name]['missed'] += 1\n",
    "                # print(f\"No animal detected in image for '{common_name}'\")\n",
    "\n",
    "            # Visualize the detections on the image for verification\n",
    "            if index_img < 10:\n",
    "                image_with_boxes = image.copy()\n",
    "                draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    box = [round(i, 2) for i in box.tolist()]\n",
    "                    detected_text = texts[0][label.item()]\n",
    "                    # print(\n",
    "                    #     f\"Detected '{detected_text}' with confidence {round(score.item(), 3)} at location {box}\"\n",
    "                    # )\n",
    "                    draw.rectangle(box, outline=\"red\", width=3)\n",
    "                    draw.text((box[0], box[1]), f\"{detected_text} {round(score.item(), 3)}\", fill=\"red\")\n",
    "\n",
    "                display(image_with_boxes)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process image {full_image_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Image file not found: {full_image_path}\")\n",
    "    index_img += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e44c17",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Automatic Labelling Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c831cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the loop, print the accuracy summary\n",
    "print(\"\\n--- OWL2 Detection Accuracy Summary ---\")\n",
    "for common_name, stats in accuracy_tracker.items():\n",
    "    total = stats['total']\n",
    "    if total > 0:\n",
    "        detected_fraction = stats['detected'] / total\n",
    "        missed_fraction = stats['missed'] / total\n",
    "        print(f\"Class: {common_name}, total: {total}, detected: {stats['detected']} ({detected_fraction:.2%}), missed: {stats['missed']} ({missed_fraction:.2%})\")\n",
    "print(\"-------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "label,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
