{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89450337",
   "metadata": {},
   "source": [
    "jupyter:\n",
    "  jupytext:\n",
    "    default_lexer: python\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.18.1\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981c3c3",
   "metadata": {},
   "source": [
    "# Object Detection with OWL2\n",
    "\n",
    "This notebook demonstrates how to use the OWL2 model for object detection on the IDLE-OO-Camera-Traps dataset.\n",
    "\n",
    "First, let's install the necessary libraries.\n",
    "\n",
    "```python\n",
    "!pip install -q transformers datasets torch torchvision Pillow ultralytics scikit-learn\n",
    "```\n",
    "\n",
    "Now, let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b491470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "from datasets import load_dataset\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display,HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebdf0d8",
   "metadata": {},
   "source": [
    "Next, we load the `imageomics/IDLE-OO-Camera-Traps` dataset. We'll just take one example from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2c416",
   "metadata": {
    "label": "load-image-cell"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(path=\"./data/IDLE-OO-Camera-Traps\", split=\"test\")\n",
    "iterator = iter(dataset)\n",
    "sample = next(iterator)\n",
    "image = sample[\"image\"]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4731b",
   "metadata": {},
   "source": [
    "## Object Detection with OWL2\n",
    "\n",
    "Now, we will use the OWL2 model for object detection.\n",
    "\n",
    "We will load the OWL2 model and processor from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b7634",
   "metadata": {},
   "source": [
    "Now, let's define the objects we want to detect. We can see a deer in the image, so let's try to detect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf869dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[\"a photo of a leopard\", \"a photo of a tiger\", \"a photo of a rock\"]]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163c4bd",
   "metadata": {},
   "source": [
    "Now we run the model to get the object detection outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5d7993",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40768bb0",
   "metadata": {},
   "source": [
    "The model outputs logits and bounding boxes. We need to post-process these to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5876fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "target_sizes = torch.Tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "\n",
    "i = 0  # Retrieve predictions for the first image\n",
    "text = texts[i]\n",
    "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "# Draw bounding boxes on a copy of the image\n",
    "image_with_boxes = image.copy()\n",
    "draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "# Define a list of colors to use for different labels\n",
    "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\"]\n",
    "\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    # Assign a color based on the label\n",
    "    color = colors[label.item() % len(colors)]\n",
    "    print(\n",
    "        f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\"\n",
    "    )\n",
    "    draw.rectangle(box, outline=color, width=3)\n",
    "    # Draw the label and confidence score\n",
    "    draw.text((box[0], box[1]), f\"{text[label]} {round(score.item(), 3)}\", fill=color)\n",
    "\n",
    "image_with_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b6ec2",
   "metadata": {},
   "source": [
    "### OWL Intermediate Layer Visualization\n",
    "\n",
    "Similar to YOLO, we can visualize the intermediate layers of the OWL's vision model. The OWL model uses a Vision Transformer (ViT), so the intermediate features are sequences of patch embeddings. We'll reshape them back into a grid and use PCA to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list to store the feature maps\n",
    "feature_maps = []\n",
    "hooks = []\n",
    "\n",
    "# The hook function that saves the output of a layer\n",
    "def hook_fn_owl(module, input, output):\n",
    "    # The output of Owlv2EncoderLayer is a tuple, we're interested in the hidden states\n",
    "    feature_maps.append(output[0])\n",
    "\n",
    "# We will visualize the output of a few layers from the vision encoder\n",
    "vision_encoder_layers = model.owlv2.vision_model.encoder.layers\n",
    "# Let's pick the first, a middle, and the last layer of the 12-layer encoder\n",
    "layers_to_hook_indices = [0, 5, 11]\n",
    "layers_to_hook = [vision_encoder_layers[i] for i in layers_to_hook_indices]\n",
    "\n",
    "# Register a forward hook for each layer\n",
    "for layer in layers_to_hook:\n",
    "    hooks.append(layer.register_forward_hook(hook_fn_owl))\n",
    "\n",
    "# Run inference to trigger the hooks\n",
    "feature_maps = []\n",
    "with torch.no_grad():\n",
    "    outputs_viz = model(**inputs)\n",
    "\n",
    "# Remove the hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Now, let's visualize the feature maps\n",
    "num_layers = len(feature_maps)\n",
    "layer_names = [f\"Layer {i}\" for i in layers_to_hook_indices]\n",
    "\n",
    "# Get the patch grid dimensions\n",
    "_, _, H_proc, W_proc = inputs['pixel_values'].shape\n",
    "patch_size = model.owlv2.vision_model.config.patch_size\n",
    "h_patches = H_proc // patch_size\n",
    "w_patches = W_proc // patch_size\n",
    "\n",
    "# Plot the feature maps in a grid\n",
    "cols = num_layers\n",
    "rows = 1\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "if num_layers == 1:\n",
    "    axes = [axes]\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (fm, name) in enumerate(zip(feature_maps, layer_names)):\n",
    "    fm = fm.detach().cpu()[0] # Get batch 0\n",
    "    \n",
    "    num_patches = fm.shape[0]\n",
    "    expected_patches = h_patches * w_patches\n",
    "    \n",
    "    ax = axes[i]\n",
    "    \n",
    "    data_to_visualize = None\n",
    "    # Check if there is an extra token (like a CLS token)\n",
    "    if num_patches == expected_patches + 1:\n",
    "        # Exclude the first token (CLS token)\n",
    "        data_to_visualize = fm[1:].numpy()\n",
    "    elif num_patches == expected_patches:\n",
    "        data_to_visualize = fm.numpy()\n",
    "\n",
    "    if data_to_visualize is not None:\n",
    "        # It's a good practice to scale data before PCA\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(data_to_visualize)\n",
    "        \n",
    "        # Apply PCA to visualize the patch embeddings\n",
    "        pca = PCA(n_components=3)\n",
    "        pca_result = pca.fit_transform(scaled_data)\n",
    "        \n",
    "        # Reshape back to an image\n",
    "        pca_image = pca_result.reshape(h_patches, w_patches, 3)\n",
    "        \n",
    "        # Normalize for display\n",
    "        for c in range(3):\n",
    "            channel = pca_image[:, :, c]\n",
    "            min_val, max_val = channel.min(), channel.max()\n",
    "            if max_val > min_val:\n",
    "                pca_image[:, :, c] = (channel - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                pca_image[:, :, c] = 0\n",
    "        \n",
    "        ax.imshow(pca_image)\n",
    "        ax.set_title(name)\n",
    "    else:\n",
    "        # If shape is still unexpected, show an error\n",
    "        ax.text(0.5, 0.5, f\"Shape mismatch:\\n{num_patches} patches vs\\n{expected_patches} expected\", ha='center', va='center')\n",
    "        ax.set_title(name)\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7acafac",
   "metadata": {},
   "source": [
    "## Automatic data labelling\n",
    "\n",
    "Here we will loop over images from the ENA24 dataset and apply the \"a photo of an animal\" prompt to automatically label them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66625ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the base path to the locally checked out dataset\n",
    "base_data_path = 'data/IDLE-OO-Camera-Traps/'\n",
    "ena24_csv_path = os.path.join(base_data_path, 'ENA24-balanced.csv')\n",
    "\n",
    "# Load the ENA24-balanced.csv file\n",
    "ena24_df = pd.read_csv(ena24_csv_path)\n",
    "print(f\"Successfully loaded {ena24_csv_path}\")\n",
    "print(f\"Total images in ENA24 dataset: {len(ena24_df)}\")\n",
    "\n",
    "# Take a sample of 5 images\n",
    "sample_images = ena24_df.sample(10, random_state=42) # Use a random state for reproducibility\n",
    "\n",
    "texts = [[\"a photo of an animal\", \"a photo of a bird\", \"animal\", \"bird\"]]\n",
    "\n",
    "for index, row in sample_images.iterrows():\n",
    "    image_relative_path = row['filepath']\n",
    "    full_image_path = os.path.join(base_data_path, 'data/test/', image_relative_path)\n",
    "    \n",
    "    if os.path.exists(full_image_path):\n",
    "        try:\n",
    "            print(f\"Processing image: {full_image_path}\")\n",
    "            image = Image.open(full_image_path).convert(\"RGB\")\n",
    "            \n",
    "            inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "            target_sizes = torch.Tensor([image.size[::-1]])\n",
    "            results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "\n",
    "            i = 0  # Predictions for the first (and only) image\n",
    "            text = texts[i]\n",
    "            boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "            image_with_boxes = image.copy()\n",
    "            draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "            for box, score, label in zip(boxes, scores, labels):\n",
    "                box = [round(i, 2) for i in box.tolist()]\n",
    "                color = \"red\"\n",
    "                print(\n",
    "                    f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\"\n",
    "                )\n",
    "                draw.rectangle(box, outline=color, width=3)\n",
    "                draw.text((box[0], box[1]), f\"{text[label]} {round(score.item(), 3)}\", fill=color)\n",
    "\n",
    "            display(image_with_boxes)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process image {full_image_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Image file not found: {full_image_path}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "label,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
